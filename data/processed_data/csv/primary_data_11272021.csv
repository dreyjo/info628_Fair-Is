,title,abstract
1,Unfair items detection in educational measurement,"  Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.
"
2,Fairness in Academic Course Timetabling,"  We consider the problem of creating fair course timetables in the setting of
a university. Our motivation is to improve the overall satisfaction of
individuals concerned (students, teachers, etc.) by providing a fair timetable
to them. The central idea is that undesirable arrangements in the course
timetable, i.e., violations of soft constraints, should be distributed in a
fair way among the individuals. We propose two formulations for the fair course
timetabling problem that are based on max-min fairness and Jain's fairness
index, respectively. Furthermore, we present and experimentally evaluate an
optimization algorithm based on simulated annealing for solving max-min fair
course timetabling problems. The new contribution is concerned with measuring
the energy difference between two timetables, i.e., how much worse a timetable
is compared to another timetable with respect to max-min fairness. We introduce
three different energy difference measures and evaluate their impact on the
overall algorithm performance. The second proposed problem formulation focuses
on the tradeoff between fairness and the total amount of soft constraint
violations. Our experimental evaluation shows that the known best solutions to
the ITC2007 curriculum-based course timetabling instances are quite fair with
respect to Jain's fairness index. However, the experiments also show that the
fairness can be improved further for only a rather small increase in the total
amount of soft constraint violations.
"
3,"Safeguarding E-Commerce against Advisor Cheating Behaviors: Towards More
  Robust Trust Models for Handling Unfair Ratings","  In electronic marketplaces, after each transaction buyers will rate the
products provided by the sellers. To decide the most trustworthy sellers to
transact with, buyers rely on trust models to leverage these ratings to
evaluate the reputation of sellers. Although the high effectiveness of
different trust models for handling unfair ratings have been claimed by their
designers, recently it is argued that these models are vulnerable to more
intelligent attacks, and there is an urgent demand that the robustness of the
existing trust models has to be evaluated in a more comprehensive way. In this
work, we classify the existing trust models into two broad categories and
propose an extendable e-marketplace testbed to evaluate their robustness
against different unfair rating attacks comprehensively. On top of highlighting
the robustness of the existing trust models for handling unfair ratings is far
from what they were claimed to be, we further propose and validate a novel
combination mechanism for the existing trust models, Discount-then-Filter, to
notably enhance their robustness against the investigated attacks.
"
4,"A Decomposition of the Max-min Fair Curriculum-based Course Timetabling
  Problem","  We propose a decomposition of the max-min fair curriculum-based course
timetabling (MMF-CB-CTT) problem. The decomposition models the room assignment
subproblem as a generalized lexicographic bottleneck optimization problem
(LBOP). We show that the generalized LBOP can be solved efficiently if the
corresponding sum optimization problem can be solved efficiently. As a
consequence, the room assignment subproblem of the MMF-CB-CTT problem can be
solved efficiently. We use this insight to improve a previously proposed
heuristic algorithm for the MMF-CB-CTT problem. Our experimental results
indicate that using the new decomposition improves the performance of the
algorithm on most of the 21 ITC2007 test instances with respect to the quality
of the best solution found. Furthermore, we introduce a measure of the quality
of a solution to a max-min fair optimization problem. This measure helps to
overcome some limitations imposed by the qualitative nature of max-min fairness
and aids the statistical evaluation of the performance of randomized algorithms
for such problems. We use this measure to show that using the new decomposition
the algorithm outperforms the original one on most instances with respect to
the average solution quality.
"
5,Fair assignment of indivisible objects under ordinal preferences,"  We consider the discrete assignment problem in which agents express ordinal
preferences over objects and these objects are allocated to the agents in a
fair manner. We use the stochastic dominance relation between fractional or
randomized allocations to systematically define varying notions of
proportionality and envy-freeness for discrete assignments. The computational
complexity of checking whether a fair assignment exists is studied for these
fairness notions. We also characterize the conditions under which a fair
assignment is guaranteed to exist. For a number of fairness concepts,
polynomial-time algorithms are presented to check whether a fair assignment
exists. Our algorithmic results also extend to the case of unequal entitlements
of agents. Our NP-hardness result, which holds for several variants of
envy-freeness, answers an open question posed by Bouveret, Endriss, and Lang
(ECAI 2010). We also propose fairness concepts that always suggest a non-empty
set of assignments with meaningful fairness properties. Among these concepts,
optimal proportionality and optimal weak proportionality appear to be desirable
fairness concepts.
"
6,Online Fair Division: analysing a Food Bank problem,"  We study an online model of fair division designed to capture features of a
real world charity problem. We consider two simple mechanisms for this model in
which agents simply declare what items they like. We analyse several axiomatic
properties of these mechanisms like strategy-proofness and envy-freeness.
Finally, we perform a competitive analysis and compute the price of anarchy.
"
7,On the relation between accuracy and fairness in binary classification,"  Our study revisits the problem of accuracy-fairness tradeoff in binary
classification. We argue that comparison of non-discriminatory classifiers
needs to account for different rates of positive predictions, otherwise
conclusions about performance may be misleading, because accuracy and
discrimination of naive baselines on the same dataset vary with different rates
of positive predictions. We provide methodological recommendations for sound
comparison of non-discriminatory classifiers, and present a brief theoretical
and empirical analysis of tradeoffs between accuracy and non-discrimination.
"
8,Fair task allocation in transportation,"  Task allocation problems have traditionally focused on cost optimization.
However, more and more attention is being given to cases in which cost should
not always be the sole or major consideration. In this paper we study a fair
task allocation problem in transportation where an optimal allocation not only
has low cost but more importantly, it distributes tasks as even as possible
among heterogeneous participants who have different capacities and costs to
execute tasks. To tackle this fair minimum cost allocation problem we analyze
and solve it in two parts using two novel polynomial-time algorithms. We show
that despite the new fairness criterion, the proposed algorithms can solve the
fair minimum cost allocation problem optimally in polynomial time. In addition,
we conduct an extensive set of experiments to investigate the trade-off between
cost minimization and fairness. Our experimental results demonstrate the
benefit of factoring fairness into task allocation. Among the majority of test
instances, fairness comes with a very small price in terms of cost.
"
9,"Efficiency and Sequenceability in Fair Division of Indivisible Goods
  with Additive Preferences","  In fair division of indivisible goods, using sequences of sincere choices (or
picking sequences) is a natural way to allocate the objects. The idea is the
following: at each stage, a designated agent picks one object among those that
remain. This paper, restricted to the case where the agents have numerical
additive preferences over objects, revisits to some extent the seminal paper by
Brams and King [9] which was specific to ordinal and linear order preferences
over items. We point out similarities and differences with this latter context.
In particular, we show that any Pareto-optimal allocation (under additive
preferences) is sequenceable, but that the converse is not true anymore. This
asymmetry leads naturally to the definition of a ""scale of efficiency"" having
three steps: Pareto-optimality, sequenceability without Pareto-optimality, and
non-sequenceability. Finally, we investigate the links between these efficiency
properties and the ""scale of fairness"" we have described in an earlier work
[7]: we first show that an allocation can be envy-free and non-sequenceable,
but that every competitive equilibrium with equal incomes is sequenceable. Then
we experimentally explore the links between the scales of efficiency and
fairness.
"
10,Fairness as a Program Property,"  We explore the following question: Is a decision-making program fair, for
some useful definition of fairness? First, we describe how several algorithmic
fairness questions can be phrased as program verification problems. Second, we
discuss an automated verification technique for proving or disproving fairness
of decision-making programs with respect to a probabilistic model of the
population.
"
11,Fair Division via Social Comparison,"  In the classical cake cutting problem, a resource must be divided among
agents with different utilities so that each agent believes they have received
a fair share of the resource relative to the other agents. We introduce a
variant of the problem in which we model an underlying social network on the
agents with a graph, and agents only evaluate their shares relative to their
neighbors' in the network. This formulation captures many situations in which
it is unrealistic to assume a global view, and also exposes interesting
phenomena in the original problem.
  Specifically, we say an allocation is locally envy-free if no agent envies a
neighbor's allocation and locally proportional if each agent values her own
allocation as much as the average value of her neighbor's allocations, with the
former implying the latter. While global envy-freeness implies local
envy-freeness, global proportionality does not imply local proportionality, or
vice versa. A general result is that for any two distinct graphs on the same
set of nodes and an allocation, there exists a set of valuation functions such
that the allocation is locally proportional on one but not the other.
  We fully characterize the set of graphs for which an oblivious single-cutter
protocol-- a protocol that uses a single agent to cut the cake into pieces
--admits a bounded protocol with $O(n^2)$ query complexity for locally
envy-free allocations in the Robertson-Webb model. We also consider the price
of envy-freeness, which compares the total utility of an optimal allocation to
the best utility of an allocation that is envy-free. We show that a lower bound
of $\Omega(\sqrt{n})$ on the price of envy-freeness for global allocations in
fact holds for local envy-freeness in any connected undirected graph. Thus,
sparse graphs surprisingly do not provide more flexibility with respect to the
quality of envy-free allocations.
"
12,"Balancing Lexicographic Fairness and a Utilitarian Objective with
  Application to Kidney Exchange","  Balancing fairness and efficiency in resource allocation is a classical
economic and computational problem. The price of fairness measures the
worst-case loss of economic efficiency when using an inefficient but fair
allocation rule; for indivisible goods in many settings, this price is
unacceptably high. One such setting is kidney exchange, where needy patients
swap willing but incompatible kidney donors. In this work, we close an open
problem regarding the theoretical price of fairness in modern kidney exchanges.
We then propose a general hybrid fairness rule that balances a strict
lexicographic preference ordering over classes of agents, and a utilitarian
objective that maximizes economic efficiency. We develop a utility function for
this rule that favors disadvantaged groups lexicographically; but if cost to
overall efficiency becomes too high, it switches to a utilitarian objective.
This rule has only one parameter which is proportional to a bound on the price
of fairness, and can be adjusted by policymakers. We apply this rule to real
data from a large kidney exchange and show that our hybrid rule produces more
reliable outcomes than other fairness rules.
"
13,FairJudge: Trustworthy User Prediction in Rating Platforms,"  Rating platforms enable large-scale collection of user opinion about items
(products, other users, etc.). However, many untrustworthy users give
fraudulent ratings for excessive monetary gains. In the paper, we present
FairJudge, a system to identify such fraudulent users. We propose three
metrics: (i) the fairness of a user that quantifies how trustworthy the user is
in rating the products, (ii) the reliability of a rating that measures how
reliable the rating is, and (iii) the goodness of a product that measures the
quality of the product. Intuitively, a user is fair if it provides reliable
ratings that are close to the goodness of the product. We formulate a mutually
recursive definition of these metrics, and further address cold start problems
and incorporate behavioral properties of users and products in the formulation.
We propose an iterative algorithm, FairJudge, to predict the values of the
three metrics. We prove that FairJudge is guaranteed to converge in a bounded
number of iterations, with linear time complexity. By conducting five different
experiments on five rating platforms, we show that FairJudge significantly
outperforms nine existing algorithms in predicting fair and unfair users. We
reported the 100 most unfair users in the Flipkart network to their review
fraud investigators, and 80 users were correctly identified (80% accuracy). The
FairJudge algorithm is already being deployed at Flipkart.
"
14,Beyond Parity: Fairness Objectives for Collaborative Filtering,"  We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.
"
15,New Fairness Metrics for Recommendation that Embrace Differences,"  We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative filtering methods to make unfair predictions against
minority groups of users. We identify the insufficiency of existing fairness
metrics and propose four new metrics that address different forms of
unfairness. These fairness metrics can be optimized by adding fairness terms to
the learning objective. Experiments on synthetic and real data show that our
new metrics can better measure fairness than the baseline, and that the
fairness objectives effectively help reduce unfairness.
"
16,"The impossibility of ""fairness"": a generalized impossibility result for
  decisions","  Various measures can be used to estimate bias or unfairness in a predictor.
Previous work has already established that some of these measures are
incompatible with each other. Here we show that, when groups differ in
prevalence of the predicted event, several intuitive, reasonable measures of
fairness (probability of positive prediction given occurrence or
non-occurrence; probability of occurrence given prediction or non-prediction;
and ratio of predictions over occurrences for each group) are all mutually
exclusive: if one of them is equal among groups, the other two must differ. The
only exceptions are for perfect, or trivial (always-positive or
always-negative) predictors. As a consequence, any non-perfect, non-trivial
predictor must necessarily be ""unfair"" under two out of three reasonable sets
of criteria. This result readily generalizes to a wide range of well-known
statistical quantities (sensitivity, specificity, false positive rate,
precision, etc.), all of which can be divided into three mutually exclusive
groups. Importantly, The results applies to all predictors, whether algorithmic
or human. We conclude with possible ways to handle this effect when assessing
and designing prediction methods.
"
17,Networked Fairness in Cake Cutting,"  We introduce a graphical framework for fair division in cake cutting, where
comparisons between agents are limited by an underlying network structure. We
generalize the classical fairness notions of envy-freeness and proportionality
to this graphical setting. Given a simple undirected graph G, an allocation is
envy-free on G if no agent envies any of her neighbor's share, and is
proportional on G if every agent values her own share no less than the average
among her neighbors, with respect to her own measure. These generalizations
open new research directions in developing simple and efficient algorithms that
can produce fair allocations under specific graph structures.
  On the algorithmic frontier, we first propose a moving-knife algorithm that
outputs an envy-free allocation on trees. The algorithm is significantly
simpler than the discrete and bounded envy-free algorithm recently designed by
Aziz and Mackenzie for complete graphs. Next, we give a discrete and bounded
algorithm for computing a proportional allocation on descendant graphs, a class
of graphs by taking a rooted tree and connecting all its ancestor-descendant
pairs.
"
18,Fairness-aware machine learning: a perspective,"  Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.
"
19,Fairness Testing: Testing Software for Discrimination,"  This paper defines software fairness and discrimination and develops a
testing-based method for measuring if and how much software discriminates,
focusing on causality in discriminatory behavior. Evidence of software
discrimination has been found in modern software systems that recommend
criminal sentences, grant access to financial products, and determine who is
allowed to participate in promotions. Our approach, Themis, generates efficient
test suites to measure discrimination. Given a schema describing valid system
inputs, Themis generates discrimination tests automatically and does not
require an oracle. We evaluate Themis on 20 software systems, 12 of which come
from prior work with explicit focus on avoiding discrimination. We find that
(1) Themis is effective at discovering software discrimination, (2)
state-of-the-art techniques for removing discrimination from algorithms fail in
many situations, at times discriminating against as much as 98% of an input
subdomain, (3) Themis optimizations are effective at producing efficient test
suites for measuring discrimination, and (4) Themis is more efficient on
systems that exhibit more discrimination. We thus demonstrate that fairness
testing is a critical aspect of the software development cycle in domains with
possible discrimination and provide initial tools for measuring software
discrimination.
"
20,On Formalizing Fairness in Prediction with Machine Learning,"  Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.
"
21,Two-stage Algorithm for Fairness-aware Machine Learning,"  Algorithmic decision making process now affects many aspects of our lives.
Standard tools for machine learning, such as classification and regression, are
subject to the bias in data, and thus direct application of such off-the-shelf
tools could lead to a specific group being unfairly discriminated. Removing
sensitive attributes of data does not solve this problem because a
\textit{disparate impact} can arise when non-sensitive attributes and sensitive
attributes are correlated. Here, we study a fair machine learning algorithm
that avoids such a disparate impact when making a decision. Inspired by the
two-stage least squares method that is widely used in the field of economics,
we propose a two-stage algorithm that removes bias in the training data. The
proposed algorithm is conceptually simple. Unlike most of existing fair
algorithms that are designed for classification tasks, the proposed method is
able to (i) deal with regression tasks, (ii) combine explanatory attributes to
remove reverse discrimination, and (iii) deal with numerical sensitive
attributes. The performance and fairness of the proposed algorithm are
evaluated in simulations with synthetic and real-world datasets.
"
22,Multiwinner Voting with Fairness Constraints,"  Multiwinner voting rules are used to select a small representative subset of
candidates or items from a larger set given the preferences of voters. However,
if candidates have sensitive attributes such as gender or ethnicity (when
selecting a committee), or specified types such as political leaning (when
selecting a subset of news items), an algorithm that chooses a subset by
optimizing a multiwinner voting rule may be unbalanced in its selection -- it
may under or over represent a particular gender or political orientation in the
examples above. We introduce an algorithmic framework for multiwinner voting
problems when there is an additional requirement that the selected subset
should be ""fair"" with respect to a given set of attributes. Our framework
provides the flexibility to (1) specify fairness with respect to multiple,
non-disjoint attributes (e.g., ethnicity and gender) and (2) specify a score
function. We study the computational complexity of this constrained multiwinner
voting problem for monotone and submodular score functions and present several
approximation algorithms and matching hardness of approximation results for
various attribute group structure and types of score functions. We also present
simulations that suggest that adding fairness constraints may not affect the
scores significantly when compared to the unconstrained case.
"
23,Groupwise Maximin Fair Allocation of Indivisible Goods,"  We study the problem of allocating indivisible goods among n agents in a fair
manner. For this problem, maximin share (MMS) is a well-studied solution
concept which provides a fairness threshold. Specifically, maximin share is
defined as the minimum utility that an agent can guarantee for herself when
asked to partition the set of goods into n bundles such that the remaining
(n-1) agents pick their bundles adversarially. An allocation is deemed to be
fair if every agent gets a bundle whose valuation is at least her maximin
share.
  Even though maximin shares provide a natural benchmark for fairness, it has
its own drawbacks and, in particular, it is not sufficient to rule out
unsatisfactory allocations. Motivated by these considerations, in this work we
define a stronger notion of fairness, called groupwise maximin share guarantee
(GMMS). In GMMS, we require that the maximin share guarantee is achieved not
just with respect to the grand bundle, but also among all the subgroups of
agents. Hence, this solution concept strengthens MMS and provides an ex-post
fairness guarantee. We show that in specific settings, GMMS allocations always
exist. We also establish the existence of approximate GMMS allocations under
additive valuations, and develop a polynomial-time algorithm to find such
allocations. Moreover, we establish a scale of fairness wherein we show that
GMMS implies approximate envy freeness.
  Finally, we empirically demonstrate the existence of GMMS allocations in a
large set of randomly generated instances. For the same set of instances, we
additionally show that our algorithm achieves an approximation factor better
than the established, worst-case bound.
"
24,Fairness in Supervised Learning: An Information Theoretic Approach,"  Automated decision making systems are increasingly being used in real-world
applications. In these systems for the most part, the decision rules are
derived by minimizing the training error on the available historical data.
Therefore, if there is a bias related to a sensitive attribute such as gender,
race, religion, etc. in the data, say, due to cultural/historical
discriminatory practices against a certain demographic, the system could
continue discrimination in decisions by including the said bias in its decision
rule. We present an information theoretic framework for designing fair
predictors from data, which aim to prevent discrimination against a specified
sensitive attribute in a supervised learning setting. We use equalized odds as
the criterion for discrimination, which demands that the prediction should be
independent of the protected attribute conditioned on the actual label. To
ensure fairness and generalization simultaneously, we compress the data to an
auxiliary variable, which is used for the prediction task. This auxiliary
variable is chosen such that it is decontaminated from the discriminatory
attribute in the sense of equalized odds. The final predictor is obtained by
applying a Bayesian decision rule to the auxiliary variable.
"
25,"Value Alignment, Fair Play, and the Rights of Service Robots","  Ethics and safety research in artificial intelligence is increasingly framed
in terms of ""alignment"" with human values and interests. I argue that Turing's
call for ""fair play for machines"" is an early and often overlooked contribution
to the alignment literature. Turing's appeal to fair play suggests a need to
correct human behavior to accommodate our machines, a surprising inversion of
how value alignment is treated today. Reflections on ""fair play"" motivate a
novel interpretation of Turing's notorious ""imitation game"" as a condition not
of intelligence but instead of value alignment: a machine demonstrates a
minimal degree of alignment (with the norms of conversation, for instance) when
it can go undetected when interrogated by a human. I carefully distinguish this
interpretation from the Moral Turing Test, which is not motivated by a
principle of fair play, but instead depends on imitation of human moral
behavior. Finally, I consider how the framework of fair play can be used to
situate the debate over robot rights within the alignment literature. I argue
that extending rights to service robots operating in public spaces is ""fair"" in
precisely the sense that it encourages an alignment of interests between humans
and machines.
"
26,Reinforcement Learning for Fair Dynamic Pricing,"  Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.
"
27,Deep Bayesian Trust : A Dominant and Fair Incentive Mechanism for Crowd,"  An important class of game-theoretic incentive mechanisms for eliciting
effort from a crowd are the peer based mechanisms, in which workers are paid by
matching their answers with one another. The other classic mechanism is to have
the workers solve some gold standard tasks and pay them according to their
accuracy on gold tasks. This mechanism ensures stronger incentive compatibility
than the peer based mechanisms but assigning gold tasks to all workers becomes
inefficient at large scale. We propose a novel mechanism that assigns gold
tasks to only a few workers and exploits transitivity to derive accuracy of the
rest of the workers from their peers' accuracy. We show that the resulting
mechanism ensures a dominant notion of incentive compatibility and fairness.
"
28,Fair Division Under Cardinality Constraints,"  We consider the problem of fairly allocating indivisible goods, among agents,
under cardinality constraints and additive valuations. In this setting, we are
given a partition of the entire set of goods---i.e., the goods are
categorized---and a limit is specified on the number of goods that can be
allocated from each category to any agent. The objective here is to find a fair
allocation in which the subset of goods assigned to any agent satisfies the
given cardinality constraints. This problem naturally captures a number of
resource-allocation applications, and is a generalization of the well-studied
(unconstrained) fair division problem.
  The two central notions of fairness, in the context of fair division of
indivisible goods, are envy freeness up to one good (EF1) and the (approximate)
maximin share guarantee (MMS). We show that the existence and algorithmic
guarantees established for these solution concepts in the unconstrained setting
can essentially be achieved under cardinality constraints. Specifically, we
develop efficient algorithms which compute EF1 and approximately MMS
allocations in the constrained setting.
  Furthermore, focusing on the case wherein all the agents have the same
additive valuation, we establish that EF1 allocations exist and can be computed
efficiently even under matroid constraints.
"
29,"CLAUDETTE: an Automated Detector of Potentially Unfair Clauses in Online
  Terms of Service","  Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.
"
30,Causal Reasoning for Algorithmic Fairness,"  In this work, we argue for the importance of causal reasoning in creating
fair algorithms for decision making. We give a review of existing approaches to
fairness, describe work in causality necessary for the understanding of causal
approaches, argue why causality is necessary for any approach that wishes to be
fair, and give a detailed analysis of the many recent approaches to
causality-based fairness.
"
31,"Pooling of Causal Models under Counterfactual Fairness via Causal
  Judgement Aggregation","  In this paper we consider the problem of combining multiple probabilistic
causal models, provided by different experts, under the requirement that the
aggregated model satisfy the criterion of counterfactual fairness. We build
upon the work on causal models and fairness in machine learning, and we express
the problem of combining multiple models within the framework of opinion
pooling. We propose two simple algorithms, grounded in the theory of
counterfactual fairness and causal judgment aggregation, that are guaranteed to
generate aggregated probabilistic causal models respecting the criterion of
fairness, and we compare their behaviors on a toy case study.
"
32,Causal Interventions for Fairness,"  Most approaches in algorithmic fairness constrain machine learning methods so
the resulting predictions satisfy one of several intuitive notions of fairness.
While this may help private companies comply with non-discrimination laws or
avoid negative publicity, we believe it is often too little, too late. By the
time the training data is collected, individuals in disadvantaged groups have
already suffered from discrimination and lost opportunities due to factors out
of their control. In the present work we focus instead on interventions such as
a new public policy, and in particular, how to maximize their positive effects
while improving the fairness of the overall system. We use causal methods to
model the effects of interventions, allowing for potential interference--each
individual's outcome may depend on who else receives the intervention. We
demonstrate this with an example of allocating a budget of teaching resources
using a dataset of schools in New York City.
"
33,Lecture Notes on Fair Division,"  Fair division is the problem of dividing one or several goods amongst two or
more agents in a way that satisfies a suitable fairness criterion. These Notes
provide a succinct introduction to the field. We cover three main topics.
First, we need to define what is to be understood by a ""fair"" allocation of
goods to individuals. We present an overview of the most important fairness
criteria (as well as the closely related criteria for economic efficiency)
developed in the literature, together with a short discussion of their
axiomatic foundations. Second, we give an introduction to cake-cutting
procedures as an example of methods for fairly dividing a single divisible
resource amongst a group of individuals. Third, we discuss the combinatorial
optimisation problem of fairly allocating a set of indivisible goods to a group
of agents, covering both centralised algorithms (similar to auctions) and a
distributed approach based on negotiation.
  While the classical literature on fair division has largely developed within
Economics, these Notes are specifically written for readers with a background
in Computer Science or similar, and who may be (or may wish to be) engaged in
research in Artificial Intelligence, Multiagent Systems, or Computational
Social Choice. References for further reading, as well as a small number of
exercises, are included.
  Notes prepared for a tutorial at the 11th European Agent Systems Summer
School (EASSS-2009), Torino, Italy, 31 August and 1 September 2009. Updated for
a tutorial at the COST-ADT Doctoral School on Computational Social Choice,
Estoril, Portugal, 9--14 April 2010.
"
34,"Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated
  Decision Making","  We draw attention to an important, yet largely overlooked aspect of
evaluating fairness for automated decision making systems---namely risk and
welfare considerations. Our proposed family of measures corresponds to the
long-established formulations of cardinal social welfare in economics, and is
justified by the Rawlsian conception of fairness behind a veil of ignorance.
The convex formulation of our welfare-based measures of fairness allows us to
integrate them as a constraint into any convex loss minimization pipeline. Our
empirical analysis reveals interesting trade-offs between our proposal and (a)
prediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of
individual fairness. Furthermore and perhaps most importantly, our work
provides both heuristic justification and empirical evidence suggesting that a
lower-bound on our measures often leads to bounded inequality in algorithmic
outcomes; hence presenting the first computationally feasible mechanism for
bounding individual-level inequality.
"
35,Comparing Fairness Criteria Based on Social Outcome,"  Fairness in algorithmic decision-making processes is attracting increasing
concern. When an algorithm is applied to human-related decision-making an
estimator solely optimizing its predictive power can learn biases on the
existing data, which motivates us the notion of fairness in machine learning.
while several different notions are studied in the literature, little studies
are done on how these notions affect the individuals. We demonstrate such a
comparison between several policies induced by well-known fairness criteria,
including the color-blind (CB), the demographic parity (DP), and the equalized
odds (EO). We show that the EO is the only criterion among them that removes
group-level disparity. Empirical studies on the social welfare and disparity of
these policies are conducted.
"
36,What About Applied Fairness?,"  Machine learning practitioners are often ambivalent about the ethical aspects
of their products. We believe anything that gets us from that current state to
one in which our systems are achieving some degree of fairness is an
improvement that should be welcomed. This is true even when that progress does
not get us 100% of the way to the goal of ""complete"" fairness or perfectly
align with our personal belief on which measure of fairness is used. Some
measure of fairness being built would still put us in a better position than
the status quo. Impediments to getting fairness and ethical concerns applied in
real applications, whether they are abstruse philosophical debates or technical
overhead such as the introduction of ever more hyper-parameters, should be
avoided. In this paper we further elaborate on our argument for this viewpoint
and its importance.
"
37,"Classification with Fairness Constraints: A Meta-Algorithm with Provable
  Guarantees","  Developing classification algorithms that are fair with respect to sensitive
attributes of the data has become an important problem due to the growing
deployment of classification algorithms in various social contexts. Several
recent works have focused on fairness with respect to a specific metric,
modeled the corresponding fair classification problem as a constrained
optimization problem, and developed tailored algorithms to solve them. Despite
this, there still remain important metrics for which we do not have fair
classifiers and many of the aforementioned algorithms do not come with
theoretical guarantees; perhaps because the resulting optimization problem is
non-convex. The main contribution of this paper is a new meta-algorithm for
classification that takes as input a large class of fairness constraints, with
respect to multiple non-disjoint sensitive attributes, and which comes with
provable guarantees. This is achieved by first developing a meta-algorithm for
a large family of classification problems with convex constraints, and then
showing that classification problems with general types of fairness constraints
can be reduced to those in this family. We present empirical results that show
that our algorithm can achieve near-perfect fairness with respect to various
fairness metrics, and that the loss in accuracy due to the imposed fairness
constraints is often small. Overall, this work unifies several prior works on
fair classification, presents a practical algorithm with theoretical
guarantees, and can handle fairness metrics that were previously not possible.
"
38,Automated Directed Fairness Testing,"  Fairness is a critical trait in decision making. As machine-learning models
are increasingly being used in sensitive application domains (e.g. education
and employment) for decision making, it is crucial that the decisions computed
by such models are free of unintended bias. But how can we automatically
validate the fairness of arbitrary machine-learning models? For a given
machine-learning model and a set of sensitive input parameters, our AEQUITAS
approach automatically discovers discriminatory inputs that highlight fairness
violation. At the core of AEQUITAS are three novel strategies to employ
probabilistic search over the input space with the objective of uncovering
fairness violation. Our AEQUITAS approach leverages inherent robustness
property in common machine-learning models to design and implement scalable
test generation methodologies. An appealing feature of our generated test
inputs is that they can be systematically added to the training set of the
underlying model and improve its fairness. To this end, we design a fully
automated module that guarantees to improve the fairness of the underlying
model.
  We implemented AEQUITAS and we have evaluated it on six state-of-the-art
classifiers, including a classifier that was designed with fairness
constraints. We show that AEQUITAS effectively generates inputs to uncover
fairness violation in all the subject classifiers and systematically improves
the fairness of the respective models using the generated test inputs. In our
evaluation, AEQUITAS generates up to 70% discriminatory inputs (w.r.t. the
total number of inputs generated) and leverages these inputs to improve the
fairness up to 94%.
"
39,Fairly Allocating Many Goods with Few Queries,"  We investigate the query complexity of the fair allocation of indivisible
goods. For two agents with arbitrary monotonic valuations, we design an
algorithm that computes an allocation satisfying envy-freeness up to one good
(EF1), a relaxation of envy-freeness, using a logarithmic number of queries. We
show that the logarithmic query complexity bound also holds for three agents
with additive valuations. These results suggest that it is possible to fairly
allocate goods in practice even when the number of goods is extremely large. By
contrast, we prove that computing an allocation satisfying envy-freeness and
another of its relaxations, envy-freeness up to any good (EFX), requires a
linear number of queries even when there are only two agents with identical
additive valuations.
"
40,"Efficiency, Sequenceability and Deal-Optimality in Fair Division of
  Indivisible Goods","  In fair division of indivisible goods, using sequences of sincere choices (or
picking sequences) is a natural way to allocate the objects. The idea is as
follows: at each stage, a designated agent picks one object among those that
remain. Another intuitive way to obtain an allocation is to give objects to
agents in the first place, and to let agents exchange them as long as such
""deals"" are beneficial. This paper investigates these notions, when agents have
additive preferences over objects, and unveils surprising connections between
them, and with other efficiency and fairness notions. In particular, we show
that an allocation is sequenceable iff it is optimal for a certain type of
deals, namely cycle deals involving a single object. Furthermore, any
Pareto-optimal allocation is sequenceable, but not the converse. Regarding
fairness, we show that an allocation can be envy-free and non-sequenceable, but
that every competitive equilibrium with equal incomes is sequenceable. To
complete the picture, we show how some domain restrictions may affect the
relations between these notions. Finally, we experimentally explore the links
between the scales of efficiency and fairness.
"
41,"Optimization with Non-Differentiable Constraints with Applications to
  Fairness, Recall, Churn, and Other Goals","  We show that many machine learning goals, such as improved fairness metrics,
can be expressed as constraints on the model's predictions, which we call rate
constraints. We study the problem of training non-convex models subject to
these rate constraints (or any non-convex and non-differentiable constraints).
In the non-convex setting, the standard approach of Lagrange multipliers may
fail. Furthermore, if the constraints are non-differentiable, then one cannot
optimize the Lagrangian with gradient-based methods. To solve these issues, we
introduce the proxy-Lagrangian formulation. This new formulation leads to an
algorithm that produces a stochastic classifier by playing a two-player
non-zero-sum game solving for what we call a semi-coarse correlated
equilibrium, which in turn corresponds to an approximately optimal and feasible
solution to the constrained optimization problem. We then give a procedure
which shrinks the randomized solution down to one that is a mixture of at most
$m+1$ deterministic solutions, given $m$ constraints. This culminates in
algorithms that can solve non-convex constrained optimization problems with
possibly non-differentiable and non-convex constraints with theoretical
guarantees. We provide extensive experimental results enforcing a wide range of
policy goals including different fairness metrics, and other goals on accuracy,
coverage, recall, and churn.
"
42,Fair lending needs explainable models for responsible recommendation,"  The financial services industry has unique explainability and fairness
challenges arising from compliance and ethical considerations in credit
decisioning. These challenges complicate the use of model machine learning and
artificial intelligence methods in business decision processes.
"
43,"Fairness-aware Classification: Criterion, Convexity, and Bounds","  Fairness-aware classification is receiving increasing attention in the
machine learning fields. Recently research proposes to formulate the
fairness-aware classification as constrained optimization problems. However,
several limitations exist in previous works due to the lack of a theoretical
framework for guiding the formulation. In this paper, we propose a general
framework for learning fair classifiers which addresses previous limitations.
The framework formulates various commonly-used fairness metrics as convex
constraints that can be directly incorporated into classic classification
models. Within the framework, we propose a constraint-free criterion on the
training data which ensures that any classifier learned from the data is fair.
We also derive the constraints which ensure that the real fairness metric is
satisfied when surrogate functions are used to achieve convexity. Our framework
can be used to for formulating fairness-aware classification with fairness
guarantee and computational efficiency. The experiments using real-world
datasets demonstrate our theoretical results and show the effectiveness of
proposed framework and methods.
"
44,Active Fairness in Algorithmic Decision Making,"  Society increasingly relies on machine learning models for automated decision
making. Yet, efficiency gains from automation have come paired with concern for
algorithmic discrimination that can systematize inequality. Recent work has
proposed optimal post-processing methods that randomize classification
decisions for a fraction of individuals, in order to achieve fairness measures
related to parity in errors and calibration. These methods, however, have
raised concern due to the information inefficiency, intra-group unfairness, and
Pareto sub-optimality they entail. The present work proposes an alternative
active framework for fair classification, where, in deployment, a
decision-maker adaptively acquires information according to the needs of
different groups or individuals, towards balancing disparities in
classification performance. We propose two such methods, where information
collection is adapted to group- and individual-level needs respectively. We
show on real-world datasets that these can achieve: 1) calibration and single
error parity (e.g., equal opportunity); and 2) parity in both false positive
and false negative rates (i.e., equal odds). Moreover, we show that by
leveraging their additional degree of freedom, active approaches can
substantially outperform randomization-based classifiers previously considered
optimal, while avoiding limitations such as intra-group unfairness.
"
45,Counterfactually Fair Prediction Using Multiple Causal Models,"  In this paper we study the problem of making predictions using multiple
structural casual models defined by different agents, under the constraint that
the prediction satisfies the criterion of counterfactual fairness. Relying on
the frameworks of causality, fairness and opinion pooling, we build upon and
extend previous work focusing on the qualitative aggregation of causal Bayesian
networks and causal models. In order to complement previous qualitative
results, we devise a method based on Monte Carlo simulations. This method
enables a decision-maker to aggregate the outputs of the causal models provided
by different experts while guaranteeing the counterfactual fairness of the
result. We demonstrate our approach on a simple, yet illustrative, toy case
study.
"
46,Can everyday AI be ethical. Fairness of Machine Learning Algorithms,"  Combining big data and machine learning algorithms, the power of automatic
decision tools induces as much hope as fear. Many recently enacted European
legislation (GDPR) and French laws attempt to regulate the use of these tools.
Leaving aside the well-identified problems of data confidentiality and
impediments to competition, we focus on the risks of discrimination, the
problems of transparency and the quality of algorithmic decisions. The detailed
perspective of the legal texts, faced with the complexity and opacity of the
learning algorithms, reveals the need for important technological disruptions
for the detection or reduction of the discrimination risk, and for addressing
the right to obtain an explanation of the auto- matic decision. Since trust of
the developers and above all of the users (citizens, litigants, customers) is
essential, algorithms exploiting personal data must be deployed in a strict
ethical framework. In conclusion, to answer this need, we list some ways of
controls to be developed: institutional control, ethical charter, external
audit attached to the issue of a label.
"
47,"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and
  Mitigating Unwanted Algorithmic Bias","  Fairness is an increasingly important concern as machine learning models are
used to support decision making in high-stakes applications such as mortgage
lending, hiring, and prison sentencing. This paper introduces a new open source
Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released
under an Apache v2.0 license {https://github.com/ibm/aif360). The main
objectives of this toolkit are to help facilitate the transition of fairness
research algorithms to use in an industrial setting and to provide a common
framework for fairness researchers to share and evaluate algorithms.
  The package includes a comprehensive set of fairness metrics for datasets and
models, explanations for these metrics, and algorithms to mitigate bias in
datasets and models. It also includes an interactive Web experience
(https://aif360.mybluemix.net) that provides a gentle introduction to the
concepts and capabilities for line-of-business users, as well as extensive
documentation, usage guidance, and industry-specific tutorials to enable data
scientists and practitioners to incorporate the most appropriate tool for their
problem into their work products. The architecture of the package has been
engineered to conform to a standard paradigm used in data science, thereby
further improving usability for practitioners. Such architectural design and
abstractions enable researchers and developers to extend the toolkit with their
new algorithms and improvements, and to use it for performance benchmarking. A
built-in testing infrastructure maintains code quality.
"
48,A General Framework for Fair Regression,"  Fairness, through its many forms and definitions, has become an important
issue facing the machine learning community. In this work, we consider how to
incorporate group fairness constraints in kernel regression methods, applicable
to Gaussian processes, support vector machines, neural network regression and
decision tree regression. Further, we focus on examining the effect of
incorporating these constraints in decision tree regression, with direct
applications to random forests and boosted trees amongst other widespread
popular inference techniques. We show that the order of complexity of memory
and computation is preserved for such models and tightly bound the expected
perturbations to the model in terms of the number of leaves of the trees.
Importantly, the approach works on trained models and hence can be easily
applied to models in current use and group labels are only required on training
data.
"
49,"Fairness for Whom? Critically reframing fairness with Nash Welfare
  Product","  Recent studies on disparate impact in machine learning applications have
sparked a debate around the concept of fairness along with attempts to
formalize its different criteria. Many of these approaches focus on reducing
prediction errors while maximizing sole utility of the institution. This work
seeks to reconceptualize and critically frame the existing discourse on
fairness by underlining the implicit biases embedded in common understandings
of fairness in the literature and how they contrast with its corresponding
economic and legal definitions. This paper expands the concept of utility and
fairness by bringing in concepts from established literature in welfare
economics and game theory. We then translate these concepts for the algorithmic
prediction domain by defining a formalization of Nash Welfare Product that
seeks to expand utility by collapsing that of the institution using the
prediction tool and the individual subject to the prediction into one function.
We then apply a modulating function that makes the fairness and welfare
trade-offs explicit based on designated policy goals and then apply it to a
temporal model to take into account the effects of decisions beyond the scope
of one-shot predictions. We apply this on a binary classification problem and
present results of a multi-epoch simulation based on the UCI Adult Income
dataset and a test case analysis of the ProPublica recidivism dataset that show
that expanding the concept of utility results in a fairer distribution
correcting for the embedded biases in the dataset without sacrificing the
classifier accuracy.
"
50,"Crowdsourcing with Fairness, Diversity and Budget Constraints","  Recent studies have shown that the labels collected from crowdworkers can be
discriminatory with respect to sensitive attributes such as gender and race.
This raises questions about the suitability of using crowdsourced data for
further use, such as for training machine learning algorithms. In this work, we
address the problem of fair and diverse data collection from a crowd under
budget constraints. We propose a novel algorithm which maximizes the expected
accuracy of the collected data, while ensuring that the errors satisfy desired
notions of fairness. We provide guarantees on the performance of our algorithm
and show that the algorithm performs well in practice through experiments on a
real dataset.
"
51,FairMod - Making Predictive Models Discrimination Aware,"  Predictive models such as decision trees and neural networks may produce
discrimination in their predictions. This paper proposes a method to
post-process the predictions of a predictive model to make the processed
predictions non-discriminatory. The method considers multiple protected
variables together. Multiple protected variables make the problem more
challenging than a simple protected variable. The method uses a well-cited
discrimination metric and adapts it to allow the specification of explanatory
variables, such as position, profession, education, that describe the contexts
of the applications. It models the post-processing of predictions problem as a
nonlinear optimization problem to find best adjustments to the predictions so
that the discrimination constraints of all protected variables are all met at
the same time. The proposed method is independent of classification methods. It
can handle the cases that existing methods cannot handle: satisfying multiple
protected attributes at the same time, allowing multiple explanatory
attributes, and being independent of classification model types. An evaluation
using four real world data sets shows that the proposed method is as
effectively as existing methods, in addition to its extra power.
"
52,"How Do Fairness Definitions Fare? Examining Public Attitudes Towards
  Algorithmic Definitions of Fairness","  What is the best way to define algorithmic fairness? While many definitions
of fairness have been proposed in the computer science literature, there is no
clear agreement over a particular definition. In this work, we investigate
ordinary people's perceptions of three of these fairness definitions. Across
two online experiments, we test which definitions people perceive to be the
fairest in the context of loan decisions, and whether fairness perceptions
change with the addition of sensitive information (i.e., race of the loan
applicants). Overall, one definition (calibrated fairness) tends to be more
preferred than the others, and the results also provide support for the
principle of affirmative action.
"
53,Aequitas: A Bias and Fairness Audit Toolkit,"  Recent work has raised concerns on the risk of unintended bias in AI systems
being used nowadays that can affect individuals unfairly based on race, gender
or religion, among other possible characteristics. While a lot of bias metrics
and fairness definitions have been proposed in recent years, there is no
consensus on which metric/definition should be used and there are very few
available resources to operationalize them. Therefore, despite recent
awareness, auditing for bias and fairness when developing and deploying AI
systems is not yet a standard practice. We present Aequitas, an open source
bias and fairness audit toolkit that is an intuitive and easy to use addition
to the machine learning workflow, enabling users to seamlessly test models for
several bias and fairness metrics in relation to multiple population
sub-groups. Aequitas facilitates informed and equitable decisions around
developing and deploying algorithmic decision making systems for both data
scientists, machine learning researchers and policymakers.
"
54,Bayesian Modeling of Intersectional Fairness: The Variance of Bias,"  Intersectionality is a framework that analyzes how interlocking systems of
power and oppression affect individuals along overlapping dimensions including
race, gender, sexual orientation, class, and disability. Intersectionality
theory therefore implies it is important that fairness in artificial
intelligence systems be protected with regard to multi-dimensional protected
attributes. However, the measurement of fairness becomes statistically
challenging in the multi-dimensional setting due to data sparsity, which
increases rapidly in the number of dimensions, and in the values per dimension.
We present a Bayesian probabilistic modeling approach for the reliable,
data-efficient estimation of fairness with multi-dimensional protected
attributes, which we apply to two existing intersectional fairness metrics.
Experimental results on census data and the COMPAS criminal justice recidivism
dataset demonstrate the utility of our methodology, and show that Bayesian
methods are valuable for the modeling and measurement of fairness in an
intersectional context.
"
55,Intersectionality: Multiple Group Fairness in Expectation Constraints,"  Group fairness is an important concern for machine learning researchers,
developers, and regulators. However, the strictness to which models must be
constrained to be considered fair is still under debate. The focus of this work
is on constraining the expected outcome of subpopulations in kernel regression
and, in particular, decision tree regression, with application to random
forests, boosted trees and other ensemble models. While individual constraints
were previously addressed, this work addresses concerns about incorporating
multiple constraints simultaneously. The proposed solution does not affect the
order of computational or memory complexity of the decision trees and is easily
integrated into models post training.
"
56,50 Years of Test (Un)fairness: Lessons for Machine Learning,"  Quantitative definitions of what is unfair and what is fair have been
introduced in multiple disciplines for well over 50 years, including in
education, hiring, and machine learning. We trace how the notion of fairness
has been defined within the testing communities of education and hiring over
the past half century, exploring the cultural and social context in which
different fairness definitions have emerged. In some cases, earlier definitions
of fairness are similar or identical to definitions of fairness in current
machine learning research, and foreshadow current formal work. In other cases,
insights into what fairness means and how to measure it have largely gone
overlooked. We compare past and current notions of fairness along several
dimensions, including the fairness criteria, the focus of the criteria (e.g., a
test, a model, or its use), the relationship of fairness to individuals,
groups, and subgroups, and the mathematical method for measuring fairness
(e.g., classification, regression). This work points the way towards future
research and measurement of (un)fairness that builds from our modern
understanding of fairness while incorporating insights from the past.
"
57,AI Fairness for People with Disabilities: Point of View,"  We consider how fair treatment in society for people with disabilities might
be impacted by the rise in the use of artificial intelligence, and especially
machine learning methods. We argue that fairness for people with disabilities
is different to fairness for other protected attributes such as age, gender or
race. One major difference is the extreme diversity of ways disabilities
manifest, and people adapt. Secondly, disability information is highly
sensitive and not always shared, precisely because of the potential for
discrimination. Given these differences, we explore definitions of fairness and
how well they work in the disability space. Finally, we suggest ways of
approaching fairness for people with disabilities in AI applications.
"
58,Probabilistic Verification of Fairness Properties via Concentration,"  As machine learning systems are increasingly used to make real world legal
and financial decisions, it is of paramount importance that we develop
algorithms to verify that these systems do not discriminate against minorities.
We design a scalable algorithm for verifying fairness specifications. Our
algorithm obtains strong correctness guarantees based on adaptive concentration
inequalities; such inequalities enable our algorithm to adaptively take samples
until it has enough data to make a decision. We implement our algorithm in a
tool called VeriFair, and show that it scales to large machine learning models,
including a deep recurrent neural network that is more than five orders of
magnitude larger than the largest previously-verified neural network. While our
technique only gives probabilistic guarantees due to the use of random samples,
we show that we can choose the probability of error to be extremely small.
"
59,Learning Controllable Fair Representations,"  Learning data representations that are transferable and are fair with respect
to certain protected attributes is crucial to reducing unfair decisions while
preserving the utility of the data. We propose an information-theoretically
motivated objective for learning maximally expressive representations subject
to fairness constraints. We demonstrate that a range of existing approaches
optimize approximations to the Lagrangian dual of our objective. In contrast to
these existing approaches, our objective allows the user to control the
fairness of the representations by specifying limits on unfairness. Exploiting
duality, we introduce a method that optimizes the model parameters as well as
the expressiveness-fairness trade-off. Empirical evidence suggests that our
proposed method can balance the trade-off between multiple notions of fairness
and achieves higher expressiveness at a lower computational cost.
"
60,"Putting Fairness Principles into Practice: Challenges, Metrics, and
  Improvements","  As more researchers have become aware of and passionate about algorithmic
fairness, there has been an explosion in papers laying out new metrics,
suggesting algorithms to address issues, and calling attention to issues in
existing applications of machine learning. This research has greatly expanded
our understanding of the concerns and challenges in deploying machine learning,
but there has been much less work in seeing how the rubber meets the road.
  In this paper we provide a case-study on the application of fairness in
machine learning research to a production classification system, and offer new
insights in how to measure and address algorithmic fairness issues. We discuss
open questions in implementing equality of opportunity and describe our
fairness metric, conditional equality, that takes into account distributional
differences. Further, we provide a new approach to improve on the fairness
metric during model training and demonstrate its efficacy in improving
performance for a real-world product
"
61,Noise-tolerant fair classification,"  Fairness-aware learning involves designing algorithms that do not
discriminate with respect to some sensitive feature (e.g., race or gender).
Existing work on the problem operates under the assumption that the sensitive
feature available in one's training sample is perfectly reliable. This
assumption may be violated in many real-world cases: for example, respondents
to a survey may choose to conceal or obfuscate their group identity out of fear
of potential discrimination. This poses the question of whether one can still
learn fair classifiers given noisy sensitive features. In this paper, we answer
the question in the affirmative: we show that if one measures fairness using
the mean-difference score, and sensitive features are subject to noise from the
mutually contaminated learning model, then owing to a simple identity we only
need to change the desired fairness-tolerance. The requisite tolerance can be
estimated by leveraging existing noise-rate estimators from the label noise
literature. We finally show that our procedure is empirically effective on two
case-studies involving sensitive feature censoring.
"
62,Stable and Fair Classification,"  Fair classification has been a topic of intense study in machine learning,
and several algorithms have been proposed towards this important task. However,
in a recent study, Friedler et al. observed that fair classification algorithms
may not be stable with respect to variations in the training dataset -- a
crucial consideration in several real-world applications. Motivated by their
work, we study the problem of designing classification algorithms that are both
fair and stable. We propose an extended framework based on fair classification
algorithms that are formulated as optimization problems, by introducing a
stability-focused regularization term. Theoretically, we prove a stability
guarantee, that was lacking in fair classification algorithms, and also provide
an accuracy guarantee for our extended framework. Our accuracy guarantee can be
used to inform the selection of the regularization parameter in our framework.
To the best of our knowledge, this is the first work that combines stability
and fairness in automated decision-making tasks. We assess the benefits of our
approach empirically by extending several fair classification algorithms that
are shown to achieve the best balance between fairness and accuracy over the
Adult dataset. Our empirical results show that our framework indeed improves
the stability at only a slight sacrifice in accuracy.
"
63,Capuchin: Causal Database Repair for Algorithmic Fairness,"  Fairness is increasingly recognized as a critical component of machine
learning systems. However, it is the underlying data on which these systems are
trained that often reflect discrimination, suggesting a database repair
problem. Existing treatments of fairness rely on statistical correlations that
can be fooled by statistical anomalies, such as Simpson's paradox. Proposals
for causality-based definitions of fairness can correctly model some of these
situations, but they require specification of the underlying causal models. In
this paper, we formalize the situation as a database repair problem, proving
sufficient conditions for fair classifiers in terms of admissible variables as
opposed to a complete causal model. We show that these conditions correctly
capture subtle fairness violations. We then use these conditions as the basis
for database repair algorithms that provide provable fairness guarantees about
classifiers trained on their training labels. We evaluate our algorithms on
real data, demonstrating improvement over the state of the art on multiple
fairness metrics proposed in the literature while retaining high utility.
"
64,Fairness in Recommendation Ranking through Pairwise Comparisons,"  Recommender systems are one of the most pervasive applications of machine
learning in industry, with many services using them to match users to products
or information. As such it is important to ask: what are the possible fairness
risks, how can we quantify them, and how should we address them? In this paper
we offer a set of novel metrics for evaluating algorithmic fairness concerns in
recommender systems. In particular we show how measuring fairness based on
pairwise comparisons from randomized experiments provides a tractable means to
reason about fairness in rankings from recommender systems. Building on this
metric, we offer a new regularizer to encourage improving this metric during
model training and thus improve fairness in the resulting rankings. We apply
this pairwise regularization to a large-scale, production recommender system
and show that we are able to significantly improve the system's pairwise
fairness.
"
65,"On the Long-term Impact of Algorithmic Decision Policies: Effort
  Unfairness and Feature Segregation through Social Learning","  Most existing notions of algorithmic fairness are one-shot: they ensure some
form of allocative equality at the time of decision making, but do not account
for the adverse impact of the algorithmic decisions today on the long-term
welfare and prosperity of certain segments of the population. We take a broader
perspective on algorithmic fairness. We propose an effort-based measure of
fairness and present a data-driven framework for characterizing the long-term
impact of algorithmic policies on reshaping the underlying population.
Motivated by the psychological literature on \emph{social learning} and the
economic literature on equality of opportunity, we propose a micro-scale model
of how individuals may respond to decision-making algorithms. We employ
existing measures of segregation from sociology and economics to quantify the
resulting macro-scale population-level change. Importantly, we observe that
different models may shift the group-conditional distribution of qualifications
in different directions. Our findings raise a number of important questions
regarding the formalization of fairness for decision-making models.
"
66,"Leveling the Playing Field -- Fairness in AI Versus Human Game
  Benchmarks","  From the beginning if the history of AI, there has been interest in games as
a platform of research. As the field developed, human-level competence in
complex games became a target researchers worked to reach. Only relatively
recently has this target been finally met for traditional tabletop games such
as Backgammon, Chess and Go. Current research focus has shifted to electronic
games, which provide unique challenges. As is often the case with AI research,
these results are liable to be exaggerated or misrepresented by either authors
or third parties. The extent to which these games benchmark consist of fair
competition between human and AI is also a matter of debate. In this work, we
review the statements made by authors and third parties in the general media
and academic circle about these game benchmark results and discuss factors that
can impact the perception of fairness in the contest between humans and
machines
"
67,Fair Classification and Social Welfare,"  Now that machine learning algorithms lie at the center of many resource
allocation pipelines, computer scientists have been unwittingly cast as partial
social planners. Given this state of affairs, important questions follow. What
is the relationship between fairness as defined by computer scientists and
notions of social welfare? In this paper, we present a welfare-based analysis
of classification and fairness regimes. We translate a loss minimization
program into a social welfare maximization problem with a set of implied
welfare weights on individuals and groups--weights that can be analyzed from a
distribution justice lens. In the converse direction, we ask what the space of
possible labelings is for a given dataset and hypothesis class. We provide an
algorithm that answers this question with respect to linear hyperplanes in
$\mathbb{R}^d$ that runs in $O(n^dd)$. Our main findings on the relationship
between fairness criteria and welfare center on sensitivity analyses of
fairness-constrained empirical risk minimization programs. We characterize the
ranges of $\Delta \epsilon$ perturbations to a fairness parameter $\epsilon$
that yield better, worse, and neutral outcomes in utility for individuals and
by extension, groups. We show that applying more strict fairness criteria that
are codified as parity constraints, can worsen welfare outcomes for both
groups. More generally, always preferring ""more fair"" classifiers does not
abide by the Pareto Principle---a fundamental axiom of social choice theory and
welfare economics. Recent work in machine learning has rallied around these
notions of fairness as critical to ensuring that algorithmic systems do not
have disparate negative impact on disadvantaged social groups. By showing that
these constraints often fail to translate into improved outcomes for these
groups, we cast doubt on their effectiveness as a means to ensure justice.
"
68,Fairness in Machine Learning with Tractable Models,"  Machine Learning techniques have become pervasive across a range of different
applications, and are now widely used in areas as disparate as recidivism
prediction, consumer credit-risk analysis and insurance pricing. The prevalence
of machine learning techniques has raised concerns about the potential for
learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning
about probabilistic events is a challenging one, owing to the intractability of
inference.
  The focus of this paper is taking steps towards the application of tractable
models to fairness. Tractable probabilistic models have emerged that guarantee
that conditional marginal can be computed in time linear in the size of the
model. In particular, we show that sum product networks (SPNs) enable an
effective technique for determining the statistical relationships between
protected attributes and other training variables. If a subset of these
training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train
a classification model without concern that the resulting classifier will
result in disparate outcomes for different demographic groups.
  Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and
female credit applicants, with a small reduction in classification accuracy
compared to state of the art. We will also motivate the concept of ""fairness
through percentile equivalence"", a new definition predicated on the notion that
individuals at the same percentile of their respective distributions should be
treated equivalently, and this prevents unfair penalisation of those
individuals who lie at the extremities of their respective distributions.
"
69,Compositional Fairness Constraints for Graph Embeddings,"  Learning high-quality node embeddings is a key building block for machine
learning models that operate on graph data, such as social networks and
recommender systems. However, existing graph embedding techniques are unable to
cope with fairness constraints, e.g., ensuring that the learned representations
do not correlate with certain attributes, such as age or gender. Here, we
introduce an adversarial framework to enforce fairness constraints on graph
embeddings. Our approach is compositional---meaning that it can flexibly
accommodate different combinations of fairness constraints during inference.
For instance, in the context of social recommendations, our framework would
allow one user to request that their recommendations are invariant to both
their age and gender, while also allowing another user to request invariance to
just their age. Experiments on standard knowledge graph and recommender system
benchmarks highlight the utility of our proposed framework.
"
70,Fairness and Missing Values,"  The causes underlying unfair decision making are complex, being internalised
in different ways by decision makers, other actors dealing with data and
models, and ultimately by the individuals being affected by these decisions.
One frequent manifestation of all these latent causes arises in the form of
missing values: protected groups are more reluctant to give information that
could be used against them, delicate information for some groups can be erased
by human operators, or data acquisition may simply be less complete and
systematic for minority groups. As a result, missing values and bias in data
are two phenomena that are tightly coupled. However, most recent techniques,
libraries and experimental results dealing with fairness in machine learning
have simply ignored missing data. In this paper, we claim that fairness
research should not miss the opportunity to deal properly with missing data. To
support this claim, (1) we analyse the sources of missing data and bias, and we
map the common causes, (2) we find that rows containing missing values are
usually fairer than the rest, which should not be treated as the uncomfortable
ugly data that different techniques and libraries get rid of at the first
occasion, and (3) we study the trade-off between performance and fairness when
the rows with missing values are used (either because the technique deals with
them directly or by imputation methods). We end the paper with a series of
recommended procedures about what to do with missing data when aiming for fair
decision making.
"
71,"Achieving Fairness in Determining Medicaid Eligibility through Fairgroup
  Construction","  Effective complements to human judgment, artificial intelligence techniques
have started to aid human decisions in complicated social problems across the
world. In the context of United States for instance, automated ML/DL
classification models offer complements to human decisions in determining
Medicaid eligibility. However, given the limitations in ML/DL model design,
these algorithms may fail to leverage various factors for decision making,
resulting in improper decisions that allocate resources to individuals who may
not be in the most need. In view of such an issue, we propose in this paper the
method of \textit{fairgroup construction}, based on the legal doctrine of
\textit{disparate impact}, to improve the fairness of regressive classifiers.
Experiments on American Community Survey dataset demonstrate that our method
could be easily adapted to a variety of regressive classification models to
boost their fairness in deciding Medicaid Eligibility, while maintaining high
levels of classification accuracy.
"
72,Towards Fair and Privacy-Preserving Federated Deep Models,"  The current standalone deep learning framework tends to result in overfitting
and low utility. This problem can be addressed by either a centralized
framework that deploys a central server to train a global model on the joint
data from all parties, or a distributed framework that leverages a parameter
server to aggregate local model updates. Server-based solutions are prone to
the problem of a single-point-of-failure. In this respect, collaborative
learning frameworks, such as federated learning (FL), are more robust. Existing
federated learning frameworks overlook an important aspect of participation:
fairness. All parties are given the same final model without regard to their
contributions. To address these issues, we propose a decentralized Fair and
Privacy-Preserving Deep Learning (FPPDL) framework to incorporate fairness into
federated deep learning models. In particular, we design a local credibility
mutual evaluation mechanism to guarantee fairness, and a three-layer
onion-style encryption scheme to guarantee both accuracy and privacy. Different
from existing FL paradigm, under FPPDL, each participant receives a different
version of the FL model with performance commensurate with his contributions.
Experiments on benchmark datasets demonstrate that FPPDL balances fairness,
privacy and accuracy. It enables federated learning ecosystems to detect and
isolate low-contribution parties, thereby promoting responsible participation.
"
73,Flexibly Fair Representation Learning by Disentanglement,"  We consider the problem of learning representations that achieve group and
subgroup fairness with respect to multiple sensitive attributes. Taking
inspiration from the disentangled representation learning literature, we
propose an algorithm for learning compact representations of datasets that are
useful for reconstruction and prediction, but are also \emph{flexibly fair},
meaning they can be easily modified at test time to achieve subgroup
demographic parity with respect to multiple sensitive attributes and their
conjunctions. We show empirically that the resulting encoder---which does not
require the sensitive attributes for inference---enables the adaptation of a
single representation to a variety of fair classification tasks with new target
labels and subgroup definitions.
"
74,Fair Division Without Disparate Impact,"  We consider the problem of dividing items between individuals in a way that
is fair both in the sense of distributional fairness and in the sense of not
having disparate impact across protected classes. An important existing
mechanism for distributionally fair division is competitive equilibrium from
equal incomes (CEEI). Unfortunately, CEEI will not, in general, respect
disparate impact constraints. We consider two types of disparate impact
measures: requiring that allocations be similar across protected classes and
requiring that average utility levels be similar across protected classes. We
modify the standard CEEI algorithm in two ways: equitable equilibrium from
equal incomes, which removes disparate impact in allocations, and competitive
equilibrium from equitable incomes which removes disparate impact in attained
utility levels. We show analytically that removing disparate impact in outcomes
breaks several of CEEI's desirable properties such as envy, regret, Pareto
optimality, and incentive compatibility. By contrast, we can remove disparate
impact in attained utility levels without affecting these properties. Finally,
we experimentally evaluate the tradeoffs between efficiency, equity, and
disparate impact in a recommender-system based market.
"
75,"Learning Fair Naive Bayes Classifiers by Discovering and Eliminating
  Discrimination Patterns","  As machine learning is increasingly used to make real-world decisions, recent
research efforts aim to define and ensure fairness in algorithmic decision
making. Existing methods often assume a fixed set of observable features to
define individuals, but lack a discussion of certain features not being
observed at test time. In this paper, we study fairness of naive Bayes
classifiers, which allow partial observations. In particular, we introduce the
notion of a discrimination pattern, which refers to an individual receiving
different classifications depending on whether some sensitive attributes were
observed. Then a model is considered fair if it has no such pattern. We propose
an algorithm to discover and mine for discrimination patterns in a naive Bayes
classifier, and show how to learn maximum likelihood parameters subject to
these fairness constraints. Our approach iteratively discovers and eliminates
discrimination patterns until a fair model is learned. An empirical evaluation
on three real-world datasets demonstrates that we can remove exponentially many
discrimination patterns by only adding a small fraction of them as constraints.
"
76,"FaRM: Fair Reward Mechanism for Information Aggregation in Spontaneous
  Localized Settings (Extended Version)","  Although peer prediction markets are widely used in crowdsourcing to
aggregate information from agents, they often fail to reward the participating
agents equitably. Honest agents can be wrongly penalized if randomly paired
with dishonest ones. In this work, we introduce \emph{selective} and
\emph{cumulative} fairness. We characterize a mechanism as fair if it satisfies
both notions and present FaRM, a representative mechanism we designed. FaRM is
a Nash incentive mechanism that focuses on information aggregation for
spontaneous local activities which are accessible to a limited number of agents
without assuming any prior knowledge of the event. All the agents in the
vicinity observe the same information. FaRM uses \textit{(i)} a \emph{report
strength score} to remove the risk of random pairing with dishonest reporters,
\textit{(ii)} a \emph{consistency score} to measure an agent's history of
accurate reports and distinguish valuable reports, \textit{(iii)} a
\emph{reliability score} to estimate the probability of an agent to collude
with nearby agents and prevents agents from getting swayed, and \textit{(iv)} a
\emph{location robustness score} to filter agents who try to participate
without being present in the considered setting. Together, report strength,
consistency, and reliability represent a fair reward given to agents based on
their reports.
"
77,Inherent Tradeoffs in Learning Fair Representations,"  With the prevalence of machine learning in high-stakes applications,
especially the ones regulated by anti-discrimination laws or societal norms, it
is crucial to ensure that the predictive models do not propagate any existing
bias or discrimination. Due to the ability of deep neural nets to learn rich
representations, recent advances in algorithmic fairness have focused on
learning fair representations with adversarial techniques to reduce bias in
data while preserving utility simultaneously. In this paper, through the lens
of information theory, we provide the first result that quantitatively
characterizes the tradeoff between demographic parity and the joint utility
across different population groups. Specifically, when the base rates differ
between groups, we show that any method aiming to learn fair representations
admits an information-theoretic lower bound on the joint error across these
groups. To complement our negative results, we also prove that if the optimal
decision functions across different groups are close, then learning fair
representations leads to an alternative notion of fairness, known as the
accuracy parity, which states that the error rates are close between groups.
Finally, our theoretical findings are also confirmed empirically on real-world
datasets.
"
78,Fairness criteria through the lens of directed acyclic graphical models,"  A substantial portion of the literature on fairness in algorithms proposes,
analyzes, and operationalizes simple formulaic criteria for assessing fairness.
Two of these criteria, Equalized Odds and Calibration by Group, have gained
significant attention for their simplicity and intuitive appeal, but also for
their incompatibility. This chapter provides a perspective on the meaning and
consequences of these and other fairness criteria using graphical models which
reveals Equalized Odds and related criteria to be ultimately misleading. An
assessment of various graphical models suggests that fairness criteria should
ultimately be case-specific and sensitive to the nature of the information the
algorithm processes.
"
79,"Reinforcement Learning with Fairness Constraints for Resource
  Distribution in Human-Robot Teams","  Much work in robotics and operations research has focused on optimal resource
distribution, where an agent dynamically decides how to sequentially distribute
resources among different candidates. However, most work ignores the notion of
fairness in candidate selection. In the case where a robot distributes
resources to human team members, disproportionately favoring the highest
performing teammate can have negative effects in team dynamics and system
acceptance. We introduce a multi-armed bandit algorithm with fairness
constraints, where a robot distributes resources to human teammates of
different skill levels. In this problem, the robot does not know the skill
level of each human teammate, but learns it by observing their performance over
time. We define fairness as a constraint on the minimum rate that each human
teammate is selected throughout the task. We provide theoretical guarantees on
performance and perform a large-scale user study, where we adjust the level of
fairness in our algorithm. Results show that fairness in resource distribution
has a significant effect on users' trust in the system.
"
80,"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural
  Architecture Search","  One of the most critical problems in two-stage weight-sharing neural
architecture search is the evaluation of candidate models. A faithful ranking
certainly leads to accurate searching results. However, current methods are
prone to making misjudgments. In this paper, we prove that they inevitably give
biased evaluations due to inherent unfairness in the supernet training. In view
of this, we propose two levels of constraints: expectation fairness and strict
fairness. Particularly, strict fairness ensures equal optimization
opportunities for all choice blocks throughout the training, which neither
overestimates nor underestimates their capacity. We demonstrate this is crucial
to improving confidence in models' ranking. Incorporating our supernet trained
under fairness constraints with a multi-objective evolutionary search
algorithm, we obtain various state-of-the-art models on ImageNet. Especially,
FairNAS-A attains 77.5% top-1 accuracy. The models and their evaluation codes
are made publicly available online http://github.com/fairnas/FairNAS .
"
81,Toward Fairness in AI for People with Disabilities: A Research Roadmap,"  AI technologies have the potential to dramatically impact the lives of people
with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for
many state-of-the-art AI systems, such as automated speech recognition tools
that can caption videos for people who are deaf and hard of hearing, or
language prediction algorithms that can augment communication for people with
speech or cognitive disabilities. However, widely deployed AI systems may not
work properly for PWD, or worse, may actively discriminate against them. These
considerations regarding fairness in AI for PWD have thus far received little
attention. In this position paper, we identify potential areas of concern
regarding how several AI technology categories may impact particular disability
constituencies if care is not taken in their design, development, and testing.
We intend for this risk assessment of how various classes of AI might interact
with various classes of disability to provide a roadmap for future research
that is needed to gather data, test these hypotheses, and build more inclusive
algorithms.
"
82,"FairST: Equitable Spatial and Temporal Demand Prediction for New
  Mobility Systems","  Emerging transportation modes, including car-sharing, bike-sharing, and
ride-hailing, are transforming urban mobility but have been shown to reinforce
socioeconomic inequities. Spatiotemporal demand prediction models for these new
mobility regimes must therefore consider fairness as a first-class design
requirement. We present FairST, a fairness-aware model for predicting demand
for new mobility systems. Our approach utilizes 1D, 2D and 3D convolutions to
integrate various urban features and learn the spatial-temporal dynamics of a
mobility system, but we include fairness metrics as a form of regularization to
make the predictions more equitable across demographic groups. We propose two
novel spatiotemporal fairness metrics, a region-based fairness gap (RFG) and an
individual-based fairness gap (IFG). Both quantify equity in a spatiotemporal
context, but vary by whether demographics are labeled at the region level (RFG)
or whether population distribution information is available (IFG). Experimental
results on real bike share and ride share datasets demonstrate the
effectiveness of the proposed model: FairST not only reduces the fairness gap
by more than 80%, but can surprisingly achieve better accuracy than
state-of-the-art yet fairness-oblivious methods including LSTMs, ConvLSTMs, and
3D CNN.
"
83,Fairness-enhancing interventions in stream classification,"  The wide spread usage of automated data-driven decision support systems has
raised a lot of concerns regarding accountability and fairness of the employed
models in the absence of human supervision. Existing fairness-aware approaches
tackle fairness as a batch learning problem and aim at learning a fair model
which can then be applied to future instances of the problem. In many
applications, however, the data comes sequentially and its characteristics
might evolve with time. In such a setting, it is counter-intuitive to ""fix"" a
(fair) model over the data stream as changes in the data might incur changes in
the underlying model therefore, affecting its fairness. In this work, we
propose fairness-enhancing interventions that modify the input data so that the
outcome of any stream classifier applied to that data will be fair. Experiments
on real and synthetic data show that our approach achieves good predictive
performance and low discrimination scores over the course of the stream.
"
84,FAHT: An Adaptive Fairness-aware Decision Tree Classifier,"  Automated data-driven decision-making systems are ubiquitous across a wide
spread of online as well as offline services. These systems, depend on
sophisticated learning algorithms and available data, to optimize the service
function for decision support assistance. However, there is a growing concern
about the accountability and fairness of the employed models by the fact that
often the available historic data is intrinsically discriminatory, i.e., the
proportion of members sharing one or more sensitive attributes is higher than
the proportion in the population as a whole when receiving positive
classification, which leads to a lack of fairness in decision support system. A
number of fairness-aware learning methods have been proposed to handle this
concern. However, these methods tackle fairness as a static problem and do not
take the evolution of the underlying stream population into consideration. In
this paper, we introduce a learning mechanism to design a fair classifier for
online stream based decision-making. Our learning model, FAHT (Fairness-Aware
Hoeffding Tree), is an extension of the well-known Hoeffding Tree algorithm for
decision tree induction over streams, that also accounts for fairness. Our
experiments show that our algorithm is able to deal with discrimination in
streaming environments, while maintaining a moderate predictive performance
over the stream.
"
85,Fairness in Reinforcement Learning,"  Decision support systems (e.g., for ecological conservation) and autonomous
systems (e.g., adaptive controllers in smart cities) start to be deployed in
real applications. Although their operations often impact many users or
stakeholders, no fairness consideration is generally taken into account in
their design, which could lead to completely unfair outcomes for some users or
stakeholders. To tackle this issue, we advocate for the use of social welfare
functions that encode fairness and present this general novel problem in the
context of (deep) reinforcement learning, although it could possibly be
extended to other machine learning tasks.
"
86,"What is the Point of Fairness? Disability, AI and The Complexity of
  Justice","  Work integrating conversations around AI and Disability is vital and valued,
particularly when done through a lens of fairness. Yet at the same time,
analyzing the ethical implications of AI for disabled people solely through the
lens of a singular idea of ""fairness"" risks reinforcing existing power
dynamics, either through reinforcing the position of existing medical
gatekeepers, or promoting tools and techniques that benefit
otherwise-privileged disabled people while harming those who are rendered
outliers in multiple ways. In this paper we present two case studies from
within computer vision - a subdiscipline of AI focused on training algorithms
that can ""see"" - of technologies putatively intended to help disabled people
but, through failures to consider structural injustices in their design, are
likely to result in harms not addressed by a ""fairness"" framing of ethics.
Drawing on disability studies and critical data science, we call on researchers
into AI ethics and disability to move beyond simplistic notions of fairness,
and towards notions of justice.
"
87,Fairness in Deep Learning: A Computational Perspective,"  Deep learning is increasingly being used in high-stake decision making
applications that affect individual lives. However, deep learning models might
exhibit algorithmic discrimination behaviors with respect to protected groups,
potentially posing negative impacts on individuals and society. Therefore,
fairness in deep learning has attracted tremendous attention recently. We
provide a review covering recent progresses to tackle algorithmic fairness
problems of deep learning from the computational perspective. Specifically, we
show that interpretability can serve as a useful ingredient to diagnose the
reasons that lead to algorithmic discrimination. We also discuss fairness
mitigation approaches categorized according to three stages of deep learning
life-cycle, aiming to push forward the area of fairness in deep learning and
build genuinely fair and reliable deep learning systems.
"
88,Fairness-Aware Process Mining,"  Process mining is a multi-purpose tool enabling organizations to improve
their processes. One of the primary purposes of process mining is finding the
root causes of performance or compliance problems in processes. The usual way
of doing so is by gathering data from the process event log and other sources
and then applying some data mining and machine learning techniques. However,
the results of applying such techniques are not always acceptable. In many
situations, this approach is prone to making obvious or unfair diagnoses and
applying them may result in conclusions that are unsurprising or even
discriminating (e.g., blaming overloaded employees for delays). In this paper,
we present a solution to this problem by creating a fair classifier for such
situations. The undesired effects are removed at the expense of reduction on
the accuracy of the resulting classifier. We have implemented this method as a
plug-in in ProM. Using the implemented plug-in on two real event logs, we
decreased the discrimination caused by the classifier, while losing a small
fraction of its accuracy.
"
89,Quantifying Infra-Marginality and Its Trade-off with Group Fairness,"  In critical decision-making scenarios, optimizing accuracy can lead to a
biased classifier, hence past work recommends enforcing group-based fairness
metrics in addition to maximizing accuracy. However, doing so exposes the
classifier to another kind of bias called infra-marginality. This refers to
individual-level bias where some individuals/subgroups can be worse off than
under simply optimizing for accuracy. For instance, a classifier implementing
race-based parity may significantly disadvantage women of the advantaged race.
To quantify this bias, we propose a general notion of $\eta$-infra-marginality
that can be used to evaluate the extent of this bias. We prove theoretically
that, unlike other fairness metrics, infra-marginality does not have a
trade-off with accuracy: high accuracy directly leads to low infra-marginality.
This observation is confirmed through empirical analysis on multiple simulated
and real-world datasets. Further, we find that maximizing group fairness often
increases infra-marginality, suggesting the consideration of both group-level
fairness and individual-level infra-marginality. However, measuring
infra-marginality requires knowledge of the true distribution of
individual-level outcomes correctly and explicitly. We propose a practical
method to measure infra-marginality, and a simple algorithm to maximize
group-wise accuracy and avoid infra-marginality.
"
90,Avoiding Resentment Via Monotonic Fairness,"  Classifiers that achieve demographic balance by explicitly using protected
attributes such as race or gender are often politically or culturally
controversial due to their lack of individual fairness, i.e. individuals with
similar qualifications will receive different outcomes. Individually and group
fair decision criteria can produce counter-intuitive results, e.g. that the
optimal constrained boundary may reject intuitively better candidates due to
demographic imbalance in similar candidates. Both approaches can be seen as
introducing individual resentment, where some individuals would have received a
better outcome if they either belonged to a different demographic class and had
the same qualifications, or if they remained in the same class but had
objectively worse qualifications (e.g. lower test scores). We show that both
forms of resentment can be avoided by using monotonically constrained machine
learning models to create individually fair, demographically balanced
classifiers.
"
91,"FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability
  and Transparency","  Machine learning algorithms can take important decisions, sometimes legally
binding, about our everyday life. In most cases, however, these systems and
decisions are neither regulated nor certified. Given the potential harm that
these algorithms can cause, qualities such as fairness, accountability and
transparency of predictive systems are of paramount importance. Recent
literature suggested voluntary self-reporting on these aspects of predictive
systems -- e.g., data sheets for data sets -- but their scope is often limited
to a single component of a machine learning pipeline, and producing them
requires manual labour. To resolve this impasse and ensure high-quality, fair,
transparent and reliable machine learning systems, we developed an open source
toolbox that can inspect selected fairness, accountability and transparency
aspects of these systems to automatically and objectively report them back to
their engineers and users. We describe design, scope and usage examples of this
Python toolbox in this paper. The toolbox provides functionality for inspecting
fairness, accountability and transparency of all aspects of the machine
learning process: data (and their features), models and predictions. It is
available to the public under the BSD 3-Clause open source licence.
"
92,Causal Modeling for Fairness in Dynamical Systems,"  In many application areas---lending, education, and online recommenders, for
example---fairness and equity concerns emerge when a machine learning system
interacts with a dynamically changing environment to produce both immediate and
long-term effects for individuals and demographic groups. We discuss causal
directed acyclic graphs (DAGs) as a unifying framework for the recent
literature on fairness in such dynamical systems. We show that this formulation
affords several new directions of inquiry to the modeler, where causal
assumptions can be expressed and manipulated. We emphasize the importance of
computing interventional quantities in the dynamical fairness setting, and show
how causal assumptions enable simulation (when environment dynamics are known)
and off-policy estimation (when dynamics are unknown) of intervention on short-
and long-term outcomes, at both the group and individual levels.
"
93,Group-based Fair Learning Leads to Counter-intuitive Predictions,"  A number of machine learning (ML) methods have been proposed recently to
maximize model predictive accuracy while enforcing notions of group parity or
fairness across sub-populations. We propose a desirable property for these
procedures, slack-consistency: For any individual, the predictions of the model
should be monotonic with respect to allowed slack (i.e., maximum allowed
group-parity violation). Such monotonicity can be useful for individuals to
understand the impact of enforcing fairness on their predictions. Surprisingly,
we find that standard ML methods for enforcing fairness violate this basic
property. Moreover, this undesirable behavior arises in situations agnostic to
the complexity of the underlying model or approximate optimizations, suggesting
that the simple act of incorporating a constraint can lead to drastically
unintended behavior in ML. We present a simple theoretical method for enforcing
slack-consistency, while encouraging further discussions on the unintended
behaviors potentially induced when enforcing group-based parity.
"
94,The Impact of Data Preparation on the Fairness of Software Systems,"  Machine learning models are widely adopted in scenarios that directly affect
people. The development of software systems based on these models raises
societal and legal concerns, as their decisions may lead to the unfair
treatment of individuals based on attributes like race or gender. Data
preparation is key in any machine learning pipeline, but its effect on fairness
is yet to be studied in detail. In this paper, we evaluate how the fairness and
effectiveness of the learned models are affected by the removal of the
sensitive attribute, the encoding of the categorical attributes, and instance
selection methods (including cross-validators and random undersampling). We
used the Adult Income and the German Credit Data datasets, which are widely
studied and known to have fairness concerns. We applied each data preparation
technique individually to analyse the difference in predictive performance and
fairness, using statistical parity difference, disparate impact, and the
normalised prejudice index. The results show that fairness is affected by
transformations made to the training data, particularly in imbalanced datasets.
Removing the sensitive attribute is insufficient to eliminate all the
unfairness in the predictions, as expected, but it is key to achieve fairer
models. Additionally, the standard random undersampling with respect to the
true labels is sometimes more prejudicial than performing no random
undersampling.
"
95,Fairness in Clustering with Multiple Sensitive Attributes,"  A clustering may be considered as fair on pre-specified sensitive attributes
if the proportions of sensitive attribute groups in each cluster reflect that
in the dataset. In this paper, we consider the task of fair clustering for
scenarios involving multiple multi-valued or numeric sensitive attributes. We
propose a fair clustering method, \textit{FairKM} (Fair K-Means), that is
inspired by the popular K-Means clustering formulation. We outline a
computational notion of fairness which is used along with a cluster coherence
objective, to yield the FairKM clustering method. We empirically evaluate our
approach, wherein we quantify both the quality and fairness of clusters, over
real-world datasets. Our experimental evaluation illustrates that the clusters
generated by FairKM fare significantly better on both clustering quality and
fair representation of sensitive attribute groups compared to the clusters from
a state-of-the-art baseline fair clustering method.
"
96,Conditional Learning of Fair Representations,"  We propose a novel algorithm for learning fair representations that can
simultaneously mitigate two notions of disparity among different demographic
subgroups in the classification setting. Two key components underpinning the
design of our algorithm are balanced error rate and conditional alignment of
representations. We show how these two components contribute to ensuring
accuracy parity and equalized false-positive and false-negative rates across
groups without impacting demographic parity. Furthermore, we also demonstrate
both in theory and on two real-world experiments that the proposed algorithm
leads to a better utility-fairness trade-off on balanced datasets compared with
existing algorithms on learning fair representations for classification.
"
97,"An Empirical Study on Learning Fairness Metrics for COMPAS Data with
  Human Supervision","  The notion of individual fairness requires that similar people receive
similar treatment. However, this is hard to achieve in practice since it is
difficult to specify the appropriate similarity metric. In this work, we
attempt to learn such similarity metric from human annotated data. We gather a
new dataset of human judgments on a criminal recidivism prediction (COMPAS)
task. By assuming the human supervision obeys the principle of individual
fairness, we leverage prior work on metric learning, evaluate the performance
of several metric learning methods on our dataset, and show that the learned
metrics outperform the Euclidean and Precision metric under various criteria.
We do not provide a way to directly learn a similarity metric satisfying the
individual fairness, but to provide an empirical study on how to derive the
similarity metric from human supervisors, then future work can use this as a
tool to understand human supervision.
"
98,Does Gender Matter? Towards Fairness in Dialogue Systems,"  Recently there are increasing concerns about the fairness of Artificial
Intelligence (AI) in real-world applications such as computer vision and
recommendations. For example, recognition algorithms in computer vision are
unfair to black people such as poorly detecting their faces and inappropriately
identifying them as ""gorillas"". As one crucial application of AI, dialogue
systems have been extensively applied in our society. They are usually built
with real human conversational data; thus they could inherit some fairness
issues which are held in the real world. However, the fairness of dialogue
systems has not been investigated. In this paper, we perform the initial study
about the fairness issues in dialogue systems. In particular, we construct the
first dataset and propose quantitative measures to understand fairness in
dialogue models. Our studies demonstrate that popular dialogue models show
significant prejudice towards different genders and races. Besides, to mitigate
the bias exhibited in dialogue systems, we propose two effective debiasing
methods. Experiments show that our methods can reduce the biases in dialogue
systems significantly. We will release the dataset and the measurement code
later to foster the fairness research in dialogue systems.
"
99,"Preventing Adversarial Use of Datasets through Fair Core-Set
  Construction","  We propose improving the privacy properties of a dataset by publishing only a
strategically chosen ""core-set"" of the data containing a subset of the
instances. The core-set allows strong performance on primary tasks, but forces
poor performance on unwanted tasks. We give methods for both linear models and
neural networks and demonstrate their efficacy on data.
"
100,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,"  A recent trend of fair machine learning is to define fairness as
causality-based notions which concern the causal connection between protected
attributes and decisions. However, one common challenge of all causality-based
fairness notions is identifiability, i.e., whether they can be uniquely
measured from observational data, which is a critical barrier to applying these
notions to real-world situations. In this paper, we develop a framework for
measuring different causality-based fairness. We propose a unified definition
that covers most of previous causality-based fairness notions, namely the
path-specific counterfactual fairness (PC fairness). Based on that, we propose
a general method in the form of a constrained optimization problem for bounding
the path-specific counterfactual fairness under all unidentifiable situations.
Experiments on synthetic and real-world datasets show the correctness and
effectiveness of our method.
"
101,Learning Fairness in Multi-Agent Systems,"  Fairness is essential for human society, contributing to stability and
productivity. Similarly, fairness is also the key for many multi-agent systems.
Taking fairness into multi-agent learning could help multi-agent systems become
both efficient and stable. However, learning efficiency and fairness
simultaneously is a complex, multi-objective, joint-policy optimization. To
tackle these difficulties, we propose FEN, a novel hierarchical reinforcement
learning model. We first decompose fairness for each agent and propose
fair-efficient reward that each agent learns its own policy to optimize. To
avoid multi-objective conflict, we design a hierarchy consisting of a
controller and several sub-policies, where the controller maximizes the
fair-efficient reward by switching among the sub-policies that provides diverse
behaviors to interact with the environment. FEN can be trained in a fully
decentralized way, making it easy to be deployed in real-world applications.
Empirically, we show that FEN easily learns both fairness and efficiency and
significantly outperforms baselines in a variety of multi-agent scenarios.
"
102,"Auditing and Achieving Intersectional Fairness in Classification
  Problems","  Machine learning algorithms are extensively used to make increasingly more
consequential decisions about people, so achieving optimal predictive
performance can no longer be the only focus. A particularly important
consideration is fairness with respect to race, gender, or any other sensitive
attribute. This paper studies intersectional fairness, where intersections of
multiple sensitive attributes are considered. Prior research has mainly focused
on fairness with respect to a single sensitive attribute, with intersectional
fairness being comparatively less studied despite its critical importance for
the safety of modern machine learning systems. We present a comprehensive
framework for auditing and achieving intersectional fairness in classification
problems: we define a suite of metrics to assess intersectional fairness in the
data or model outputs by extending known single-attribute fairness metrics, and
propose methods for robustly estimating them even when some intersectional
subgroups are underrepresented. Furthermore, we develop post-processing
techniques to mitigate any detected intersectional bias in a classification
model. Our techniques do not rely on any assumptions regarding the underlying
model and preserve predictive performance at a guaranteed level of fairness.
Finally, we give guidance on a practical implementation, showing how the
proposed methods perform on a real-world dataset.
"
103,"A Human-in-the-loop Framework to Construct Context-dependent
  Mathematical Formulations of Fairness","  Despite the recent surge of interest in designing and guaranteeing
mathematical formulations of fairness, virtually all existing notions of
algorithmic fairness fail to be adaptable to the intricacies and nuances of the
decision-making context at hand. We argue that capturing such factors is an
inherently human task, as it requires knowledge of the social background in
which machine learning tools impact real people's outcomes and a deep
understanding of the ramifications of automated decisions for decision subjects
and society. In this work, we present a framework to construct a
context-dependent mathematical formulation of fairness utilizing people's
judgment of fairness. We utilize the theoretical model of Heidari et al.
(2019)---which shows that most existing formulations of algorithmic fairness
are special cases of economic models of Equality of Opportunity (EOP)---and
present a practical human-in-the-loop approach to pinpoint the fairness notion
in the EOP family that best captures people's perception of fairness in the
given context. To illustrate our framework, we run human-subject experiments
designed to learn the parameters of Heidari et al.'s EOP model (including
circumstance, desert, and utility) in a hypothetical recidivism decision-making
scenario. Our work takes an initial step toward democratizing the formulation
of fairness and utilizing human-judgment to tackle a fundamental shortcoming of
automated decision-making systems: that the machine on its own is incapable of
understanding and processing the human aspects and social context of its
decisions.
"
104,Fairness-Aware Neural R\'eyni Minimization for Continuous Features,"  The past few years have seen a dramatic rise of academic and societal
interest in fair machine learning. While plenty of fair algorithms have been
proposed recently to tackle this challenge for discrete variables, only a few
ideas exist for continuous ones. The objective in this paper is to ensure some
independence level between the outputs of regression models and any given
continuous sensitive variables. For this purpose, we use the
Hirschfeld-Gebelein-R\'enyi (HGR) maximal correlation coefficient as a fairness
metric. We propose two approaches to minimize the HGR coefficient. First, by
reducing an upper bound of the HGR with a neural network estimation of the
$\chi^{2}$ divergence. Second, by minimizing the HGR directly with an
adversarial neural network architecture. The idea is to predict the output Y
while minimizing the ability of an adversarial neural network to find the
estimated transformations which are required to predict the HGR coefficient. We
empirically assess and compare our approaches and demonstrate significant
improvements on previously presented work in the field.
"
105,Fair Adversarial Gradient Tree Boosting,"  Fair classification has become an important topic in machine learning
research. While most bias mitigation strategies focus on neural networks, we
noticed a lack of work on fair classifiers based on decision trees even though
they have proven very efficient. In an up-to-date comparison of
state-of-the-art classification algorithms in tabular data, tree boosting
outperforms deep learning. For this reason, we have developed a novel approach
of adversarial gradient tree boosting. The objective of the algorithm is to
predict the output $Y$ with gradient tree boosting while minimizing the ability
of an adversarial neural network to predict the sensitive attribute $S$. The
approach incorporates at each iteration the gradient of the neural network
directly in the gradient tree boosting. We empirically assess our approach on 4
popular data sets and compare against state-of-the-art algorithms. The results
show that our algorithm achieves a higher accuracy while obtaining the same
level of fairness, as measured using a set of different common fairness
definitions.
"
106,Fair Data Adaptation with Quantile Preservation,"  Fairness of classification and regression has received much attention
recently and various, partially non-compatible, criteria have been proposed.
The fairness criteria can be enforced for a given classifier or, alternatively,
the data can be adapated to ensure that every classifier trained on the data
will adhere to desired fairness criteria. We present a practical data adaption
method based on quantile preservation in causal structural equation models. The
data adaptation is based on a presumed counterfactual model for the data. While
the counterfactual model itself cannot be verified experimentally, we show that
certain population notions of fairness are still guaranteed even if the
counterfactual model is misspecified. The precise nature of the fulfilled
non-causal fairness notion (such as demographic parity, separation or
sufficiency) depends on the structure of the underlying causal model and the
choice of resolving variables. We describe an implementation of the proposed
data adaptation procedure based on Random Forests and demonstrate its practical
use on simulated and real-world data.
"
107,Fairness through Equality of Effort,"  Fair machine learning is receiving an increasing attention in machine
learning fields. Researchers in fair learning have developed correlation or
association-based measures such as demographic disparity, mistreatment
disparity, calibration, causal-based measures such as total effect, direct and
indirect discrimination, and counterfactual fairness, and fairness notions such
as equality of opportunity and equal odds that consider both decisions in the
training data and decisions made by predictive models. In this paper, we
develop a new causal-based fairness notation, called equality of effort.
Different from existing fairness notions which mainly focus on discovering the
disparity of decisions between two groups of individuals, the proposed equality
of effort notation helps answer questions like to what extend a legitimate
variable should change to make a particular individual achieve a certain
outcome level and addresses the concerns whether the efforts made to achieve
the same outcome level for individuals from the protected group and that from
the unprotected group are different. We develop algorithms for determining
whether an individual or a group of individuals is discriminated in terms of
equality of effort. We also develop an optimization-based method for removing
discriminatory effects from the data if discrimination is detected. We conduct
empirical evaluations to compare the equality of effort and existing fairness
notion and show the effectiveness of our proposed algorithms.
"
108,Online Fair Division: A Survey,"  We survey a burgeoning and promising new research area that considers the
online nature of many practical fair division problems. We identify wide
variety of such online fair division problems, as well as discuss new
mechanisms and normative properties that apply to this online setting. The
online nature of such fair division problems provides both opportunities and
challenges such as the possibility to develop new online mechanisms as well as
the difficulty of dealing with an uncertain future.
"
109,Towards FAIR protocols and workflows: The OpenPREDICT case study,"  It is essential for the advancement of science that scientists and
researchers share, reuse and reproduce workflows and protocols used by others.
The FAIR principles are a set of guidelines that aim to maximize the value and
usefulness of research data, and emphasize a number of important points
regarding the means by which digital objects are found and reused by others.
The question of how to apply these principles not just to the static input and
output data but also to the dynamic workflows and protocols that consume and
produce them is still under debate and poses a number of challenges. In this
paper we describe our inclusive and overarching approach to apply the FAIR
principles to workflows and protocols and demonstrate its benefits. We apply
and evaluate our approach on a case study that consists of making the PREDICT
workflow, a highly cited drug repurposing workflow, open and FAIR. This
includes FAIRification of the involved datasets, as well as applying semantic
technologies to represent and store data about the detailed versions of the
general protocol, of the concrete workflow instructions, and of their execution
traces. A semantic model was proposed to better address these specific
requirements and were evaluated by answering competency questions. This
semantic model consists of classes and relations from a number of existing
ontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then
to formulate and answer new kinds of competency questions. Our evaluation shows
the high degree to which our FAIRified OpenPREDICT workflow now adheres to the
FAIR principles and the practicality and usefulness of being able to answer our
new competency questions.
"
110,Greedy Algorithms for Fair Division of Mixed Manna,"  We consider a multi-agent model for fair division of mixed manna (i.e. items
for which agents can have positive, zero or negative utilities), in which
agents have additive utilities for bundles of items. For this model, we give
several general impossibility results and special possibility results for three
common fairness concepts (i.e. EF1, EFX, EFX3) and one popular efficiency
concept (i.e. PO). We also study how these interact with common welfare
objectives such as the Nash, disutility Nash and egalitarian welfares. For
example, we show that maximizing the Nash welfare with mixed manna (or
minimizing the disutility Nash welfare) does not ensure an EF1 allocation
whereas with goods and the Nash welfare it does. We also prove that an EFX3
allocation may not exist even with identical utilities. By comparison, with
tertiary utilities, EFX and PO allocations, or EFX3 and PO allocations always
exist. Also, with identical utilities, EFX and PO allocations always exist. For
these cases, we give polynomial-time algorithms, returning such allocations and
approximating further the Nash, disutility Nash and egalitarian welfares in
special cases.
"
111,Fair in the Eyes of Others,"  Envy-freeness is a widely studied notion in resource allocation, capturing
some aspects of fairness. The notion of envy being inherently subjective
though, it might be the case that an agent envies another agent, but that she
objectively has no reason to do so. The difficulty here is to define the notion
of objectivity, since no ground-truth can properly serve as a basis of this
definition. A natural approach is to consider the judgement of the other agents
as a proxy for objectivity. Building on previous work by Parijs (who introduced
""unanimous envy"") we propose the notion of approval envy: an agent $a_i$
experiences approval envy towards $a_j$ if she is envious of $a_j$, and
sufficiently many agents agree that this should be the case, from their own
perspectives. Some interesting properties of this notion are put forward.
Computing the minimal threshold guaranteeing approval envy clearly inherits
well-known intractable results from envy-freeness, but (i) we identify some
tractable cases such as house allocation; and (ii) we provide a general method
based on a mixed integer programming encoding of the problem, which proves to
be efficient in practice. This allows us in particular to show experimentally
that existence of such allocations, with a rather small threshold, is very
often observed.
"
112,"Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture
  Search","  Differentiable Architecture Search (DARTS) is now a widely disseminated
weight-sharing neural architecture search method. However, it suffers from
well-known performance collapse due to an inevitable aggregation of skip
connections. In this paper, we first disclose that its root cause lies in an
unfair advantage in exclusive competition. Through experiments, we show that if
either of two conditions is broken, the collapse disappears. Thereby, we
present a novel approach called Fair DARTS where the exclusive competition is
relaxed to be collaborative. Specifically, we let each operation's
architectural weight be independent of others. Yet there is still an important
issue of discretization discrepancy. We then propose a zero-one loss to push
architectural weights towards zero or one, which approximates an expected
multi-hot solution. Our experiments are performed on two mainstream search
spaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet.
Our code is available on https://github.com/xiaomi-automl/fairdarts .
"
113,On the Legal Compatibility of Fairness Definitions,"  Past literature has been effective in demonstrating ideological gaps in
machine learning (ML) fairness definitions when considering their use in
complex socio-technical systems. However, we go further to demonstrate that
these definitions often misunderstand the legal concepts from which they
purport to be inspired, and consequently inappropriately co-opt legal language.
In this paper, we demonstrate examples of this misalignment and discuss the
differences in ML terminology and their legal counterparts, as well as what
both the legal and ML fairness communities can learn from these tensions. We
focus this paper on U.S. anti-discrimination law since the ML fairness research
community regularly references terms from this body of law.
"
114,Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?,"  Multiple fairness constraints have been proposed in the literature, motivated
by a range of concerns about how demographic groups might be treated unfairly
by machine learning classifiers. In this work we consider a different
motivation; learning from biased training data. We posit several ways in which
training data may be biased, including having a more noisy or negatively biased
labeling process on members of a disadvantaged group, or a decreased prevalence
of positive or negative examples from the disadvantaged group, or both.
  Given such biased training data, Empirical Risk Minimization (ERM) may
produce a classifier that not only is biased but also has suboptimal accuracy
on the true data distribution. We examine the ability of fairness-constrained
ERM to correct this problem. In particular, we find that the Equal Opportunity
fairness constraint (Hardt, Price, and Srebro 2016) combined with ERM will
provably recover the Bayes Optimal Classifier under a range of bias models. We
also consider other recovery methods including reweighting the training data,
Equalized Odds, and Demographic Parity. These theoretical results provide
additional motivation for considering fairness interventions even if an actor
cares primarily about accuracy.
"
115,Group Fairness in Bandit Arm Selection,"  We propose a novel formulation of group fairness in the contextual
multi-armed bandit (CMAB) setting. In the CMAB setting a sequential decision
maker must at each time step choose an arm to pull from a finite set of arms
after observing some context for each of the potential arm pulls. In our model
arms are partitioned into two or more sensitive groups based on some protected
feature (e.g., age, race, or socio-economic status). Despite the fact that
there may be differences in expected payout between the groups, we may wish to
ensure some form of fairness between picking arms from the various groups. In
this work we explore two definitions of fairness: equal group probability,
wherein the probability of pulling an arm from any of the protected groups is
the same; and proportional parity, wherein the probability of choosing an arm
from a particular group is proportional to the size of that group. We provide a
novel algorithm that can accommodate these notions of fairness for an arbitrary
number of groups, and provide bounds on the regret for our algorithm. We then
validate our algorithm using synthetic data as well as two real-world datasets
for intervention settings wherein we want to allocate resources fairly across
protected groups.
"
116,LTLf Synthesis with Fairness and Stability Assumptions,"  In synthesis, assumptions are constraints on the environment that rule out
certain environment behaviors. A key observation here is that even if we
consider systems with LTLf goals on finite traces, environment assumptions need
to be expressed over infinite traces, since accomplishing the agent goals may
require an unbounded number of environment action. To solve synthesis with
respect to finite-trace LTLf goals under infinite-trace assumptions, we could
reduce the problem to LTL synthesis. Unfortunately, while synthesis in LTLf and
in LTL have the same worst-case complexity (both 2EXPTIME-complete), the
algorithms available for LTL synthesis are much more difficult in practice than
those for LTLf synthesis. In this work we show that in interesting cases we can
avoid such a detour to LTL synthesis and keep the simplicity of LTLf synthesis.
Specifically, we develop a BDD-based fixpoint-based technique for handling
basic forms of fairness and of stability assumptions. We show, empirically,
that this technique performs much better than standard LTL synthesis.
"
117,Fair Contextual Multi-Armed Bandits: Theory and Experiments,"  When an AI system interacts with multiple users, it frequently needs to make
allocation decisions. For instance, a virtual agent decides whom to pay
attention to in a group setting, or a factory robot selects a worker to deliver
a part. Demonstrating fairness in decision making is essential for such systems
to be broadly accepted. We introduce a Multi-Armed Bandit algorithm with
fairness constraints, where fairness is defined as a minimum rate that a task
or a resource is assigned to a user. The proposed algorithm uses contextual
information about the users and the task and makes no assumptions on how the
losses capturing the performance of different users are generated. We provide
theoretical guarantees of performance and empirical results from simulation and
an online user study. The results highlight the benefit of accounting for
contexts in fair decision making, especially when users perform better at some
contexts and worse at others.
"
118,"Balancing the Tradeoff between Profit and Fairness in Rideshare
  Platforms During High-Demand Hours","  Rideshare platforms, when assigning requests to drivers, tend to maximize
profit for the system and/or minimize waiting time for riders. Such platforms
can exacerbate biases that drivers may have over certain types of requests. We
consider the case of peak hours when the demand for rides is more than the
supply of drivers. Drivers are well aware of their advantage during the peak
hours and can choose to be selective about which rides to accept. Moreover, if
in such a scenario, the assignment of requests to drivers (by the platform) is
made only to maximize profit and/or minimize wait time for riders, requests of
a certain type (e.g. from a non-popular pickup location, or to a non-popular
drop-off location) might never be assigned to a driver. Such a system can be
highly unfair to riders. However, increasing fairness might come at a cost of
the overall profit made by the rideshare platform. To balance these conflicting
goals, we present a flexible, non-adaptive algorithm, \lpalg, that allows the
platform designer to control the profit and fairness of the system via
parameters $\alpha$ and $\beta$ respectively. We model the matching problem as
an online bipartite matching where the set of drivers is offline and requests
arrive online. Upon the arrival of a request, we use \lpalg to assign it to a
driver (the driver might then choose to accept or reject it) or reject the
request. We formalize the measures of profit and fairness in our setting and
show that by using \lpalg, the competitive ratios for profit and fairness
measures would be no worse than $\alpha/e$ and $\beta/e$ respectively.
Extensive experimental results on both real-world and synthetic datasets
confirm the validity of our theoretical lower bounds. Additionally, they show
that $\lpalg$ under some choice of $(\alpha, \beta)$ can beat two natural
heuristics, Greedy and Uniform, on \emph{both} fairness and profit.
"
119,"Stochastic Fairness and Language-Theoretic Fairness in Planning on
  Nondeterministic Domains","  We address two central notions of fairness in the literature of planning on
nondeterministic fully observable domains. The first, which we call stochastic
fairness, is classical, and assumes an environment which operates
probabilistically using possibly unknown probabilities. The second, which is
language-theoretic, assumes that if an action is taken from a given state
infinitely often then all its possible outcomes should appear infinitely often
(we call this state-action fairness). While the two notions coincide for
standard reachability goals, they diverge for temporally extended goals. This
important difference has been overlooked in the planning literature, and we
argue has led to confusion in a number of published algorithms which use
reductions that were stated for state-action fairness, for which they are
incorrect, while being correct for stochastic fairness. We remedy this and
provide an optimal sound and complete algorithm for solving state-action fair
planning for LTL/LTLf goals, as well as a correct proof of the lower bound of
the goal-complexity (our proof is general enough that it provides new proofs
also for the no-fairness and stochastic-fairness cases). Overall, we show that
stochastic fairness is better behaved than state-action fairness.
"
120,Leveraging Semi-Supervised Learning for Fairness using Neural Networks,"  There has been a growing concern about the fairness of decision-making
systems based on machine learning. The shortage of labeled data has been always
a challenging problem facing machine learning based systems. In such scenarios,
semi-supervised learning has shown to be an effective way of exploiting
unlabeled data to improve upon the performance of model. Notably, unlabeled
data do not contain label information which itself can be a significant source
of bias in training machine learning systems. This inspired us to tackle the
challenge of fairness by formulating the problem in a semi-supervised
framework. In this paper, we propose a semi-supervised algorithm using neural
networks benefiting from unlabeled data to not just improve the performance but
also improve the fairness of the decision-making process. The proposed model,
called SSFair, exploits the information in the unlabeled data to mitigate the
bias in the training data.
"
121,Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,"  Bias in machine learning has manifested injustice in several areas, such as
medicine, hiring, and criminal justice. In response, computer scientists have
developed myriad definitions of fairness to correct this bias in fielded
algorithms. While some definitions are based on established legal and ethical
norms, others are largely mathematical. It is unclear whether the general
public agrees with these fairness definitions, and perhaps more importantly,
whether they understand these definitions. We take initial steps toward
bridging this gap between ML researchers and the public, by addressing the
question: does a lay audience understand a basic definition of ML fairness? We
develop a metric to measure comprehension of three such
definitions--demographic parity, equal opportunity, and equalized odds. We
evaluate this metric using an online survey, and investigate the relationship
between comprehension and sentiment, demographics, and the definition itself.
"
122,On Consequentialism and Fairness,"  Recent work on fairness in machine learning has primarily emphasized how to
define, quantify, and encourage ""fair"" outcomes. Less attention has been paid,
however, to the ethical foundations which underlie such efforts. Among the
ethical perspectives that should be taken into consideration is
consequentialism, the position that, roughly speaking, outcomes are all that
matter. Although consequentialism is not free from difficulties, and although
it does not necessarily provide a tractable way of choosing actions (because of
the combined problems of uncertainty, subjectivity, and aggregation), it
nevertheless provides a powerful foundation from which to critique the existing
literature on machine learning fairness. Moreover, it brings to the fore some
of the tradeoffs involved, including the problem of who counts, the pros and
cons of using a policy, and the relative value of the distant future. In this
paper we provide a consequentialist critique of common definitions of fairness
within machine learning, as well as a machine learning perspective on
consequentialism. We conclude with a broader discussion of the issues of
learning and randomization, which have important implications for the ethics of
automated decision making systems.
"
123,Fairness in Learning-Based Sequential Decision Algorithms: A Survey,"  Algorithmic fairness in decision-making has been studied extensively in
static settings where one-shot decisions are made on tasks such as
classification. However, in practice most decision-making processes are of a
sequential nature, where decisions made in the past may have an impact on
future data. This is particularly the case when decisions affect the
individuals or users generating the data used for future decisions. In this
survey, we review existing literature on the fairness of data-driven sequential
decision-making. We will focus on two types of sequential decisions: (1) past
decisions have no impact on the underlying user population and thus no impact
on future data; (2) past decisions have an impact on the underlying user
population and therefore the future data, which can then impact future
decisions. In each case the impact of various fairness interventions on the
underlying population is examined.
"
124,Fair Transfer of Multiple Style Attributes in Text,"  To preserve anonymity and obfuscate their identity on online platforms users
may morph their text and portray themselves as a different gender or
demographic. Similarly, a chatbot may need to customize its communication style
to improve engagement with its audience. This manner of changing the style of
written text has gained significant attention in recent years. Yet these past
research works largely cater to the transfer of single style attributes. The
disadvantage of focusing on a single style alone is that this often results in
target text where other existing style attributes behave unpredictably or are
unfairly dominated by the new style. To counteract this behavior, it would be
nice to have a style transfer mechanism that can transfer or control multiple
styles simultaneously and fairly. Through such an approach, one could obtain
obfuscated or written text incorporated with a desired degree of multiple soft
styles such as female-quality, politeness, or formalness.
  In this work, we demonstrate that the transfer of multiple styles cannot be
achieved by sequentially performing multiple single-style transfers. This is
because each single style-transfer step often reverses or dominates over the
style incorporated by a previous transfer step. We then propose a neural
network architecture for fairly transferring multiple style attributes in a
given text. We test our architecture on the Yelp data set to demonstrate our
superior performance as compared to existing one-style transfer steps performed
in a sequence.
"
125,Adequate and fair explanations,"  Explaining sophisticated machine-learning based systems is an important issue
at the foundations of AI. Recent efforts have shown various methods for
providing explanations. These approaches can be broadly divided into two
schools: those that provide a local and human interpreatable approximation of a
machine learning algorithm, and logical approaches that exactly characterise
one aspect of the decision. In this paper we focus upon the second school of
exact explanations with a rigorous logical foundation. There is an
epistemological problem with these exact methods. While they can furnish
complete explanations, such explanations may be too complex for humans to
understand or even to write down in human readable form. Interpretability
requires epistemically accessible explanations, explanations humans can grasp.
Yet what is a sufficiently complete epistemically accessible explanation still
needs clarification. We do this here in terms of counterfactuals, following
[Wachter et al., 2017]. With counterfactual explanations, many of the
assumptions needed to provide a complete explanation are left implicit. To do
so, counterfactual explanations exploit the properties of a particular data
point or sample, and as such are also local as well as partial explanations. We
explore how to move from local partial explanations to what we call complete
local explanations and then to global ones. But to preserve accessibility we
argue for the need for partiality. This partiality makes it possible to hide
explicit biases present in the algorithm that may be injurious or unfair.We
investigate how easy it is to uncover these biases in providing complete and
fair explanations by exploiting the structure of the set of counterfactuals
providing a complete local explanation.
"
126,Algorithmic Fairness from a Non-ideal Perspective,"  Inspired by recent breakthroughs in predictive modeling, practitioners in
both industry and government have turned to machine learning with hopes of
operationalizing predictions to drive automated decisions. Unfortunately, many
social desiderata concerning consequential decisions, such as justice or
fairness, have no natural formulation within a purely predictive framework. In
efforts to mitigate these problems, researchers have proposed a variety of
metrics for quantifying deviations from various statistical parities that we
might expect to observe in a fair world and offered a variety of algorithms in
attempts to satisfy subsets of these parities or to trade off the degree to
which they are satisfied against utility. In this paper, we connect this
approach to \emph{fair machine learning} to the literature on ideal and
non-ideal methodological approaches in political philosophy. The ideal approach
requires positing the principles according to which a just world would operate.
In the most straightforward application of ideal theory, one supports a
proposed policy by arguing that it closes a discrepancy between the real and
the perfectly just world. However, by failing to account for the mechanisms by
which our non-ideal world arose, the responsibilities of various
decision-makers, and the impacts of proposed policies, naive applications of
ideal thinking can lead to misguided interventions. In this paper, we
demonstrate a connection between the fair machine learning literature and the
ideal approach in political philosophy, and argue that the increasingly
apparent shortcomings of proposed fair machine learning algorithms reflect
broader troubles faced by the ideal approach. We conclude with a critical
discussion of the harms of misguided solutions, a reinterpretation of
impossibility results, and directions for future research.
"
127,Algorithmic Fairness,"  An increasing number of decisions regarding the daily lives of human beings
are being controlled by artificial intelligence (AI) algorithms in spheres
ranging from healthcare, transportation, and education to college admissions,
recruitment, provision of loans and many more realms. Since they now touch on
many aspects of our lives, it is crucial to develop AI algorithms that are not
only accurate but also objective and fair. Recent studies have shown that
algorithmic decision-making may be inherently prone to unfairness, even when
there is no intention for it. This paper presents an overview of the main
concepts of identifying, measuring and improving algorithmic fairness when
using AI algorithms. The paper begins by discussing the causes of algorithmic
bias and unfairness and the common definitions and measures for fairness.
Fairness-enhancing mechanisms are then reviewed and divided into pre-process,
in-process and post-process mechanisms. A comprehensive comparison of the
mechanisms is then conducted, towards a better understanding of which
mechanisms should be used in different scenarios. The paper then describes the
most commonly used fairness-related datasets in this field. Finally, the paper
ends by reviewing several emerging research sub-fields of algorithmic fairness.
"
128,FAE: A Fairness-Aware Ensemble Framework,"  Automated decision making based on big data and machine learning (ML)
algorithms can result in discriminatory decisions against certain protected
groups defined upon personal data like gender, race, sexual orientation etc.
Such algorithms designed to discover patterns in big data might not only pick
up any encoded societal biases in the training data, but even worse, they might
reinforce such biases resulting in more severe discrimination. The majority of
thus far proposed fairness-aware machine learning approaches focus solely on
the pre-, in- or post-processing steps of the machine learning process, that
is, input data, learning algorithms or derived models, respectively. However,
the fairness problem cannot be isolated to a single step of the ML process.
Rather, discrimination is often a result of complex interactions between big
data and algorithms, and therefore, a more holistic approach is required. The
proposed FAE (Fairness-Aware Ensemble) framework combines fairness-related
interventions at both pre- and postprocessing steps of the data analysis
process. In the preprocessing step, we tackle the problems of
under-representation of the protected group (group imbalance) and of
class-imbalance by generating balanced training samples. In the post-processing
step, we tackle the problem of class overlapping by shifting the decision
boundary in the direction of fairness.
"
129,Joint Optimization of AI Fairness and Utility: A Human-Centered Approach,"  Today, AI is increasingly being used in many high-stakes decision-making
applications in which fairness is an important concern. Already, there are many
examples of AI being biased and making questionable and unfair decisions. The
AI research community has proposed many methods to measure and mitigate
unwanted biases, but few of them involve inputs from human policy makers. We
argue that because different fairness criteria sometimes cannot be
simultaneously satisfied, and because achieving fairness often requires
sacrificing other objectives such as model accuracy, it is key to acquire and
adhere to human policy makers' preferences on how to make the tradeoff among
these objectives. In this paper, we propose a framework and some exemplar
methods for eliciting such preferences and for optimizing an AI model according
to these preferences.
"
130,Fair Correlation Clustering,"  In this paper, we study correlation clustering under fairness constraints.
Fair variants of $k$-median and $k$-center clustering have been studied
recently, and approximation algorithms using a notion called fairlet
decomposition have been proposed. We obtain approximation algorithms for fair
correlation clustering under several important types of fairness constraints.
  Our results hinge on obtaining a fairlet decomposition for correlation
clustering by introducing a novel combinatorial optimization problem. We define
a fairlet decomposition with cost similar to the $k$-median cost and this
allows us to obtain approximation algorithms for a wide range of fairness
constraints.
  We complement our theoretical results with an in-depth analysis of our
algorithms on real graphs where we show that fair solutions to correlation
clustering can be obtained with limited increase in cost compared to the
state-of-the-art (unfair) algorithms.
"
131,Fair Correlation Clustering,"  In this paper we study the problem of correlation clustering under fairness
constraints. In the classic correlation clustering problem, we are given a
complete graph where each edge is labeled positive or negative. The goal is to
obtain a clustering of the vertices that minimizes disagreements -- the number
of negative edges trapped inside a cluster plus positive edges between
different clusters.
  We consider two variations of fairness constraint for the problem of
correlation clustering where each node has a color, and the goal is to form
clusters that do not over-represent vertices of any color.
  The first variant aims to generate clusters with minimum disagreements, where
the distribution of a feature (e.g. gender) in each cluster is same as the
global distribution. For the case of two colors when the desired ratio of the
number of colors in each cluster is $1:p$, we get
$\mathcal{O}(p^2)$-approximation algorithm. Our algorithm could be extended to
the case of multiple colors. We prove this problem is NP-hard.
  The second variant considers relative upper and lower bounds on the number of
nodes of any color in a cluster. The goal is to avoid violating upper and lower
bounds corresponding to each color in each cluster while minimizing the total
number of disagreements. Along with our theoretical results, we show the
effectiveness of our algorithm to generate fair clusters by empirical
evaluation on real world data sets.
"
132,Convex Fairness Constrained Model Using Causal Effect Estimators,"  Recent years have seen much research on fairness in machine learning. Here,
mean difference (MD) or demographic parity is one of the most popular measures
of fairness. However, MD quantifies not only discrimination but also
explanatory bias which is the difference of outcomes justified by explanatory
features. In this paper, we devise novel models, called FairCEEs, which remove
discrimination while keeping explanatory bias. The models are based on
estimators of causal effect utilizing propensity score analysis. We prove that
FairCEEs with the squared loss theoretically outperform a naive MD constraint
model. We provide an efficient algorithm for solving FairCEEs in regression and
binary classification tasks. In our experiment on synthetic and real-world data
in these two tasks, FairCEEs outperformed an existing model that considers
explanatory bias in specific cases.
"
133,"Learning Individually Fair Classifier with Path-Specific Causal-Effect
  Constraint","  Machine learning is increasingly used to make decisions for individuals in
various fields, which require to achieve good prediction accuracy while
ensuring fairness with respect to such sensitive features as race or gender.
This problem, however, remains difficult in complex real-world scenarios. To
effectively quantify unfairness in such scenarios, existing methods utilize
{\it path-specific causal effects}. However, none of them can ensure fairness
for each individual without making impractical assumptions. Specifically, these
assumptions require us to formulate the true data-generating processes as the
{\it causal model}, which requires an extremely deep understanding of data and
is unrealistic in practice. In this paper, we propose a framework for learning
an individually fair classifier without relying on the causal model. For this
goal, we define the {\it probability of individual unfairness} (PIU) and solve
an optimization problem that constrains its upper bound, which can be estimated
from data without the causal model. We elucidate why this constraint can
guarantee fairness for each individual. Experimental results demonstrate that
our method learns an individually fair classifier at a slight cost of
prediction accuracy.
"
134,Fair Prediction with Endogenous Behavior,"  There is increasing regulatory interest in whether machine learning
algorithms deployed in consequential domains (e.g. in criminal justice) treat
different demographic groups ""fairly."" However, there are several proposed
notions of fairness, typically mutually incompatible. Using criminal justice as
an example, we study a model in which society chooses an incarceration rule.
Agents of different demographic groups differ in their outside options (e.g.
opportunity for legal employment) and decide whether to commit crimes. We show
that equalizing type I and type II errors across groups is consistent with the
goal of minimizing the overall crime rate; other popular notions of fairness
are not.
"
135,"Designing Fair AI for Managing Employees in Organizations: A Review,
  Critique, and Design Agenda","  Organizations are rapidly deploying artificial intelligence (AI) systems to
manage their workers. However, AI has been found at times to be unfair to
workers. Unfairness toward workers has been associated with decreased worker
effort and increased worker turnover. To avoid such problems, AI systems must
be designed to support fairness and redress instances of unfairness. Despite
the attention related to AI unfairness, there has not been a theoretical and
systematic approach to developing a design agenda. This paper addresses the
issue in three ways. First, we introduce the organizational justice theory,
three different fairness types (distributive, procedural, interactional), and
the frameworks for redressing instances of unfairness (retributive justice,
restorative justice). Second, we review the design literature that specifically
focuses on issues of AI fairness in organizations. Third, we propose a design
agenda for AI fairness in organizations that applies each of the fairness types
to organizational scenarios. Then, the paper concludes with implications for
future research.
"
136,Learning Fairness-aware Relational Structures,"  The development of fair machine learning models that effectively avert bias
and discrimination is an important problem that has garnered attention in
recent years. The necessity of encoding complex relational dependencies among
the features and variables for competent predictions require the development of
fair, yet expressive relational models. In this work, we introduce Fair-A3SL, a
fairness-aware structure learning algorithm for learning relational structures,
which incorporates fairness measures while learning relational graphical model
structures. Our approach is versatile in being able to encode a wide range of
fairness metrics such as statistical parity difference, overestimation,
equalized odds, and equal opportunity, including recently proposed relational
fairness measures. While existing approaches employ the fairness measures on
pre-determined model structures post prediction, Fair-A3SL directly learns the
structure while optimizing for the fairness measures and hence is able to
remove any structural bias in the model. We demonstrate the effectiveness of
our learned model structures when compared with the state-of-the-art fairness
models quantitatively and qualitatively on datasets representing three
different modeling scenarios: i) a relational dataset, ii) a recidivism
prediction dataset widely used in studying discrimination, and iii) a
recommender systems dataset. Our results show that Fair-A3SL can learn fair,
yet interpretable and expressive structures capable of making accurate
predictions.
"
137,Learning Certified Individually Fair Representations,"  To effectively enforce fairness constraints one needs to define an
appropriate notion of fairness and employ representation learning in order to
impose this notion without compromising downstream utility for the data
consumer. A desirable notion is individual fairness as it guarantees similar
treatment for similar individuals. In this work, we introduce the first method
which generalizes individual fairness to rich similarity notions via logical
constraints while also enabling data consumers to obtain fairness certificates
for their models. The key idea is to learn a representation that provably maps
similar individuals to latent representations at most $\epsilon$ apart in
$\ell_{\infty}$-distance, enabling data consumers to certify individual
fairness by proving $\epsilon$-robustness of their classifier. Our experimental
evaluation on six real-world datasets and a wide range of fairness constraints
demonstrates that our approach is expressive enough to capture similarity
notions beyond existing distance metrics while scaling to realistic use cases.
"
138,"FairRec: Two-Sided Fairness for Personalized Recommendations in
  Two-Sided Platforms","  We investigate the problem of fair recommendation in the context of two-sided
online platforms, comprising customers on one side and producers on the other.
Traditionally, recommendation services in these platforms have focused on
maximizing customer satisfaction by tailoring the results according to the
personalized preferences of individual customers. However, our investigation
reveals that such customer-centric design may lead to unfair distribution of
exposure among the producers, which may adversely impact their well-being. On
the other hand, a producer-centric design might become unfair to the customers.
Thus, we consider fairness issues that span both customers and producers. Our
approach involves a novel mapping of the fair recommendation problem to a
constrained version of the problem of fairly allocating indivisible goods. Our
proposed FairRec algorithm guarantees at least Maximin Share (MMS) of exposure
for most of the producers and Envy-Free up to One item (EF1) fairness for every
customer. Extensive evaluations over multiple real-world datasets show the
effectiveness of FairRec in ensuring two-sided fairness while incurring a
marginal loss in the overall recommendation quality.
"
139,Counterfactual fairness: removing direct effects through regularization,"  Building machine learning models that are fair with respect to an
unprivileged group is a topical problem. Modern fairness-aware algorithms often
ignore causal effects and enforce fairness through modifications applicable to
only a subset of machine learning models. In this work, we propose a new
definition of fairness that incorporates causality through the Controlled
Direct Effect (CDE). We develop regularizations to tackle classical fairness
measures and present a causal regularization that satisfies our new fairness
definition by removing the impact of unprivileged group variables on the model
outcomes as measured by the CDE. These regularizations are applicable to any
model trained using by iteratively minimizing a loss through differentiation.
We demonstrate our approaches using both gradient boosting and logistic
regression on: a synthetic dataset, the UCI Adult (Census) Dataset, and a
real-world credit-risk dataset. Our results were found to mitigate unfairness
from the predictions with small reductions in model performance.
"
140,CheXclusion: Fairness gaps in deep chest X-ray classifiers,"  Machine learning systems have received much attention recently for their
ability to achieve expert-level performance on clinical tasks, particularly in
medical imaging. Here, we examine the extent to which state-of-the-art deep
learning classifiers trained to yield diagnostic labels from X-ray images are
biased with respect to protected attributes. We train convolution neural
networks to predict 14 diagnostic labels in three prominent public chest X-ray
datasets: MIMIC-CXR, Chest-Xray8, and CheXpert. We then evaluate the TPR
disparity - the difference in true positive rates (TPR) and - underdiagnosis
rate - the false positive rate of a non-diagnosis - among different protected
attributes such as patient sex, age, race, and insurance type. We demonstrate
that TPR disparities exist in the state-of-the-art classifiers in all datasets,
for all clinical tasks, and all subgroups. We find that TPR disparities are
most commonly not significantly correlated with a subgroup's proportional
disease burden; further, we find that some subgroups and subsection of the
population are chronically underdiagnosed. Such performance disparities have
real consequences as models move from papers to products, and should be
carefully audited prior to deployment.
"
141,Evidence-based explanation to promote fairness in AI systems,"  As Artificial Intelligence (AI) technology gets more intertwined with every
system, people are using AI to make decisions on their everyday activities. In
simple contexts, such as Netflix recommendations, or in more complex context
like in judicial scenarios, AI is part of people's decisions. People make
decisions and usually, they need to explain their decision to others or in some
matter. It is particularly critical in contexts where human expertise is
central to decision-making. In order to explain their decisions with AI
support, people need to understand how AI is part of that decision. When
considering the aspect of fairness, the role that AI has on a decision-making
process becomes even more sensitive since it affects the fairness and the
responsibility of those people making the ultimate decision. We have been
exploring an evidence-based explanation design approach to 'tell the story of a
decision'. In this position paper, we discuss our approach for AI systems using
fairness sensitive cases in the literature.
"
142,Getting Fairness Right: Towards a Toolbox for Practitioners,"  The potential risk of AI systems unintentionally embedding and reproducing
bias has attracted the attention of machine learning practitioners and society
at large. As policy makers are willing to set the standards of algorithms and
AI techniques, the issue on how to refine existing regulation, in order to
enforce that decisions made by automated systems are fair and
non-discriminatory, is again critical. Meanwhile, researchers have demonstrated
that the various existing metrics for fairness are statistically mutually
exclusive and the right choice mostly depends on the use case and the
definition of fairness.
  Recognizing that the solutions for implementing fair AI are not purely
mathematical but require the commitments of the stakeholders to define the
desired nature of fairness, this paper proposes to draft a toolbox which helps
practitioners to ensure fair AI practices. Based on the nature of the
application and the available training data, but also on legal requirements and
ethical, philosophical and cultural dimensions, the toolbox aims to identify
the most appropriate fairness objective. This approach attempts to structure
the complex landscape of fairness metrics and, therefore, makes the different
available options more accessible to non-technical people. In the proven
absence of a silver bullet solution for fair AI, this toolbox intends to
produce the fairest AI systems possible with respect to their local context.
"
143,Finding Fair and Efficient Allocations When Valuations Don't Add Up,"  In this paper, we present new results on the fair and efficient allocation of
indivisible goods to agents whose preferences correspond to matroid rank
functions. This is a versatile valuation class with several desirable
properties (such as monotonicity and submodularity), which naturally lends
itself to a number of real-world domains. We use these properties to our
advantage; first, we show that when agent valuations are matroid rank
functions, a socially optimal (i.e. utilitarian social welfare-maximizing)
allocation that achieves envy-freeness up to one item (EF1) exists and is
computationally tractable. We also prove that the Nash welfare-maximizing and
the leximin allocations both exhibit this fairness/efficiency combination, by
showing that they can be achieved by minimizing any symmetric strictly convex
function over utilitarian optimal outcomes. To the best of our knowledge, this
is the first valuation function class not subsumed by additive valuations for
which it has been established that an allocation maximizing Nash welfare is
EF1. Moreover, for a subclass of these valuation functions based on maximum
(unweighted) bipartite matching, we show that a leximin allocation can be
computed in polynomial time. Additionally, we explore possible extensions of
our results to fairness criteria other than EF1 as well as to generalizations
of the above valuation classes.
"
144,"Best Practices for Implementing FAIR Vocabularies and Ontologies on the
  Web","  With the adoption of Semantic Web technologies, an increasing number of
vocabularies and ontologies have been developed in different domains, ranging
from Biology to Agronomy or Geosciences. However, many of these ontologies are
still difficult to find, access and understand by researchers due to a lack of
documentation, URI resolving issues, versioning problems, etc. In this chapter
we describe guidelines and best practices for creating accessible,
understandable and reusable ontologies on the Web, using standard practices and
pointing to existing tools and frameworks developed by the Semantic Web
community. We illustrate our guidelines with concrete examples, in order to
help researchers implement these practices in their future vocabularies.
"
145,Games for Fairness and Interpretability,"  As Machine Learning (ML) systems becomes more ubiquitous, ensuring the fair
and equitable application of their underlying algorithms is of paramount
importance. We argue that one way to achieve this is to proactively cultivate
public pressure for ML developers to design and develop fairer algorithms --
and that one way to cultivate public pressure while simultaneously serving the
interests and objectives of algorithm developers is through gameplay. We
propose a new class of games -- ``games for fairness and interpretability'' --
as one example of an incentive-aligned approach for producing fairer and more
equitable algorithms. Games for fairness and interpretability are
carefully-designed games with mass appeal. They are inherently engaging,
provide insights into how machine learning models work, and ultimately produce
data that helps researchers and developers improve their algorithms. We
highlight several possible examples of games, their implications for fairness
and interpretability, how their proliferation could creative positive public
pressure by narrowing the gap between algorithm developers and the general
public, and why the machine learning community could benefit from them.
"
146,"Fairness in Bio-inspired Optimization Research: A Prescription of
  Methodological Guidelines for Comparing Meta-heuristics","  Bio-inspired optimization (including Evolutionary Computation and Swarm
Intelligence) is a growing research topic with many competitive bio-inspired
algorithms being proposed every year. In such an active area, preparing a
successful proposal of a new bio-inspired algorithm is not an easy task. Given
the maturity of this research field, proposing a new optimization technique
with innovative elements is no longer enough. Apart from the novelty, results
reported by the authors should be proven to achieve a significant advance over
previous outcomes from the state of the art. Unfortunately, not all new
proposals deal with this requirement properly. Some of them fail to select an
appropriate benchmark or reference algorithms to compare with. In other cases,
the validation process carried out is not defined in a principled way (or is
even not done at all). Consequently, the significance of the results presented
in such studies cannot be guaranteed. In this work we review several
recommendations in the literature and propose methodological guidelines to
prepare a successful proposal, taking all these issues into account. We expect
these guidelines to be useful not only for authors, but also for reviewers and
editors along their assessment of new contributions to the field.
"
147,Hierarchically Fair Federated Learning,"  When the federated learning is adopted among competitive agents with siloed
datasets, agents are self-interested and participate only if they are fairly
rewarded. To encourage the application of federated learning, this paper
employs a management strategy, i.e., more contributions should lead to more
rewards. We propose a novel hierarchically fair federated learning (HFFL)
framework. Under this framework, agents are rewarded in proportion to their
pre-negotiated contribution levels. HFFL+ extends this to incorporate
heterogeneous models. Theoretical analysis and empirical evaluation on several
datasets confirm the efficacy of our frameworks in upholding fairness and thus
facilitating federated learning in the competitive settings.
"
148,"Jealousy-freeness and other common properties in Fair Division of Mixed
  Manna","  We consider a fair division setting where indivisible items are allocated to
agents. Each agent in the setting has strictly negative, zero or strictly
positive utility for each item. We, thus, make a distinction between items that
are good for some agents and bad for other agents (i.e. mixed), good for
everyone (i.e. goods) or bad for everyone (i.e. bads). For this model, we study
axiomatic concepts of allocations such as jealousy-freeness up to one item,
envy-freeness up to one item and Pareto-optimality. We obtain many new
possibility and impossibility results in regard to combinations of these
properties. We also investigate new computational tasks related to such
combinations. Thus, we advance the state-of-the-art in fair division of mixed
manna.
"
149,Ensuring Fairness under Prior Probability Shifts,"  In this paper, we study the problem of fair classification in the presence of
prior probability shifts, where the training set distribution differs from the
test set. This phenomenon can be observed in the yearly records of several
real-world datasets, such as recidivism records and medical expenditure
surveys. If unaccounted for, such shifts can cause the predictions of a
classifier to become unfair towards specific population subgroups. While the
fairness notion called Proportional Equality (PE) accounts for such shifts, a
procedure to ensure PE-fairness was unknown.
  In this work, we propose a method, called CAPE, which provides a
comprehensive solution to the aforementioned problem. CAPE makes novel use of
prevalence estimation techniques, sampling and an ensemble of classifiers to
ensure fair predictions under prior probability shifts. We introduce a metric,
called prevalence difference (PD), which CAPE attempts to minimize in order to
ensure PE-fairness. We theoretically establish that this metric exhibits
several desirable properties.
  We evaluate the efficacy of CAPE via a thorough empirical evaluation on
synthetic datasets. We also compare the performance of CAPE with several
popular fair classifiers on real-world datasets like COMPAS (criminal risk
assessment) and MEPS (medical expenditure panel survey). The results indicate
that CAPE ensures PE-fair predictions, while performing well on other
performance metrics.
"
150,Fair Division: The Computer Scientist's Perspective,"  I survey recent progress on a classic and challenging problem in social
choice: the fair division of indivisible items. I discuss how a computational
perspective has provided interesting insights into and understanding of how to
divide items fairly and efficiently. This has involved bringing to bear tools
such as those used in knowledge representation, computational complexity,
approximation methods, game theory, online analysis and communication
complexity
"
151,"Why Fairness Cannot Be Automated: Bridging the Gap Between EU
  Non-Discrimination Law and AI","  This article identifies a critical incompatibility between European notions
of discrimination and existing statistical measures of fairness. First, we
review the evidential requirements to bring a claim under EU non-discrimination
law. Due to the disparate nature of algorithmic and human discrimination, the
EU's current requirements are too contextual, reliant on intuition, and open to
judicial interpretation to be automated. Second, we show how the legal
protection offered by non-discrimination law is challenged when AI, not humans,
discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes,
prejudice) and unintentional biases (e.g. organisational practices or
internalised stereotypes) which can act as a signal to victims that
discrimination has occurred. Finally, we examine how existing work on fairness
in machine learning lines up with procedures for assessing cases under EU
non-discrimination law. We propose ""conditional demographic disparity"" (CDD) as
a standard baseline statistical measurement that aligns with the European Court
of Justice's ""gold standard."" Establishing a standard set of statistical
evidence for automated discrimination cases can help ensure consistent
procedures for assessment, but not judicial interpretation, of cases involving
AI and automated systems. Through this proposal for procedural regularity in
the identification and assessment of automated discrimination, we clarify how
to build considerations of fairness into automated systems as far as possible
while still respecting and enabling the contextual approach to judicial
interpretation practiced under EU non-discrimination law.
  N.B. Abridged abstract
"
152,Reputation Agent: Prompting Fair Reviews in Gig Markets,"  Our study presents a new tool, Reputation Agent, to promote fairer reviews
from requesters (employers or customers) on gig markets. Unfair reviews,
created when requesters consider factors outside of a worker's control, are
known to plague gig workers and can result in lost job opportunities and even
termination from the marketplace. Our tool leverages machine learning to
implement an intelligent interface that: (1) uses deep learning to
automatically detect when an individual has included unfair factors into her
review (factors outside the worker's control per the policies of the market);
and (2) prompts the individual to reconsider her review if she has incorporated
unfair factors. To study the effectiveness of Reputation Agent, we conducted a
controlled experiment over different gig markets. Our experiment illustrates
that across markets, Reputation Agent, in contrast with traditional approaches,
motivates requesters to review gig workers' performance more fairly. We discuss
how tools that bring more transparency to employers about the policies of a gig
market can help build empathy thus resulting in reasoned discussions around
potential injustices towards workers generated by these interfaces. Our vision
is that with tools that promote truth and transparency we can bring fairer
treatment to gig workers.
"
153,"Ethical Adversaries: Towards Mitigating Unfairness with Adversarial
  Machine Learning","  Machine learning is being integrated into a growing number of critical
systems with far-reaching impacts on society. Unexpected behaviour and unfair
decision processes are coming under increasing scrutiny due to this widespread
use and its theoretical considerations. Individuals, as well as organisations,
notice, test, and criticize unfair results to hold model designers and
deployers accountable. We offer a framework that assists these groups in
mitigating unfair representations stemming from the training datasets. Our
framework relies on two inter-operating adversaries to improve fairness. First,
a model is trained with the goal of preventing the guessing of protected
attributes' values while limiting utility losses. This first step optimizes the
model's parameters for fairness. Second, the framework leverages evasion
attacks from adversarial machine learning to generate new examples that will be
misclassified. These new examples are then used to retrain and improve the
model in the first step. These two steps are iteratively applied until a
significant improvement in fairness is obtained. We evaluated our framework on
well-studied datasets in the fairness literature -- including COMPAS -- where
it can surpass other approaches concerning demographic parity, equality of
opportunity and also the model's utility. We also illustrate our findings on
the subtle difficulties when mitigating unfairness and highlight how our
framework can assist model designers.
"
154,Statistical Equity: A Fairness Classification Objective,"  Machine learning systems have been shown to propagate the societal errors of
the past. In light of this, a wealth of research focuses on designing solutions
that are ""fair."" Even with this abundance of work, there is no singular
definition of fairness, mainly because fairness is subjective and context
dependent. We propose a new fairness definition, motivated by the principle of
equity, that considers existing biases in the data and attempts to make
equitable decisions that account for these previous historical biases. We
formalize our definition of fairness, and motivate it with its appropriate
contexts. Next, we operationalize it for equitable classification. We perform
multiple automatic and human evaluations to show the effectiveness of our
definition and demonstrate its utility for aspects of fairness, such as the
feedback loop.
"
155,Fair Outlier Detection,"  An outlier detection method may be considered fair over specified sensitive
attributes if the results of outlier detection are not skewed towards
particular groups defined on such sensitive attributes. In this task, we
consider, for the first time to our best knowledge, the task of fair outlier
detection. In this work, we consider the task of fair outlier detection over
multiple multi-valued sensitive attributes (e.g., gender, race, religion,
nationality, marital status etc.). We propose a fair outlier detection method,
FairLOF, that is inspired by the popular LOF formulation for neighborhood-based
outlier detection. We outline ways in which unfairness could be induced within
LOF and develop three heuristic principles to enhance fairness, which form the
basis of the FairLOF method. Being a novel task, we develop an evaluation
framework for fair outlier detection, and use that to benchmark FairLOF on
quality and fairness of results. Through an extensive empirical evaluation over
real-world datasets, we illustrate that FairLOF is able to achieve significant
improvements in fairness at sometimes marginal degradations on result quality
as measured against the fairness-agnostic LOF method.
"
156,"Gender Slopes: Counterfactual Fairness for Computer Vision Models by
  Attribute Manipulation","  Automated computer vision systems have been applied in many domains including
security, law enforcement, and personal devices, but recent reports suggest
that these systems may produce biased results, discriminating against people in
certain demographic groups. Diagnosing and understanding the underlying true
causes of model biases, however, are challenging tasks because modern computer
vision systems rely on complex black-box models whose behaviors are hard to
decode. We propose to use an encoder-decoder network developed for image
attribute manipulation to synthesize facial images varying in the dimensions of
gender and race while keeping other signals intact. We use these synthesized
images to measure counterfactual fairness of commercial computer vision
classifiers by examining the degree to which these classifiers are affected by
gender and racial cues controlled in the images, e.g., feminine faces may
elicit higher scores for the concept of nurse and lower scores for STEM-related
concepts. We also report the skewed gender representations in an online search
service on profession-related keywords, which may explain the origin of the
biases encoded in the models.
"
157,Opportunistic Multi-aspect Fairness through Personalized Re-ranking,"  As recommender systems have become more widespread and moved into areas with
greater social impact, such as employment and housing, researchers have begun
to seek ways to ensure fairness in the results that such systems produce. This
work has primarily focused on developing recommendation approaches in which
fairness metrics are jointly optimized along with recommendation accuracy.
However, the previous work had largely ignored how individual preferences may
limit the ability of an algorithm to produce fair recommendations. Furthermore,
with few exceptions, researchers have only considered scenarios in which
fairness is measured relative to a single sensitive feature or attribute (such
as race or gender). In this paper, we present a re-ranking approach to
fairness-aware recommendation that learns individual preferences across
multiple fairness dimensions and uses them to enhance provider fairness in
recommendation results. Specifically, we show that our opportunistic and
metric-agnostic approach achieves a better trade-off between accuracy and
fairness than prior re-ranking approaches and does so across multiple fairness
dimensions.
"
158,Fair Classification via Unconstrained Optimization,"  Achieving the Bayes optimal binary classification rule subject to group
fairness constraints is known to be reducible, in some cases, to learning a
group-wise thresholding rule over the Bayes regressor. In this paper, we extend
this result by proving that, in a broader setting, the Bayes optimal fair
learning rule remains a group-wise thresholding rule over the Bayes regressor
but with a (possible) randomization at the thresholds. This provides a stronger
justification to the post-processing approach in fair classification, in which
(1) a predictor is learned first, after which (2) its output is adjusted to
remove bias. We show how the post-processing rule in this two-stage approach
can be learned quite efficiently by solving an unconstrained optimization
problem. The proposed algorithm can be applied to any black-box machine
learning model, such as deep neural networks, random forests and support vector
machines. In addition, it can accommodate many fairness criteria that have been
previously proposed in the literature, such as equalized odds and statistical
parity. We prove that the algorithm is Bayes consistent and motivate it,
furthermore, via an impossibility result that quantifies the tradeoff between
accuracy and fairness across multiple demographic groups. Finally, we conclude
by validating the algorithm on the Adult benchmark dataset.
"
159,What's Sex Got To Do With Fair Machine Learning?,"  Debate about fairness in machine learning has largely centered around
competing definitions of what fairness or nondiscrimination between groups
requires. However, little attention has been paid to what precisely a group is.
Many recent approaches to ""fairness"" require one to specify a causal model of
the data generating process. These exercises make an implicit ontological
assumption that a racial or sex group is simply a collection of individuals who
share a given trait. We show this by exploring the formal assumption of
modularity in causal models, which holds that the dependencies captured by one
causal pathway are invariant to interventions on any other pathways. Causal
models of sex propose two substantive claims: 1) There exists a feature,
sex-on-its-own, that is an inherent trait of an individual that causally brings
about social phenomena external to it in the world; and 2) the relations
between sex and its effects can be modified in whichever ways and the former
feature would still retain the meaning that sex has in our world. We argue that
this ontological picture is false. Many of the ""effects"" that sex purportedly
""causes"" are in fact constitutive features of sex as a social status. They give
the social meaning of sex features, meanings that are precisely what make sex
discrimination a distinctively morally problematic type of action. Correcting
this conceptual error has a number of implications for how models can be used
to detect discrimination. Formal diagrams of constitutive relations present an
entirely different path toward reasoning about discrimination. Whereas causal
diagrams guide the construction of sophisticated modular counterfactuals,
constitutive diagrams identify a different kind of counterfactual as central to
an inquiry on discrimination: one that asks how the social meaning of a group
would be changed if its non-modular features were altered.
"
160,Fairness-Aware Explainable Recommendation over Knowledge Graphs,"  There has been growing attention on fairness considerations recently,
especially in the context of intelligent decision making systems. Explainable
recommendation systems, in particular, may suffer from both explanation bias
and performance disparity. In this paper, we analyze different groups of users
according to their level of activity, and find that bias exists in
recommendation performance between different groups. We show that inactive
users may be more susceptible to receiving unsatisfactory recommendations, due
to insufficient training data for the inactive users, and that their
recommendations may be biased by the training records of more active users, due
to the nature of collaborative filtering, which leads to an unfair treatment by
the system. We propose a fairness constrained approach via heuristic re-ranking
to mitigate this unfairness problem in the context of explainable
recommendation over knowledge graphs. We experiment on several real-world
datasets with state-of-the-art knowledge graph-based explainable recommendation
algorithms. The promising results show that our algorithm is not only able to
provide high-quality explainable recommendations, but also reduces the
recommendation unfairness in several respects.
"
161,A quest for a fair schedule: The Young Physicists' Tournament,"  The Young Physicists Tournament is an established team-oriented scientific
competition between high school students from 37 countries on 5 continents. The
competition consists of scientific discussions called Fights. Three or four
teams participate in each Fight, each of whom presents a problem while rotating
the roles of Presenter, Opponent, Reviewer, and Observer among them.
  The rules of a few countries require that each team announce in advance 3
problems they will present at the national tournament. The task of the
organizers is to choose the composition of Fights in such a way that each team
presents each of its chosen problems exactly once and within a single Fight no
problem is presented more than once. Besides formalizing these feasibility
conditions, in this paper we formulate several additional fairness conditions
for tournament schedules. We show that the fulfillment of some of them can be
ensured by constructing suitable edge colorings in bipartite graphs. To find
fair schedules, we propose integer linear programs and test them on real as
well as randomly generated data.
"
162,"Fair Classification with Noisy Protected Attributes: A Framework with
  Provable Guarantees","  Due to the deployment of classification algorithms in a multitude of
applications directly and indirectly affecting people and society, developing
methods that are fair with respect to protected attributes such as gender or
race is crucial. However, protected attributes in datasets may be inaccurate
due to noise in the data collection or if the protected attributes are imputed
either in whole or in part. Such inaccuracies can prevent existing fair
classification algorithms from achieving their claimed fairness guarantees.
Motivated by this, recent works have studied the fair classification problem in
which a binary protected attribute is ""noisy"" (the protected type is flipped
with a known fixed probability) by either suggesting optimization using tighter
statistical or equalized odds constraints to counter the noise or by
identifying conditions under which prior equalized odds post-processing
algorithms can handle noisy attributes. We extend the study of noise-tolerant
fair classification to a very general setting. Our main contribution is an
optimization framework for learning a fair classifier in the presence of noisy
perturbations in the protected attributes that can be employed with linear and
linear-fractional class of fairness constraints, comes with probabilistic
guarantees on accuracy and fairness, and can handle multiple, non-binary
protected attributes. Empirically, we show that our framework can be used to
attain either statistical rate or false positive rate fairness guarantees with
a minimal loss in accuracy, even when the noise corruption is large in two
real-world datasets. Prior existing noisy fair classification approaches, on
the other hand, either do not always achieve the desired fairness levels or
suffer a larger loss in accuracy for guaranteeing high fairness compared to our
framework.
"
163,DeepFair: Deep Learning for Improving Fairness in Recommender Systems,"  The lack of bias management in Recommender Systems leads to minority groups
receiving unfair recommendations. Moreover, the trade-off between equity and
precision makes it difficult to obtain recommendations that meet both criteria.
Here we propose a Deep Learning based Collaborative Filtering algorithm that
provides recommendations with an optimum balance between fairness and accuracy
without knowing demographic information about the users. Experimental results
show that it is possible to make fair recommendations without losing a
significant proportion of accuracy.
"
164,Balancing Fairness and Efficiency in an Optimization Model,"  Optimization models generally aim for efficiency by maximizing total benefit
or minimizing cost. Yet a trade-off between fairness and efficiency is an
important element of many practical decisions. We propose a principled and
practical method for balancing these two criteria in an optimization model.
Following a critical assessment of existing schemes, we define a set of social
welfare functions (SWFs) that combine Rawlsian leximax fairness and
utilitarianism and overcome some of the weaknesses of previous approaches. In
particular, we regulate the equity/efficiency trade-off with a single parameter
that has a meaningful interpretation in practical contexts. We formulate the
SWFs using mixed integer constraints and sequentially maximize them subject to
constraints that define the problem at hand. After providing practical
step-by-step instructions for implementation, we demonstrate the method on
problems of realistic size involving healthcare resource allocation and
disaster preparation. The solution times are modest, ranging from a fraction of
a second to 18 seconds for a given value of the trade-off parameter.
"
165,System to Integrate Fairness Transparently: An Industry Approach,"  There have been significant research efforts to address the issue of
unintentional bias in Machine Learning (ML). Many well-known companies have
dealt with the fallout after the deployment of their products due to this
issue. In an industrial context, enterprises have large-scale ML solutions for
a broad class of use cases deployed for different swaths of customers. Trading
off the cost of detecting and mitigating bias across this landscape over the
lifetime of each use case against the risk of impact to the brand image is a
key consideration. We propose a framework for industrial uses that addresses
their methodological and mechanization needs. Our approach benefits from prior
experience handling security and privacy concerns as well as past internal ML
projects. Through significant reuse of bias handling ability at every stage in
the ML development lifecycle to guide users we can lower overall costs of
reducing bias.
"
166,Fair Influence Maximization: A Welfare Optimization Approach,"  Several social interventions (e.g., suicide and HIV prevention) leverage
social network information to maximize outreach. Algorithmic influence
maximization techniques have been proposed to aid with the choice of
influencers (or peer leaders) in such interventions. Traditional algorithms for
influence maximization have not been designed with social interventions in
mind. As a result, they may disproportionately exclude minority communities
from the benefits of the intervention. This has motivated research on fair
influence maximization. Existing techniques require committing to a single
domain-specific fairness measure. This makes it hard for a decision maker to
meaningfully compare these notions and their resulting trade-offs across
different applications.
  We address these shortcomings by extending the principles of cardinal welfare
to the influence maximization setting, which is underlain by complex
connections between members of different communities. We generalize the theory
regarding these principles and show under what circumstances these principles
can be satisfied by a welfare function. We then propose a family of welfare
functions that are governed by a single inequity aversion parameter which
allows a decision maker to study task-dependent trade-offs between fairness and
total influence and effectively trade off quantities like influence gap by
varying this parameter. We use these welfare functions as a fairness notion to
rule out undesirable allocations. We show that the resulting optimization
problem is monotone and submodular and can be solved with optimality
guarantees. Finally, we carry out a detailed experimental analysis on synthetic
and real social networks and should that high welfare can be achieved without
sacrificing the total influence significantly. Interestingly we can show there
exists welfare functions that empirically satisfy all of the principles.
"
167,Causal intersectionality for fair ranking,"  In this paper we propose a causal modeling approach to intersectional
fairness, and a flexible, task-specific method for computing intersectionally
fair rankings. Rankings are used in many contexts, ranging from Web search
results to college admissions, but causal inference for fair rankings has
received limited attention. Additionally, the growing literature on causal
fairness has directed little attention to intersectionality. By bringing these
issues together in a formal causal framework we make the application of
intersectionality in fair machine learning explicit, connected to important
real world effects and domain knowledge, and transparent about technical
limitations. We experimentally evaluate our approach on real and synthetic
datasets, exploring its behaviour under different structural assumptions.
"
168,Fair k-Means Clustering,"  We show that the popular $k$-means clustering algorithm (Lloyd's heuristic),
used for a variety of scientific data, can result in outcomes that are
unfavorable to subgroups of data (e.g., demographic groups). Such biased
clusterings can have deleterious implications for human-centric applications
such as resource allocation. We present a fair $k$-means objective and
algorithm to choose cluster centers that provide equitable costs for different
groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for
$k$-means, inheriting its simplicity, efficiency, and stability. In comparison
with standard Lloyd's, we find that on benchmark data sets, Fair-Lloyd exhibits
unbiased performance by ensuring that all groups have balanced costs in the
output $k$-clustering, while incurring a negligible increase in running time,
thus making it a viable fair option wherever $k$-means is currently used.
"
169,Probabilistic Fair Clustering,"  In clustering problems, a central decision-maker is given a complete metric
graph over vertices and must provide a clustering of vertices that minimizes
some objective function. In fair clustering problems, vertices are endowed with
a color (e.g., membership in a group), and the features of a valid clustering
might also include the representation of colors in that clustering. Prior work
in fair clustering assumes complete knowledge of group membership. In this
paper, we generalize prior work by assuming imperfect knowledge of group
membership through probabilistic assignments. We present clustering algorithms
in this more general setting with approximation ratio guarantees. We also
address the problem of ""metric membership"", where different groups have a
notion of order and distance. Experiments are conducted using our proposed
algorithms as well as baselines to validate our approach and also surface
nuanced concerns when group membership is not known deterministically.
"
170,Verifying Individual Fairness in Machine Learning Models,"  We consider the problem of whether a given decision model, working with
structured data, has individual fairness. Following the work of Dwork, a model
is individually biased (or unfair) if there is a pair of valid inputs which are
close to each other (according to an appropriate metric) but are treated
differently by the model (different class label, or large difference in
output), and it is unbiased (or fair) if no such pair exists. Our objective is
to construct verifiers for proving individual fairness of a given model, and we
do so by considering appropriate relaxations of the problem. We construct
verifiers which are sound but not complete for linear classifiers, and
kernelized polynomial/radial basis function classifiers. We also report the
experimental results of evaluating our proposed algorithms on publicly
available datasets.
"
171,"Machine Learning Pipelines: Provenance, Reproducibility and FAIR Data
  Principles","  Machine learning (ML) is an increasingly important scientific tool supporting
decision making and knowledge generation in numerous fields. With this, it also
becomes more and more important that the results of ML experiments are
reproducible. Unfortunately, that often is not the case. Rather, ML, similar to
many other disciplines, faces a reproducibility crisis. In this paper, we
describe our goals and initial steps in supporting the end-to-end
reproducibility of ML pipelines. We investigate which factors beyond the
availability of source code and datasets influence reproducibility of ML
experiments. We propose ways to apply FAIR data practices to ML workflows. We
present our preliminary results on the role of our tool, ProvBook, in capturing
and comparing provenance of ML experiments and their reproducibility using
Jupyter Notebooks.
"
172,A Framework for Fairness in Two-Sided Marketplaces,"  Many interesting problems in the Internet industry can be framed as a
two-sided marketplace problem. Examples include search applications and
recommender systems showing people, jobs, movies, products, restaurants, etc.
Incorporating fairness while building such systems is crucial and can have a
deep social and economic impact (applications include job recommendations,
recruiters searching for candidates, etc.). In this paper, we propose a
definition and develop an end-to-end framework for achieving fairness while
building such machine learning systems at scale. We extend prior work to
develop an optimization framework that can tackle fairness constraints from
both the source and destination sides of the marketplace, as well as dynamic
aspects of the problem. The framework is flexible enough to adapt to different
definitions of fairness and can be implemented in very large-scale settings. We
perform simulations to show the efficacy of our approach.
"
173,On the Applicability of ML Fairness Notions,"  ML-based predictive systems are increasingly used to support decisions with a
critical impact on individuals' lives such as college admission, job hiring,
child custody, criminal risk assessment, etc. As a result, fairness emerged as
an important requirement to guarantee that predictive systems do not
discriminate against specific individuals or entire sub-populations, in
particular, minorities. Given the inherent subjectivity of viewing the concept
of fairness, several notions of fairness have been introduced in the
literature. This paper is a survey of fairness notions that, unlike other
surveys in the literature, addresses the question of ""which notion of fairness
is most suited to a given real-world scenario and why?"". Our attempt to answer
this question consists in (1) identifying the set of fairness-related
characteristics of the real-world scenario at hand, (2) analyzing the behavior
of each fairness notion, and then (3) fitting these two elements to recommend
the most suitable fairness notion in every specific setup. The results are
summarized in a decision diagram that can be used by practitioners and policy
makers to navigate the relatively large catalogue of fairness notions.
"
174,"Fairness in machine learning: against false positive rate equality as a
  measure of fairness","  As machine learning informs increasingly consequential decisions, different
metrics have been proposed for measuring algorithmic bias or unfairness. Two
popular fairness measures are calibration and equality of false positive rate.
Each measure seems intuitively important, but notably, it is usually impossible
to satisfy both measures. For this reason, a large literature in machine
learning speaks of a fairness tradeoff between these two measures. This framing
assumes that both measures are, in fact, capturing something important. To
date, philosophers have not examined this crucial assumption, and examined to
what extent each measure actually tracks a normatively important property. This
makes this inevitable statistical conflict, between calibration and false
positive rate equality, an important topic for ethics. In this paper, I give an
ethical framework for thinking about these measures and argue that, contrary to
initial appearances, false positive rate equality does not track anything about
fairness, and thus sets an incoherent standard for evaluating the fairness of
algorithms.
"
175,Algorithmic Fairness in Education,"  Data-driven predictive models are increasingly used in education to support
students, instructors, and administrators. However, there are concerns about
the fairness of the predictions and uses of these algorithmic systems. In this
introduction to algorithmic fairness in education, we draw parallels to prior
literature on educational access, bias, and discrimination, and we examine core
components of algorithmic systems (measurement, model learning, and action) to
identify sources of bias and discrimination in the process of developing and
deploying these systems. Statistical, similarity-based, and causal notions of
fairness are reviewed and contrasted in the way they apply in educational
contexts. Recommendations for policy makers and developers of educational
technology offer guidance for how to promote algorithmic fairness in education.
"
176,"A Causal Linear Model to Quantify Edge Unfairness for Unfair Edge
  Prioritization and Discrimination Removal","  The dataset can be generated by an unfair mechanism in numerous settings. For
instance, a judicial system is unfair if it rejects the bail plea of an accused
based on the race. To mitigate the unfairness in the procedure generating the
dataset, we need to identify the sources of unfairness, quantify the unfairness
in these sources, quantify how these sources affect the overall unfairness, and
prioritize the sources before addressing the real-world issues underlying them.
Prior work of (Zhang, et. al, 2017) identifies and removes discrimination after
data is generated but does not suggest a methodology to mitigate unfairness in
the data generation phase. We use the notion of an unfair edge, same as
(Chiappa, et. al, 2018), to be the source of discrimination and quantify
unfairness along an unfair edge. We also quantify overall unfairness in a
particular decision towards a subset of sensitive attributes in terms of edge
unfairness and measure the sensitivity of the former when the latter is varied.
Using the formulation of cumulative unfairness in terms of edge unfairness, we
alter the discrimination removal methodology discussed in (Zhang, et. al, 2017)
by not formulating it as an optimization problem. This helps in getting rid of
constraints that grow exponentially in the number of sensitive attributes and
values taken by them. Finally, we discuss a priority algorithm for policymakers
to address the real-world issues underlying the edges that result in
unfairness. The experimental section validates the linear model assumption made
to quantify edge unfairness.
"
177,The Impossibility Theorem of Machine Fairness -- A Causal Perspective,"  With the increasing pervasive use of machine learning in social and economic
settings, there has been an interest in the notion of machine bias in the AI
community. Models trained on historic data reflect the biases that exist in
society and are propagated to the future through their decisions. A recent
study conducted by ProPublica revealed that the COMPAS recidivism prediction
tool was biased against the African-American community. There are three
prominent metrics of fairness used in the community, and it has been
statistically proved that it is impossible to satisfy them at the same time --
which has led to ambiguity about the definition of fairness. In this report,
causal perspective to the impossibility theorem of fairness is presented along
with a causal goal for machine fairness.
"
178,Fair Algorithms for Multi-Agent Multi-Armed Bandits,"  We propose a multi-agent variant of the classical multi-armed bandit problem,
in which there are N agents and K arms, and pulling an arm generates a
(possibly different) stochastic reward to each agent. Unlike the classical
multi-armed bandit problem, the goal is not to learn the ""best arm"", as each
agent may perceive a different arm as best for her. Instead, we seek to learn a
fair distribution over arms. Drawing on a long line of research in economics
and computer science, we use the Nash social welfare as our notion of fairness.
We design multi-agent variants of three classic multi-armed bandit algorithms,
and show that they achieve sublinear regret, now measured in terms of the Nash
social welfare.
"
179,"Algorithmic Stability in Fair Allocation of Indivisible Goods Among Two
  Agents","  We propose a notion of algorithmic stability for scenarios where cardinal
preferences are elicited. Informally, our definition captures the idea that an
agent should not experience a large change in their utility as long as they
make ""small"" or ""innocuous"" mistakes while reporting their preferences. We
study this notion in the context of fair and efficient allocations of
indivisible goods among two agents, and show that it is impossible to achieve
exact stability along with even a weak notion of fairness and even approximate
efficiency. As a result, we propose two relaxations to stability, namely,
approximate-stability and weak-approximate-stability, and show how existing
algorithms in the fair division literature that guarantee fair and efficient
outcomes perform poorly with respect to these relaxations. This leads us to the
explore the possibility of designing new algorithms that are more stable.
Towards this end we present a general characterization result for pairwise
maximin share allocations, and in turn use it to design an algorithm that is
approximately-stable and guarantees a pairwise maximin share and Pareto optimal
allocation for two agents. Finally, we present a simple framework that can be
used to modify existing fair and efficient algorithms in order to ensure that
they also achieve weak-approximate-stability.
"
180,Fairness-Aware Online Personalization,"  Decision making in crucial applications such as lending, hiring, and college
admissions has witnessed increasing use of algorithmic models and techniques as
a result of a confluence of factors such as ubiquitous connectivity, ability to
collect, aggregate, and process large amounts of fine-grained data using cloud
computing, and ease of access to applying sophisticated machine learning
models. Quite often, such applications are powered by search and recommendation
systems, which in turn make use of personalized ranking algorithms. At the same
time, there is increasing awareness about the ethical and legal challenges
posed by the use of such data-driven systems. Researchers and practitioners
from different disciplines have recently highlighted the potential for such
systems to discriminate against certain population groups, due to biases in the
datasets utilized for learning their underlying recommendation models. We
present a study of fairness in online personalization settings involving the
ranking of individuals. Starting from a fair warm-start machine-learned model,
we first demonstrate that online personalization can cause the model to learn
to act in an unfair manner if the user is biased in his/her responses. For this
purpose, we construct a stylized model for generating training data with
potentially biased features as well as potentially biased labels and quantify
the extent of bias that is learned by the model when the user responds in a
biased manner as in many real-world scenarios. We then formulate the problem of
learning personalized models under fairness constraints and present a
regularization based approach for mitigating biases in machine learning. We
demonstrate the efficacy of our approach through extensive simulations with
different parameter settings. Code:
https://github.com/groshanlal/Fairness-Aware-Online-Personalization
"
181,Predictability and Fairness in Social Sensing,"  In many applications, one may benefit from the collaborative collection of
data for sensing a physical phenomenon, which is known as social sensing. We
show how to make social sensing (1) predictable, in the sense of guaranteeing
that the number of queries per participant will be independent of the initial
state, in expectation, even when the population of participants varies over
time, and (2) fair, in the sense of guaranteeing that the number of queries per
participant will be equalised among the participants, in expectation, even when
the population of participants varies over time.
  In a use case, we consider a large, high-density network of participating
parked vehicles. When awoken by an administrative centre, this network proceeds
to search for moving, missing entities of interest using RFID-based techniques.
We regulate the number and geographical distribution of the parked vehicles
that are ""Switched On"" and thus actively searching for the moving entity of
interest. In doing so, we seek to conserve vehicular energy consumption while,
at the same time, maintaining good geographical coverage of the city such that
the moving entity of interest is likely to be located within an acceptable time
frame. Which vehicle participants are ""Switched On"" at any point in time is
determined periodically through the use of stochastic techniques. This is
illustrated on the example of a missing Alzheimer's patient in Melbourne,
Australia.
"
182,Online Task Scheduling for Fog Computing with Multi-Resource Fairness,"  In fog computing systems, one key challenge is online task scheduling, i.e.,
to decide the resource allocation for tasks that are continuously generated
from end devices. The design is challenging because of various uncertainties
manifested in fog computing systems; e.g., tasks' resource demands remain
unknown before their actual arrivals. Recent works have applied deep
reinforcement learning (DRL) techniques to conduct online task scheduling and
improve various objectives. However, they overlook the multi-resource fairness
for different tasks, which is key to achieving fair resource sharing among
tasks but in general non-trivial to achieve. Thusly, it is still an open
problem to design an online task scheduling scheme with multi-resource
fairness. In this paper, we address the above challenges. Particularly, by
leveraging DRL techniques and adopting the idea of dominant resource fairness
(DRF), we propose FairTS, an online task scheduling scheme that learns directly
from experience to effectively shorten average task slowdown while ensuring
multi-resource fairness among tasks. Simulation results show that FairTS
outperforms state-of-the-art schemes with an ultra-low task slowdown and better
resource fairness.
"
183,"Machine Learning Fairness in Justice Systems: Base Rates, False
  Positives, and False Negatives","  Machine learning best practice statements have proliferated, but there is a
lack of consensus on what the standards should be. For fairness standards in
particular, there is little guidance on how fairness might be achieved in
practice. Specifically, fairness in errors (both false negatives and false
positives) can pose a problem of how to set weights, how to make unavoidable
tradeoffs, and how to judge models that present different kinds of errors
across racial groups. This paper considers the consequences of having higher
rates of false positives for one racial group and higher rates of false
negatives for another racial group. The paper examines how different errors in
justice settings can present problems for machine learning applications, the
limits of computation for resolving tradeoffs, and how solutions might have to
be crafted through courageous conversations with leadership, line workers,
stakeholders, and impacted communities.
"
184,Memory networks for consumer protection:unfairness exposed,"  Recent work has demonstrated how data-driven AI methods can leverage consumer
protection by supporting the automated analysis of legal documents. However, a
shortcoming of data-driven approaches is poor explainability. We posit that in
this domain useful explanations of classifier outcomes can be provided by
resorting to legal rationales. We thus consider several configurations of
memory-augmented neural networks where rationales are given a special role in
the modeling of context knowledge. Our results show that rationales not only
contribute to improve the classification accuracy, but are also able to offer
meaningful, natural language explanations of otherwise opaque classifier
outcomes.
"
185,LiFT: A Scalable Framework for Measuring Fairness in ML Applications,"  Many internet applications are powered by machine learned models, which are
usually trained on labeled datasets obtained through either implicit / explicit
user feedback signals or human judgments. Since societal biases may be present
in the generation of such datasets, it is possible for the trained models to be
biased, thereby resulting in potential discrimination and harms for
disadvantaged groups. Motivated by the need for understanding and addressing
algorithmic bias in web-scale ML systems and the limitations of existing
fairness toolkits, we present the LinkedIn Fairness Toolkit (LiFT), a framework
for scalable computation of fairness metrics as part of large ML systems. We
highlight the key requirements in deployed settings, and present the design of
our fairness measurement system. We discuss the challenges encountered in
incorporating fairness tools in practice and the lessons learned during
deployment at LinkedIn. Finally, we provide open problems based on practical
experience.
"
186,"Learning Fair Policies in Multiobjective (Deep) Reinforcement Learning
  with Average and Discounted Rewards","  As the operations of autonomous systems generally affect simultaneously
several users, it is crucial that their designs account for fairness
considerations. In contrast to standard (deep) reinforcement learning (RL), we
investigate the problem of learning a policy that treats its users equitably.
In this paper, we formulate this novel RL problem, in which an objective
function, which encodes a notion of fairness that we formally define, is
optimized. For this problem, we provide a theoretical discussion where we
examine the case of discounted rewards and that of average rewards. During this
analysis, we notably derive a new result in the standard RL setting, which is
of independent interest: it states a novel bound on the approximation error
with respect to the optimal average reward of that of a policy optimal for the
discounted reward. Since learning with discounted rewards is generally easier,
this discussion further justifies finding a fair policy for the average reward
by learning a fair policy for the discounted reward. Thus, we describe how
several classic deep RL algorithms can be adapted to our fair optimization
problem, and we validate our approach with extensive experiments in three
different domains.
"
187,Improving Fair Predictions Using Variational Inference In Causal Models,"  The importance of algorithmic fairness grows with the increasing impact
machine learning has on people's lives. Recent work on fairness metrics shows
the need for causal reasoning in fairness constraints. In this work, a
practical method named FairTrade is proposed for creating flexible prediction
models which integrate fairness constraints on sensitive causal paths. The
method uses recent advances in variational inference in order to account for
unobserved confounders. Further, a method outline is proposed which uses the
causal mechanism estimates to audit black box models. Experiments are conducted
on simulated data and on a real dataset in the context of detecting unlawful
social welfare. This research aims to contribute to machine learning techniques
which honour our ethical and legal boundaries.
"
188,Adversarial Learning for Counterfactual Fairness,"  In recent years, fairness has become an important topic in the machine
learning research community. In particular, counterfactual fairness aims at
building prediction models which ensure fairness at the most individual level.
Rather than globally considering equity over the entire population, the idea is
to imagine what any individual would look like with a variation of a given
attribute of interest, such as a different gender or race for instance.
Existing approaches rely on Variational Auto-encoding of individuals, using
Maximum Mean Discrepancy (MMD) penalization to limit the statistical dependence
of inferred representations with their corresponding sensitive attributes. This
enables the simulation of counterfactual samples used for training the target
fair model, the goal being to produce similar outcomes for every alternate
version of any individual. In this work, we propose to rely on an adversarial
neural learning approach, that enables more powerful inference than with MMD
penalties, and is particularly better fitted for the continuous setting, where
values of sensitive attributes cannot be exhaustively enumerated. Experiments
show significant improvements in term of counterfactual fairness for both the
discrete and the continuous settings.
"
189,FairXGBoost: Fairness-aware Classification in XGBoost,"  Highly regulated domains such as finance have long favoured the use of
machine learning algorithms that are scalable, transparent, robust and yield
better performance. One of the most prominent examples of such an algorithm is
XGBoost. Meanwhile, there is also a growing interest in building fair and
unbiased models in these regulated domains and numerous bias-mitigation
algorithms have been proposed to this end. However, most of these
bias-mitigation methods are restricted to specific model families such as
logistic regression or support vector machine models, thus leaving modelers
with a difficult decision of choosing between fairness from the bias-mitigation
algorithms and scalability, transparency, performance from algorithms such as
XGBoost. We aim to leverage the best of both worlds by proposing a fair variant
of XGBoost that enjoys all the advantages of XGBoost, while also matching the
levels of fairness from the state-of-the-art bias-mitigation algorithms.
Furthermore, the proposed solution requires very little in terms of changes to
the original XGBoost library, thus making it easy for adoption. We provide an
empirical analysis of our proposed method on standard benchmark datasets used
in the fairness community.
"
190,Fairness in the Eyes of the Data: Certifying Machine-Learning Models,"  We present a framework that allows to certify the fairness degree of a model
based on an interactive and privacy-preserving test. The framework verifies any
trained model, regardless of its training process and architecture. Thus, it
allows us to evaluate any deep learning model on multiple fairness definitions
empirically. We tackle two scenarios, where either the test data is privately
available only to the tester or is publicly known in advance, even to the model
creator. We investigate the soundness of the proposed approach using
theoretical analysis and present statistical guarantees for the interactive
test. Finally, we provide a cryptographic technique to automate fairness
testing and certified inference with only black-box access to the model at hand
while hiding the participants' sensitive data.
"
191,A General Framework for Fairness in Multistakeholder Recommendations,"  Contemporary recommender systems act as intermediaries on multi-sided
platforms serving high utility recommendations from sellers to buyers. Such
systems attempt to balance the objectives of multiple stakeholders including
sellers, buyers, and the platform itself. The difficulty in providing
recommendations that maximize the utility for a buyer, while simultaneously
representing all the sellers on the platform has lead to many interesting
research problems.Traditionally, they have been formulated as integer linear
programs which compute recommendations for all the buyers together in an
\emph{offline} fashion, by incorporating coverage constraints so that the
individual sellers are proportionally represented across all the recommended
items. Such approaches can lead to unforeseen biases wherein certain buyers
consistently receive low utility recommendations in order to meet the global
seller coverage constraints. To remedy this situation, we propose a general
formulation that incorporates seller coverage objectives alongside individual
buyer objectives in a real-time personalized recommender system. In addition,
we leverage highly scalable submodular optimization algorithms to provide
recommendations to each buyer with provable theoretical quality bounds.
Furthermore, we empirically evaluate the efficacy of our approach using data
from an online real-estate marketplace.
"
192,"""And the Winner Is..."": Dynamic Lotteries for Multi-group Fairness-Aware
  Recommendation","  As recommender systems are being designed and deployed for an increasing
number of socially-consequential applications, it has become important to
consider what properties of fairness these systems exhibit. There has been
considerable research on recommendation fairness. However, we argue that the
previous literature has been based on simple, uniform and often uni-dimensional
notions of fairness assumptions that do not recognize the real-world
complexities of fairness-aware applications. In this paper, we explicitly
represent the design decisions that enter into the trade-off between accuracy
and fairness across multiply-defined and intersecting protected groups,
supporting multiple fairness metrics. The framework also allows the recommender
to adjust its performance based on the historical view of recommendations that
have been delivered over a time horizon, dynamically rebalancing between
fairness concerns. Within this framework, we formulate lottery-based mechanisms
for choosing between fairness concerns, and demonstrate their performance in
two recommendation domains.
"
193,"On the Identification of Fair Auditors to Evaluate Recommender Systems
  based on a Novel Non-Comparative Fairness Notion","  Decision-support systems are information systems that offer support to
people's decisions in various applications such as judiciary, real-estate and
banking sectors. Lately, these support systems have been found to be
discriminatory in the context of many practical deployments. In an attempt to
evaluate and mitigate these biases, algorithmic fairness literature has been
nurtured using notions of comparative justice, which relies primarily on
comparing two/more individuals or groups within the society that is supported
by such systems. However, such a fairness notion is not very useful in the
identification of fair auditors who are hired to evaluate latent biases within
decision-support systems. As a solution, we introduce a paradigm shift in
algorithmic fairness via proposing a new fairness notion based on the principle
of non-comparative justice. Assuming that the auditor makes fairness
evaluations based on some (potentially unknown) desired properties of the
decision-support system, the proposed fairness notion compares the system's
outcome with that of the auditor's desired outcome. We show that the proposed
fairness notion also provides guarantees in terms of comparative fairness
notions by proving that any system can be deemed fair from the perspective of
comparative fairness (e.g. individual fairness and statistical parity) if it is
non-comparatively fair with respect to an auditor who has been deemed fair with
respect to the same fairness notions. We also show that the converse holds true
in the context of individual fairness. A brief discussion is also presented
regarding how our fairness notion can be used to identify fair and reliable
auditors, and how we can use them to quantify biases in decision-support
systems.
"
194,"Addressing Fairness in Classification with a Model-Agnostic
  Multi-Objective Algorithm","  The goal of fairness in classification is to learn a classifier that does not
discriminate against groups of individuals based on sensitive attributes, such
as race and gender. One approach to designing fair algorithms is to use
relaxations of fairness notions as regularization terms or in a constrained
optimization problem. We observe that the hyperbolic tangent function can
approximate the indicator function. We leverage this property to define a
differentiable relaxation that approximates fairness notions provably better
than existing relaxations. In addition, we propose a model-agnostic
multi-objective architecture that can simultaneously optimize for multiple
fairness notions and multiple sensitive attributes and supports all statistical
parity-based notions of fairness. We use our relaxation with the
multi-objective architecture to learn fair classifiers. Experiments on public
datasets show that our method suffers a significantly lower loss of accuracy
than current debiasing algorithms relative to the unconstrained model.
"
195,On the Fairness of 'Fake' Data in Legal AI,"  The economics of smaller budgets and larger case numbers necessitates the use
of AI in legal proceedings. We examine the concept of disparate impact and how
biases in the training data lead to the search for fairer AI. This paper seeks
to begin the discourse on what such an implementation would actually look like
with a criticism of pre-processing methods in a legal context . We outline how
pre-processing is used to correct biased data and then examine the legal
implications of effectively changing cases in order to achieve a fairer outcome
including the black box problem and the slow encroachment on legal precedent.
Finally we present recommendations on how to avoid the pitfalls of
pre-processed data with methods that either modify the classifier or correct
the output in the final step.
"
196,"Fairness Matters -- A Data-Driven Framework Towards Fair and High
  Performing Facial Recognition Systems","  Facial recognition technologies are widely used in governmental and
industrial applications. Together with the advancements in deep learning (DL),
human-centric tasks such as accurate age prediction based on face images become
feasible. However, the issue of fairness when predicting the age for different
ethnicity and gender remains an open problem. Policing systems use age to
estimate the likelihood of someone to commit a crime, where younger suspects
tend to be more likely involved. Unfair age prediction may lead to unfair
treatment of humans not only in crime prevention but also in marketing,
identity acquisition and authentication. Therefore, this work follows two
parts. First, an empirical study is conducted evaluating performance and
fairness of state-of-the-art systems for age prediction including baseline and
most recent works of academia and the main industrial service providers (Amazon
AWS and Microsoft Azure). Building on the findings we present a novel approach
to mitigate unfairness and enhance performance, using distribution-aware
dataset curation and augmentation. Distribution-awareness is based on
out-of-distribution detection which is utilized to validate equal and diverse
DL system behavior towards e.g. ethnicity and gender. In total we train 24 DNN
models and utilize one million data points to assess performance and fairness
of the state-of-the-art for face recognition algorithms. We demonstrate an
improvement in mean absolute age prediction error from 7.70 to 3.39 years and a
4-fold increase in fairness towards ethnicity when compared to related work.
Utilizing the presented methodology we are able to outperform leading industry
players such as Amazon AWS or Microsoft Azure in both fairness and age
prediction accuracy and provide the necessary guidelines to assess quality and
enhance face recognition systems based on DL techniques.
"
197,Active Fairness Instead of Unawareness,"  The possible risk that AI systems could promote discrimination by reproducing
and enforcing unwanted bias in data has been broadly discussed in research and
society. Many current legal standards demand to remove sensitive attributes
from data in order to achieve ""fairness through unawareness"". We argue that
this approach is obsolete in the era of big data where large datasets with
highly correlated attributes are common. In the contrary, we propose the active
use of sensitive attributes with the purpose of observing and controlling any
kind of discrimination, and thus leading to fair results.
"
198,"Neither Private Nor Fair: Impact of Data Imbalance on Utility and
  Fairness in Differential Privacy","  Deployment of deep learning in different fields and industries is growing day
by day due to its performance, which relies on the availability of data and
compute. Data is often crowd-sourced and contains sensitive information about
its contributors, which leaks into models that are trained on it. To achieve
rigorous privacy guarantees, differentially private training mechanisms are
used. However, it has recently been shown that differential privacy can
exacerbate existing biases in the data and have disparate impacts on the
accuracy of different subgroups of data. In this paper, we aim to study these
effects within differentially private deep learning. Specifically, we aim to
study how different levels of imbalance in the data affect the accuracy and the
fairness of the decisions made by the model, given different levels of privacy.
We demonstrate that even small imbalances and loose privacy guarantees can
cause disparate impacts.
"
199,Justicia: A Stochastic SAT Approach to Formally Verify Fairness,"  As a technology ML is oblivious to societal good or bad, and thus, the field
of fair machine learning has stepped up to propose multiple mathematical
definitions, algorithms, and systems to ensure different notions of fairness in
ML applications. Given the multitude of propositions, it has become imperative
to formally verify the fairness metrics satisfied by different algorithms on
different datasets. In this paper, we propose a \textit{stochastic
satisfiability} (SSAT) framework, Justicia, that formally verifies different
fairness measures of supervised learning algorithms with respect to the
underlying data distribution. We instantiate Justicia on multiple
classification and bias mitigation algorithms, and datasets to verify different
fairness metrics, such as disparate impact, statistical parity, and equalized
odds. Justicia is scalable, accurate, and operates on non-Boolean and compound
sensitive attributes unlike existing distribution-based verifiers, such as
FairSquare and VeriFair. Being distribution-based by design, Justicia is more
robust than the verifiers, such as AIF360, that operate on specific test
samples. We also theoretically bound the finite-sample error of the verified
fairness measure.
"
200,Group Fairness by Probabilistic Modeling with Latent Fair Decisions,"  Machine learning systems are increasingly being used to make impactful
decisions such as loan applications and criminal justice risk assessments, and
as such, ensuring fairness of these systems is critical. This is often
challenging as the labels in the data are biased. This paper studies learning
fair probability distributions from biased data by explicitly modeling a latent
variable that represents a hidden, unbiased label. In particular, we aim to
achieve demographic parity by enforcing certain independencies in the learned
model. We also show that group fairness guarantees are meaningful only if the
distribution used to provide those guarantees indeed captures the real-world
data. In order to closely model the data distribution, we employ probabilistic
circuits, an expressive and tractable probabilistic model, and propose an
algorithm to learn them from incomplete data. We evaluate our approach on a
synthetic dataset in which observed labels indeed come from fair labels but
with added bias, and demonstrate that the fair labels are successfully
retrieved. Moreover, we show on real-world datasets that our approach not only
is a better model than existing methods of how the data was generated but also
achieves competitive accuracy.
"
201,"Understanding Fairness of Gender Classification Algorithms Across
  Gender-Race Groups","  Automated gender classification has important applications in many domains,
such as demographic research, law enforcement, online advertising, as well as
human-computer interaction. Recent research has questioned the fairness of this
technology across gender and race. Specifically, the majority of the studies
raised the concern of higher error rates of the face-based gender
classification system for darker-skinned people like African-American and for
women. However, to date, the majority of existing studies were limited to
African-American and Caucasian only. The aim of this paper is to investigate
the differential performance of the gender classification algorithms across
gender-race groups. To this aim, we investigate the impact of (a) architectural
differences in the deep learning algorithms and (b) training set imbalance, as
a potential source of bias causing differential performance across gender and
race. Experimental investigations are conducted on two latest large-scale
publicly available facial attribute datasets, namely, UTKFace and FairFace. The
experimental results suggested that the algorithms with architectural
differences varied in performance with consistency towards specific gender-race
groups. For instance, for all the algorithms used, Black females (Black race in
general) always obtained the least accuracy rates. Middle Eastern males and
Latino females obtained higher accuracy rates most of the time. Training set
imbalance further widens the gap in the unequal accuracy rates across all
gender-race groups. Further investigations using facial landmarks suggested
that facial morphological differences due to the bone structure influenced by
genetic and environmental factors could be the cause of the least performance
of Black females and Black race, in general.
"
202,"Differentially Private and Fair Deep Learning: A Lagrangian Dual
  Approach","  A critical concern in data-driven decision making is to build models whose
outcomes do not discriminate against some demographic groups, including gender,
ethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of
the sensitive attributes is essential, while, in practice, these attributes may
not be available due to legal and ethical requirements. To address this
challenge, this paper studies a model that protects the privacy of the
individuals sensitive information while also allowing it to learn
non-discriminatory predictors. The method relies on the notion of differential
privacy and the use of Lagrangian duality to design neural networks that can
accommodate fairness constraints while guaranteeing the privacy of sensitive
attributes. The paper analyses the tension between accuracy, privacy, and
fairness and the experimental evaluation illustrates the benefits of the
proposed model on several prediction tasks.
"
203,Fair Meta-Learning For Few-Shot Classification,"  Artificial intelligence nowadays plays an increasingly prominent role in our
life since decisions that were once made by humans are now delegated to
automated systems. A machine learning algorithm trained based on biased data,
however, tends to make unfair predictions. Developing classification algorithms
that are fair with respect to protected attributes of the data thus becomes an
important problem. Motivated by concerns surrounding the fairness effects of
sharing and few-shot machine learning tools, such as the Model Agnostic
Meta-Learning framework, we propose a novel fair fast-adapted few-shot
meta-learning approach that efficiently mitigates biases during meta-train by
ensuring controlling the decision boundary covariance that between the
protected variable and the signed distance from the feature vectors to the
decision boundary. Through extensive experiments on two real-world image
benchmarks over three state-of-the-art meta-learning algorithms, we empirically
demonstrate that our proposed approach efficiently mitigates biases on model
output and generalizes both accuracy and fairness to unseen tasks with a
limited amount of training samples.
"
204,Towards a Measure of Individual Fairness for Deep Learning,"  Deep learning has produced big advances in artificial intelligence, but
trained neural networks often reflect and amplify bias in their training data,
and thus produce unfair predictions. We propose a novel measure of individual
fairness, called prediction sensitivity, that approximates the extent to which
a particular prediction is dependent on a protected attribute. We show how to
compute prediction sensitivity using standard automatic differentiation
capabilities present in modern deep learning frameworks, and present
preliminary empirical results suggesting that prediction sensitivity may be
effective for measuring bias in individual predictions.
"
205,Fairness and Diversity for Rankings in Two-Sided Markets,"  Ranking items by their probability of relevance has long been the goal of
conventional ranking systems. While this maximizes traditional criteria of
ranking performance, there is a growing understanding that it is an
oversimplification in online platforms that serve not only a diverse user
population, but also the producers of the items. In particular, ranking
algorithms are expected to be fair in how they serve all groups of users -- not
just the majority group -- and they also need to be fair in how they divide
exposure among the items. These fairness considerations can partially be met by
adding diversity to the rankings, as done in several recent works, but we show
in this paper that user fairness, item fairness and diversity are fundamentally
different concepts. In particular, we find that algorithms that consider only
one of the three desiderata can fail to satisfy and even harm the other two. To
overcome this shortcoming, we present the first ranking algorithm that
explicitly enforces all three desiderata. The algorithm optimizes user and item
fairness as a convex optimization problem which can be solved optimally. From
its solution, a ranking policy can be derived via a new Birkhoff-von Neumann
decomposition algorithm that optimizes diversity. Beyond the theoretical
analysis, we provide a comprehensive empirical evaluation on a new benchmark
dataset to show the effectiveness of the proposed ranking algorithm on
controlling the three desiderata and the interplay between them.
"
206,Astraea: Grammar-based Fairness Testing,"  Software often produces biased outputs. In particular, machine learning (ML)
based software are known to produce erroneous predictions when processing
discriminatory inputs. Such unfair program behavior can be caused by societal
bias. In the last few years, Amazon, Microsoft and Google have provided
software services that produce unfair outputs, mostly due to societal bias
(e.g. gender or race). In such events, developers are saddled with the task of
conducting fairness testing. Fairness testing is challenging; developers are
tasked with generating discriminatory inputs that reveal and explain biases.
  We propose a grammar-based fairness testing approach (called ASTRAEA) which
leverages context-free grammars to generate discriminatory inputs that reveal
fairness violations in software systems. Using probabilistic grammars, ASTRAEA
also provides fault diagnosis by isolating the cause of observed software bias.
ASTRAEA's diagnoses facilitate the improvement of ML fairness.
  ASTRAEA was evaluated on 18 software systems that provide three major natural
language processing (NLP) services. In our evaluation, ASTRAEA generated
fairness violations with a rate of ~18%. ASTRAEA generated over 573K
discriminatory test cases and found over 102K fairness violations. Furthermore,
ASTRAEA improves software fairness by ~76%, via model-retraining.
"
207,"FairMixRep : Self-supervised Robust Representation Learning for
  Heterogeneous Data with Fairness constraints","  Representation Learning in a heterogeneous space with mixed variables of
numerical and categorical types has interesting challenges due to its complex
feature manifold. Moreover, feature learning in an unsupervised setup, without
class labels and a suitable learning loss function, adds to the problem
complexity. Further, the learned representation and subsequent predictions
should not reflect discriminatory behavior towards certain sensitive groups or
attributes. The proposed feature map should preserve maximum variations present
in the data and needs to be fair with respect to the sensitive variables. We
propose, in the first phase of our work, an efficient encoder-decoder framework
to capture the mixed-domain information. The second phase of our work focuses
on de-biasing the mixed space representations by adding relevant fairness
constraints. This ensures minimal information loss between the representations
before and after the fairness-preserving projections. Both the information
content and the fairness aspect of the final representation learned has been
validated through several metrics where it shows excellent performance. Our
work (FairMixRep) addresses the problem of Mixed Space Fair Representation
learning from an unsupervised perspective and learns a Universal representation
that is timely, unique, and a novel research contribution.
"
208,A Bandit-Based Algorithm for Fairness-Aware Hyperparameter Optimization,"  Considerable research effort has been guided towards algorithmic fairness but
there is still no major breakthrough. In practice, an exhaustive search over
all possible techniques and hyperparameters is needed to find optimal
fairness-accuracy trade-offs. Hence, coupled with the lack of tools for ML
practitioners, real-world adoption of bias reduction methods is still scarce.
To address this, we present Fairband, a bandit-based fairness-aware
hyperparameter optimization (HO) algorithm. Fairband is conceptually simple,
resource-efficient, easy to implement, and agnostic to both the objective
metrics, model types and the hyperparameter space being explored. Moreover, by
introducing fairness notions into HO, we enable seamless and efficient
integration of fairness objectives into real-world ML pipelines. We compare
Fairband with popular HO methods on four real-world decision-making datasets.
We show that Fairband can efficiently navigate the fairness-accuracy trade-off
through hyperparameter optimization. Furthermore, without extra training cost,
it consistently finds configurations attaining substantially improved fairness
at a comparatively small decrease in predictive accuracy.
"
209,Assessing the Fairness of Classifiers with Collider Bias,"  The increasing maturity of machine learning technologies and their
applications to decisions relate to everyday decision making have brought
concerns about the fairness of the decisions. However, current fairness
assessment systems often suffer from collider bias, which leads to a spurious
association between the protected attribute and the outcomes. To achieve
fairness evaluation on prediction models at the individual level, in this
paper, we develop the causality-based theorems to support the use of direct
causal effect estimation for fairness assessment on a given a classifier
without access to original training data. Based on the theorems, an unbiased
situation test method is presented to assess individual fairness of predictions
by a classifier, through the elimination of the impact of collider bias of the
classifier on the fairness assessment. Extensive experiments have been
performed on synthetic and real-world data to evaluate the performance of the
proposed method. The experimental results show that the proposed method reduces
bias significantly.
"
210,"Metrics and methods for a systematic comparison of fairness-aware
  machine learning algorithms","  Understanding and removing bias from the decisions made by machine learning
models is essential to avoid discrimination against unprivileged groups.
Despite recent progress in algorithmic fairness, there is still no clear answer
as to which bias-mitigation approaches are most effective. Evaluation
strategies are typically use-case specific, rely on data with unclear bias, and
employ a fixed policy to convert model outputs to decision outcomes. To address
these problems, we performed a systematic comparison of a number of popular
fairness algorithms applicable to supervised classification. Our study is the
most comprehensive of its kind. It utilizes three real and four synthetic
datasets, and two different ways of converting model outputs to decisions. It
considers fairness, predictive-performance, calibration quality, and speed of
28 different modelling pipelines, corresponding to both fairness-unaware and
fairness-aware algorithms. We found that fairness-unaware algorithms typically
fail to produce adequately fair models and that the simplest algorithms are not
necessarily the fairest ones. We also found that fairness-aware algorithms can
induce fairness without material drops in predictive power. Finally, we found
that dataset idiosyncracies (e.g., degree of intrinsic unfairness, nature of
correlations) do affect the performance of fairness-aware approaches. Our
results allow the practitioner to narrow down the approach(es) they would like
to adopt without having to know in advance their fairness requirements.
"
211,CryptoCredit: Securely Training Fair Models,"  When developing models for regulated decision making, sensitive features like
age, race and gender cannot be used and must be obscured from model developers
to prevent bias. However, the remaining features still need to be tested for
correlation with sensitive features, which can only be done with the knowledge
of those features. We resolve this dilemma using a fully homomorphic encryption
scheme, allowing model developers to train linear regression and logistic
regression models and test them for possible bias without ever revealing the
sensitive features in the clear. We demonstrate how it can be applied to
leave-one-out regression testing, and show using the adult income data set that
our method is practical to run.
"
212,Fairness Perception from a Network-Centric Perspective,"  Algorithmic fairness is a major concern in recent years as the influence of
machine learning algorithms becomes more widespread. In this paper, we
investigate the issue of algorithmic fairness from a network-centric
perspective. Specifically, we introduce a novel yet intuitive function known as
network-centric fairness perception and provide an axiomatic approach to
analyze its properties. Using a peer-review network as case study, we also
examine its utility in terms of assessing the perception of fairness in paper
acceptance decisions. We show how the function can be extended to a group
fairness metric known as fairness visibility and demonstrate its relationship
to demographic parity. We also illustrate a potential pitfall of the fairness
visibility measure that can be exploited to mislead individuals into perceiving
that the algorithmic decisions are fair. We demonstrate how the problem can be
alleviated by increasing the local neighborhood size of the fairness perception
function.
"
213,FaiR-N: Fair and Robust Neural Networks for Structured Data,"  Fairness in machine learning is crucial when individuals are subject to
automated decisions made by models in high-stake domains. Organizations that
employ these models may also need to satisfy regulations that promote
responsible and ethical A.I. While fairness metrics relying on comparing model
error rates across subpopulations have been widely investigated for the
detection and mitigation of bias, fairness in terms of the equalized ability to
achieve recourse for different protected attribute groups has been relatively
unexplored. We present a novel formulation for training neural networks that
considers the distance of data points to the decision boundary such that the
new objective: (1) reduces the average distance to the decision boundary
between two groups for individuals subject to a negative outcome in each group,
i.e. the network is more fair with respect to the ability to obtain recourse,
and (2) increases the average distance of data points to the boundary to
promote adversarial robustness. We demonstrate that training with this loss
yields more fair and robust neural networks with similar accuracies to models
trained without it. Moreover, we qualitatively motivate and empirically show
that reducing recourse disparity across groups also improves fairness measures
that rely on error rates. To the best of our knowledge, this is the first time
that recourse capabilities across groups are considered to train fairer neural
networks, and a relation between error rates based fairness and recourse based
fairness is investigated.
"
214,On the Fairness of Causal Algorithmic Recourse,"  While many recent works have studied the problem of algorithmic fairness from
the perspective of predictions, here we investigate the fairness of recourse
actions recommended to individuals to recover from an unfavourable
classification. To this end, we propose two new fairness criteria at the group
and individual level which---unlike prior work on equalising the average
distance from the decision boundary across protected groups---are based on a
causal framework that explicitly models relationships between input features,
thereby allowing to capture downstream effects of recourse actions performed in
the physical world. We explore how our criteria relate to others, such as
counterfactual fairness, and show that fairness of recourse is complementary to
fairness of prediction. We then investigate how to enforce fair recourse in the
training of the classifier. Finally, we discuss whether fairness violations in
the data generating process revealed by our criteria may be better addressed by
societal interventions and structural changes to the system, as opposed to
constraints on the classifier.
"
215,Equitable Allocation of Healthcare Resources with Fair Cox Models,"  Healthcare programs such as Medicaid provide crucial services to vulnerable
populations, but due to limited resources, many of the individuals who need
these services the most languish on waiting lists. Survival models, e.g. the
Cox proportional hazards model, can potentially improve this situation by
predicting individuals' levels of need, which can then be used to prioritize
the waiting lists. Providing care to those in need can prevent
institutionalization for those individuals, which both improves quality of life
and reduces overall costs. While the benefits of such an approach are clear,
care must be taken to ensure that the prioritization process is fair or
independent of demographic information-based harmful stereotypes. In this work,
we develop multiple fairness definitions for survival models and corresponding
fair Cox proportional hazards models to ensure equitable allocation of
healthcare resources. We demonstrate the utility of our methods in terms of
fairness and predictive accuracy on two publicly available survival datasets.
"
216,Representativity Fairness in Clustering,"  Incorporating fairness constructs into machine learning algorithms is a topic
of much societal importance and recent interest. Clustering, a fundamental task
in unsupervised learning that manifests across a number of web data scenarios,
has also been subject of attention within fair ML research. In this paper, we
develop a novel notion of fairness in clustering, called representativity
fairness. Representativity fairness is motivated by the need to alleviate
disparity across objects' proximity to their assigned cluster representatives,
to aid fairer decision making. We illustrate the importance of representativity
fairness in real-world decision making scenarios involving clustering and
provide ways of quantifying objects' representativity and fairness over it. We
develop a new clustering formulation, RFKM, that targets to optimize for
representativity fairness along with clustering quality. Inspired by the
$K$-Means framework, RFKM incorporates novel loss terms to formulate an
objective function. The RFKM objective and optimization approach guides it
towards clustering configurations that yield higher representativity fairness.
Through an empirical evaluation over a variety of public datasets, we establish
the effectiveness of our method. We illustrate that we are able to
significantly improve representativity fairness at only marginal impact to
clustering quality.
"
217,"Exchanging Lessons Between Algorithmic Fairness and Domain
  Generalization","  Standard learning approaches are designed to perform well on average for the
data distribution available at training time. Developing learning approaches
that are not overly sensitive to the training distribution is central to
research on domain- or out-of-distribution generalization, robust optimization
and fairness. In this work we focus on links between research on domain
generalization and algorithmic fairness -- where performance under a distinct
but related test distributions is studied -- and show how the two fields can be
mutually beneficial. While domain generalization methods typically rely on
knowledge of disjoint ""domains"" or ""environments"", ""sensitive"" label
information indicating which demographic groups are at risk of discrimination
is often used in the fairness literature. Drawing inspiration from recent
fairness approaches that improve worst-case performance without knowledge of
sensitive groups, we propose a novel domain generalization method that handles
the more realistic scenario where environment partitions are not provided. We
then show theoretically and empirically how different partitioning schemes can
lead to increased or decreased generalization performance, enabling us to
outperform Invariant Risk Minimization with handcrafted environments in
multiple cases. We also show how a re-interpretation of IRMv1 allows us for the
first time to directly optimize a common fairness criterion, group-sufficiency,
and thereby improve performance on a fair prediction task.
"
218,Explainability for fair machine learning,"  As the decisions made or influenced by machine learning models increasingly
impact our lives, it is crucial to detect, understand, and mitigate unfairness.
But even simply determining what ""unfairness"" should mean in a given context is
non-trivial: there are many competing definitions, and choosing between them
often requires a deep understanding of the underlying task. It is thus tempting
to use model explainability to gain insights into model fairness, however
existing explainability tools do not reliably indicate whether a model is
indeed fair. In this work we present a new approach to explaining fairness in
machine learning, based on the Shapley value paradigm. Our fairness
explanations attribute a model's overall unfairness to individual input
features, even in cases where the model does not operate on sensitive
attributes directly. Moreover, motivated by the linearity of Shapley
explainability, we propose a meta algorithm for applying existing training-time
fairness interventions, wherein one trains a perturbation to the original
model, rather than a new model entirely. By explaining the original model, the
perturbation, and the fair-corrected model, we gain insight into the
accuracy-fairness trade-off that is being made by the intervention. We further
show that this meta algorithm enjoys both flexibility and stability benefits
with no loss in performance.
"
219,Model Reconstruction from Model Explanations,"
We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.
"
220,Actionable Recourse in Linear Classification,"
Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.
"
221,Efficient Search for Diverse Coherent Explanations,"
This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a ""mixed polytope"" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.
"
222,On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection,"
Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.
"
223,Problem Formulation and Fairness,"
Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team---and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases---we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways---and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.
"
224,50 Years of Test (Un)fairness: Lessons for Machine Learning,"
Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.
"
225,Fairness and Abstraction in Sociotechnical Systems,"
A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.
"
226,"Clear Sanctions, Vague Rewards: How China's Social Credit System Currently Defines ""Good"" and ""Bad"" Behavior","
China's Social Credit System (SCS,  or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, ""good"" behavior can result in material rewards and reputational gain while ""bad"" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines ""good"" and ""bad"" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform ""Credit China"", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on ""good"" behavior) and the Blacklist (information on ""bad"" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.
"
227,A Taxonomy of Ethical Tensions in Inferring Mental Health States from Social Media,"
Powered by machine learning techniques, social media provides an unobtrusive lens into individual behaviors, emotions, and psychological states. Recent research has successfully employed social media data to predict mental health states of individuals, ranging from the presence and severity of mental disorders like depression to the risk of suicide. These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions. At the same time, the outcomes of this research can pose great risks to individuals, such as issues of incorrect, opaque algorithmic predictions, involvement of bad or unaccountable actors, and potential biases from intentional or inadvertent misuse of insights. Amplifying these tensions, there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions. This paper presents a taxonomy of these concerns and ethical challenges, drawing from existing literature, and poses questions to be resolved as this research gains traction. We identify three areas of tension: ethics committees and the gap of social media research; questions of validity, data, and machine learning; and implications of this research for key stakeholders. We conclude with calls to action to begin resolving these interdisciplinary dilemmas.
"
228,Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 Million People,"
A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care. To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs). We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate. An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t--1 (features), fine-grained care utilization data in year t -- 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks. The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health -- not just costs -- also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost -- for example, race. We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health. The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on -- cost -- is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care -- incentives that induce health systems to focus on dollars rather than health -- also has consequences for the way algorithms are built and monitored.
"
229,Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,"
Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.
"
230,An Empirical Study of Rich Subgroup Fairness for Machine Learning,"
Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.
"
231,The Profiling Potential of Computer Vision and the Challenge of Computational Empiricism,"
Computer vision and other biometrics data science applications have commenced a new project of profiling people. Rather than using 'transaction generated information', these systems measure the 'real world' and produce an assessment of the 'world state' - in this case an assessment of some individual trait. Instead of using proxies or scores to evaluate people, they increasingly deploy a logic of revealing the truth about reality and the people within it. While these profiling knowledge claims are sometimes tentative, they increasingly suggest that only through computation can these excesses of reality be captured and understood. This article explores the bases of those claims in the systems of measurement, representation, and classification deployed in computer vision. It asks if there is something new in this type of knowledge claim, sketches an account of a new form of computational empiricism being operationalised, and questions what kind of human subject is being constructed by these technological systems and practices. Finally, the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it.
"
232,Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting,"
We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are ""scrubbed,"" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.
"
233,Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations,"
To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups. To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.
"
234,Analyzing Biases in Perception of Truth in News Stories and Their Implications for Fact Checking,"
Recently, social media sites like Facebook and Twitter have been severely criticized by policy makers, and media watchdog groups for allowing fake news stories to spread unchecked on their platforms. In response, these sites are encouraging their users to report any news story they encounter on the site, which they perceive as fake. Stories that are reported as fake by a large number of users are prioritized for fact checking by (human) experts at fact checking organizations like Snopes and PolitiFact. Thus, social media sites today are relying on their users' perceptions of the truthfulness of news stories to select stories to fact check. However, few studies have focused on understanding how users perceive truth in news stories, or how biases in their perceptions might affect current strategies to detect and label fake news stories. To this end, we present an in-depth analysis on users' perceptions of truth in news stories. Specifically, we analyze users' truth perception biases for 150 stories fact checked by Snopes. Based on their ground truth and the truth value perceived by users, we can classify the stories into four categories -- (i) C1: false stories perceived as false by most users, (ii) C2: true stories perceived as false by most users, (iii) C3: false stories perceived as true by most users, and (iv) C4: true stories perceived as true by most users. The stories that are likely to be reported (flagged) for fact checking are from the two classes C1 and C2 that have the lowest perceived truth levels. We argue that there is little to be gained by fact checking stories from C1 whose truth value is correctly perceived by most users. Although stories in C2 reveal the cynicality of users about true stories, social media sites presently do not explicitly mark them as true to resolve the confusion. On the contrary, stories in C3 are false stories, yet perceived as true by most users. Arguably, these stories are more damaging than C1 because the truth values of the the story in former situation is incorrectly perceived while truth values of the latter is correctly perceived. Nevertheless, the stories in C1 is likely to be fact checked with greater priority than the stories in C3! In fact, in today's social media sites, the higher the gullibility of users towards believing a false story, the less likely it is to be reported for fact checking. In summary, we make the following contributions in this work. 1. Methodological: We develop a novel method for assessing users' truth perceptions of news stories. We design a test for users to rapidly assess (i.e., at the rate of a few seconds per story) how truthful or untruthful the claims in a news story are. We then conduct our truth perception tests on-line and gather truth perceptions of 100 US-based Amazon Mechanical Turk workers for each story. 2. Empirical: Our exploratory analysis of users' truth perceptions reveal several interesting insights. For instance, (i) for many stories, the collective wisdom of the crowd (average truth rating) differs significantly from the actual truth of the story, i.e., wisdom of crowds is inaccurate, (ii) across different stories, we find evidence for both false positive perception bias (i.e., a gullible user perceiving the story to be more true than it is in reality) and false negative perception bias (i.e., a cynical user perceiving a story to be more false than it is in reality), and (iii) users' political ideologies influence their truth perceptions for the most controversial stories, it is frequently the result of users' political ideologies influencing their truth perceptions. 3. Practical: Based on our observations, we call for prioritizing stories to fact check in order to achieve the following three important goals: (i) Remove false news stories from circulation, (ii) Correct the misperception of the users, and (iii) Decrease the disagreement between different users' perceptions of truth. Finally, we provide strategies which utilize users' truth perceptions (and predictive analysis of their biases) to achieve the three goals stated above while prioritizing stories for fact checking. The full paper is available at: https://bit.ly/2T7raFO
"
235,On Microtargeting Socially Divisive Ads: A Case Study of Russia-Linked Ad Campaigns on Facebook,"
Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are ""divisive"": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.
"
236,SIREN: A Simulation Framework for Understanding the Effects of Recommender Systems in Online News Environments,"
The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as ""Matthew effects"", ""filter bubbles"", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.
"
237,Controlling Polarization in Personalization: An Algorithmic Framework,"
Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but -- subject to these constraints -- will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.
"
238,Fair Algorithms for Learning in Allocation Problems,"
Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested. In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low. As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.
"
239,Fair Allocation through Competitive Equilibrium from Generic Incomes,"
Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a ""fair"" allocation of the items among them? Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to ""purchase"" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency -- prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible. We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.
"
240,A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity,"
We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.
"
241,Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing,"
Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.
"
242,Who's the Guinea Pig?: Investigating Online A/B/n Tests in-the-Wild,"
A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.
"
243,Fairness-Aware Programming,"
Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness. We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested. We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.
"
244,Model Cards for Model Reporting,"
Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.
"
245,The Social Cost of Strategic Classification,"
Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.
"
246,Downstream Effects of Affirmative Action,"
We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.
"
247,Access to Population-Level Signaling as a Source of Inequality,"
We identify and explore differential access to population-level signaling (also known as information design) as a source of unequal access to opportunity. A population-level signaler has potentially noisy observations of a binary type for each member of a population and, based on this, produces a signal about each member. A decision-maker infers types from signals and accepts those individuals whose type is high in expectation. We assume the signaler of the disadvantaged population reveals her observations to the decision-maker, whereas the signaler of the advantaged population forms signals strategically. We study the expected utility of the populations as measured by the fraction of accepted members, as well as the false positive rates (FPR) and false negative rates (FNR). We first show the intuitive results that for a fixed environment, the advantaged population has higher expected utility, higher FPR, and lower FNR, than the disadvantaged one (despite having identical population quality), and that more accurate observations improve the expected utility of the advantaged population while harming that of the disadvantaged one. We next explore the introduction of a publicly-observable signal, such as a test score, as a potential intervention. Our main finding is that this natural intervention, intended to reduce the inequality between the populations' utilities, may actually exacerbate it in settings where observations and test scores are noisy.
"
248,The Disparate Effects of Strategic Manipulation,"
When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed ""strategic manipulation,"" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to ""trick"" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off---even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's ""quality"" when agents' capacities to adaptively respond differ.
"
249,What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability,"
As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.
"
250,Algorithmic realism: expanding the boundaries of algorithmic thought,"
Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as ""algorithmic formalism"" and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking---""algorithmic realism""---that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.
"
251,Algorithmic accountability in public administration: the GDPR paradox,"
The EU General Data Protection Regulation (""GDPR"") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders. With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration. Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation. The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.
"
252,Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing,"
Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.
"
253,Toward situated interventions for algorithmic equity: lessons from the field,"
Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.
"
254,Explainability fact sheets: a framework for systematic assessment of explainable approaches,"
Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.
"
255,Multi-layered explanations from algorithmic impact assessments in the GDPR,"
Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability---individual rights and systemic governance--- and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.
"
256,The hidden assumptions behind counterfactual explanations and principal reasons,"
Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established ""principal reason"" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others. These ""feature-highlighting explanations"" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.
"
257,Why does my model fail?: contrastive local explanations for retail forecasting,"
In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be ""black boxes,"" even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a ""black-box"" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.
"
258,"""The human body is a black box"": supporting clinical decision-making with deep learning","
Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.
"
259,Assessing algorithmic fairness with unobserved protected class using data combination,"
The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.
"
260,FlipTest: fairness testing via optimal transport,"
We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.
"
261,"Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation","
Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.
"
262,Auditing radicalization pathways on YouTube,"
Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.
"
263,Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions,"
The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.
"
264,The concept of fairness in the GDPR: a linguistic and contextual interpretation,"
There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.
"
265,The concept of fairness in the GDPR: a linguistic and contextual interpretation,"
There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.
"
266,POTs: protective optimization technologies,"
Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems. We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial. We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.
"
267,Fair decision making using privacy-protected data,"
Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem. Our results show that if decisions are made using an -differentially private version of the data, under strict privacy constraints (smaller ), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.
"
268,Fairness warnings and fair-MAML: learning fairly with minimal data,"
Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.
"
269,Fairness warnings and fair-MAML: learning fairly with minimal data,"
Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.
"
270,Fairness warnings and fair-MAML: learning fairly with minimal data,"
Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.
"
271,"Whose side are ethics codes on?: power, responsibility and the social good","
The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the ""social good"", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..
"
272,"Algorithmic targeting of social policies: fairness, accuracy, and distributed governance","
Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them---and who is not---are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.
"
273,Roles for computing in social change,"
A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.
"
274,Roles for computing in social change,"
A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.
"
275,The relationship between trust in AI and trustworthy machine learning technologies,"
To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.
"
276,The philosophical basis of algorithmic recourse,"
Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse. We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.
"
277,The philosophical basis of algorithmic recourse,"
Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse. We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.
"
278,Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,"
Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.
"
279,Leave-one-out Unfairness,"
We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.
"
280,"Fairness, Welfare, and Equity in Personalized Pricing","
We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a ""triple bottom line"": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.
"
281,Re-imagining Algorithmic Fairness in India and Beyond,"
Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.
"
282,Narratives and Counternarratives on Data Sharing in Africa,"
As machine learning and data science applications grow ever more prevalent, there is an increased focus on data sharing and open data initiatives, particularly in the context of the African continent. Many argue that data sharing can support research and policy design to alleviate poverty, inequality, and derivative effects in Africa. Despite the fact that the datasets in question are often extracted from African communities, conversations around the challenges of accessing and sharing African data are too often driven by non-African stakeholders. These perspectives frequently employ a deficit narratives, often focusing on lack of education, training, and technological resources in the continent as the leading causes of friction in the data ecosystem. We argue that these narratives obfuscate and distort the full complexity of the African data sharing landscape. In particular, we use storytelling via fictional personas built from a series of interviews with African data experts to complicate dominant narratives and to provide counternarratives. Coupling these personas with research on data practices within the continent, we identify recurring barriers to data sharing as well as inequities in the distribution of data sharing benefits. In particular, we discuss issues arising from power imbalances resulting from the legacies of colonialism, ethno-centrism, and slavery, disinvestment in building trust, lack of acknowledgement of historical and present-day extractive practices, and Western-centric policies that are ill-suited to the African context. After outlining these problems, we discuss avenues for addressing them when sharing data generated in the continent.
"
283,This Whole Thing Smacks of Gender: Algorithmic Exclusion in Bioimpedance-based Body Composition Analysis,"
Smart weight scales offer bioimpedance-based body composition analysis as a supplement to pure body weight measurement. Companies such as Withings and Fitbit tout composition analysis as providing self-knowledge and the ability to make more informed decisions. However, these aspirational statements elide the reality that these numbers are a product of proprietary regression equations that require a binary sex/gender as their input. Our paper combines transgender studies-influenced personal narrative with an analysis of the scientific basis of bioimpedance technology used as part of the Withings smart scale. Attempting to include nonbinary people reveals that bioelectrical impedance analysis has always rested on physiologically shaky ground. White nonbinary people are merely the tip of the iceberg of those who may find that their smart scale is not so intelligent when it comes to their bodies. Using body composition analysis as an example, we explore how the problem of trans and nonbinary inclusion in personal health tech goes beyond the issues of adding a third ""gender"" box or slapping a rainbow flag on the packaging. We also provide recommendations as to how to approach creating more inclusive technologies even while still relying on exclusionary data.
"
284,Algorithmic Recourse: from Counterfactual Explanations to Interventions,"
As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -""how the world would have (had) to be different for a desirable outcome to occur""- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.
"
285,A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development,"
One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.
"
286,Measurement and Fairness,"
We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.
"
287,Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds,"
In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed. The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria. In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.
"
288,High Dimensional Model Explanations: An Axiomatic Approach,"
Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets. We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model. Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.
"
289,An Agent-based Model to Evaluate Interventions on Online Dating Platforms to Decrease Racial Homogamy,"
Perhaps the most controversial questions in the study of online platforms today surround the extent to which platforms can intervene to reduce the societal ills perpetrated on them. Up for debate is whether there exist any effective and lasting interventions a platform can adopt to address, e.g., online bullying, or if other, more far-reaching change is necessary to address such problems. Empirical work is critical to addressing such questions. But it is also challenging, because it is time-consuming, expensive, and sometimes limited to the questions companies are willing to ask. To help focus and inform this empirical work, we here propose an agent-based modeling (ABM) approach. As an application, we analyze the impact of a set of interventions on a simulated online dating platform on the lack of long-term interracial relationships in an artificial society. In the real world, a lack of interracial relationships are a critical vehicle through which inequality is maintained. Our work shows that many previously hypothesized interventions online dating platforms could take to increase the number of interracial relationships from their website have limited effects, and that the effectiveness of any intervention is subject to assumptions about sociocultural structure. Further, interventions that are effective in increasing diversity in long-term relationships are at odds with platforms' profit-oriented goals. At a general level, the present work shows the value of using an ABM approach to help understand the potential effects and side effects of different interventions that a platform could take.
"
290,Designing Accountable Systems,"
Accountability is an often called for property of technical systems. It is a requirement for algorithmic decision systems, autonomous cyber-physical systems, and for software systems in general. As a concept, accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power. This long history has also given us many, often slightly differing, definitions of accountability. The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a system's design. To enable the rigorous study of accountability in a system, we need models that are suitable for capturing such a varied concept. In this paper, we present a method to express and compare different definitions of accountability using Structural Causal Models. We show how these models can be used to evaluate a system's design and present a small use case based on an autonomous car.
"
291,Socially Fair k-Means Clustering,"
We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.
"
292,Towards Cross-Lingual Generalization of Translation Gender Bias,"
Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.
"
293,A Pilot Study in Surveying Clinical Judgments to Evaluate Radiology Report Generation,"
The recent release of many Chest X-Ray datasets has prompted a lot of interest in radiology report generation. To date, this has been framed as an image captioning task, where the machine takes an RGB image as input and generates a 2-3 sentence summary of findings as output. The quality of these reports has been canonically measured using metrics from the NLP community for language generation such as Machine Translation and Summarization. However, the evaluation metrics (e.g. BLEU, CIDEr) are inappropriate for the medical domain, where clinical correctness is critical. To address this, our team brought together machine learning experts with radiologists for a pilot study in co-designing a better metric for evaluating the quality of an algorithmically-generated radiology report. The interdisciplinary collaborative process involved multiple interviews, outreach, and preliminary annotation to design a larger scale study - which is now underway - to build a more meaningful evaluation tool.
"
294,Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,"
Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness
"
295,Operationalizing Framing to Support Multiperspective Recommendations of Opinion Pieces,"
Diversity in personalized news recommender systems is often defined as dissimilarity, and operationalized based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.
"
296,Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness,"
Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.
"
297,Fair Clustering via Equitable Group Representations,"
What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity. But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being ""close"" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common ""use case"" for clustering. For such a clustering to be fair, the centers should ""represent"" different groups equally well. We call such a clustering a group-representative clustering. In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting
"
298,You Can't Sit With Us: Exclusionary Pedagogy in AI Ethics Education,"
Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)---and its implications for AI---has led to the current ""ethics crisis"". However, we claim that the current AI ethics education space relies on a form of ""exclusionary pedagogy,"" where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as ""ethical unicorns"" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.
"
299,Fair Classification with Group-Dependent Label Noise,"
This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.
"
300,Censorship of Online Encyclopedias: Implications for NLP Models,"
While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.
"
301,Impossible Explanations?: Beyond explainable AI in the GDPR from a COVID-19 use case scenario,"
Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.
"
302,Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure,"
Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.
"
303,"Fairness, Equality, and Power in Algorithmic Decision-Making","
Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.
"
304,"One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision","
Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.
"
305,Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems,"
This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.
"
306,On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ,"
The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.
"
307,"Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI","
Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.
"
308,TILT: A GDPR-Aligned Transparency Information Language and Toolkit for Practical Privacy Engineering,"
In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do. We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty. Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.
"
