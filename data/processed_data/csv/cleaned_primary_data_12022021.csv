"X","title","abstract"
1,"unfair items detection educational measurement"," measurement professionals come agreement definition term item fairness paper continuous measure item unfairness proposed unfairness measure deviates zero less fair item measure exceeds cutoff value item identified definitely unfair new approach can identify unfair items identified conventional procedures results accord experts judgments item qualities since assumptions scores distributions andor correlations assumed method applicable educational test performance illustrated application scores real test "
2,"fairness academic course timetabling"," consider problem creating fair course timetables setting university motivation improve overall satisfaction individuals concerned students teachers etc providing fair timetable central idea undesirable arrangements course timetable ie violations soft constraints distributed fair way among individuals propose two formulations fair course timetabling problem based maxmin fairness jains fairness index respectively furthermore present experimentally evaluate optimization algorithm based simulated annealing solving maxmin fair course timetabling problems new contribution concerned measuring energy difference two timetables ie much worse timetable compared another timetable respect maxmin fairness introduce three different energy difference measures evaluate impact overall algorithm performance second proposed problem formulation focuses tradeoff fairness total amount soft constraint violations experimental evaluation shows known best solutions itc curriculumbased course timetabling instances quite fair respect jains fairness index however experiments also show fairness can improved rather small increase total amount soft constraint violations "
3,"safeguarding ecommerce advisor cheating behaviors towards robust trust models handling unfair ratings"," electronic marketplaces transaction buyers will rate products provided sellers decide trustworthy sellers transact buyers rely trust models leverage ratings evaluate reputation sellers although high effectiveness different trust models handling unfair ratings claimed designers recently argued models vulnerable intelligent attacks urgent demand robustness existing trust models evaluated comprehensive way work classify existing trust models two broad categories propose extendable emarketplace testbed evaluate robustness different unfair rating attacks comprehensively top highlighting robustness existing trust models handling unfair ratings far claimed propose validate novel combination mechanism existing trust models discountthenfilter notably enhance robustness investigated attacks "
4," decomposition maxmin fair curriculumbased course timetabling problem"," propose decomposition maxmin fair curriculumbased course timetabling mmfcbctt problem decomposition models room assignment subproblem generalized lexicographic bottleneck optimization problem lbop show generalized lbop can solved efficiently corresponding sum optimization problem can solved efficiently consequence room assignment subproblem mmfcbctt problem can solved efficiently use insight improve previously proposed heuristic algorithm mmfcbctt problem experimental results indicate using new decomposition improves performance algorithm itc test instances respect quality best solution found furthermore introduce measure quality solution maxmin fair optimization problem measure helps overcome limitations imposed qualitative nature maxmin fairness aids statistical evaluation performance randomized algorithms problems use measure show using new decomposition algorithm outperforms original one instances respect average solution quality "
5,"fair assignment indivisible objects ordinal preferences"," consider discrete assignment problem agents express ordinal preferences objects objects allocated agents fair manner use stochastic dominance relation fractional randomized allocations systematically define varying notions proportionality envyfreeness discrete assignments computational complexity checking whether fair assignment exists studied fairness notions also characterize conditions fair assignment guaranteed exist number fairness concepts polynomialtime algorithms presented check whether fair assignment exists algorithmic results also extend case unequal entitlements agents nphardness result holds several variants envyfreeness answers open question posed bouveret endriss lang ecai also propose fairness concepts always suggest nonempty set assignments meaningful fairness properties among concepts optimal proportionality optimal weak proportionality appear desirable fairness concepts "
6,"online fair division analysing food bank problem"," study online model fair division designed capture features real world charity problem consider two simple mechanisms model agents simply declare items like analyse several axiomatic properties mechanisms like strategyproofness envyfreeness finally perform competitive analysis compute price anarchy "
7," relation accuracy fairness binary classification"," study revisits problem accuracyfairness tradeoff binary classification argue comparison nondiscriminatory classifiers needs account different rates positive predictions otherwise conclusions performance may misleading accuracy discrimination naive baselines dataset vary different rates positive predictions provide methodological recommendations sound comparison nondiscriminatory classifiers present brief theoretical empirical analysis tradeoffs accuracy nondiscrimination "
8,"fair task allocation transportation"," task allocation problems traditionally focused cost optimization however attention given cases cost always sole major consideration paper study fair task allocation problem transportation optimal allocation low cost importantly distributes tasks even possible among heterogeneous participants different capacities costs execute tasks tackle fair minimum cost allocation problem analyze solve two parts using two novel polynomialtime algorithms show despite new fairness criterion proposed algorithms can solve fair minimum cost allocation problem optimally polynomial time addition conduct extensive set experiments investigate tradeoff cost minimization fairness experimental results demonstrate benefit factoring fairness task allocation among majority test instances fairness comes small price terms cost "
9,"efficiency sequenceability fair division indivisible goods additive preferences"," fair division indivisible goods using sequences sincere choices picking sequences natural way allocate objects idea following stage designated agent picks one object among remain paper restricted case agents numerical additive preferences objects revisits extent seminal paper brams king specific ordinal linear order preferences items point similarities differences latter context particular show paretooptimal allocation additive preferences sequenceable converse true anymore asymmetry leads naturally definition scale efficiency three steps paretooptimality sequenceability without paretooptimality nonsequenceability finally investigate links efficiency properties scale fairness described earlier work first show allocation can envyfree nonsequenceable every competitive equilibrium equal incomes sequenceable experimentally explore links scales efficiency fairness "
10,"fairness program property"," explore following question decisionmaking program fair useful definition fairness first describe several algorithmic fairness questions can phrased program verification problems second discuss automated verification technique proving disproving fairness decisionmaking programs respect probabilistic model population "
11,"fair division via social comparison"," classical cake cutting problem resource must divided among agents different utilities agent believes received fair share resource relative agents introduce variant problem model underlying social network agents graph agents evaluate shares relative neighbors network formulation captures many situations unrealistic assume global view also exposes interesting phenomena original problem specifically say allocation locally envyfree agent envies neighbors allocation locally proportional agent values allocation much average value neighbors allocations former implying latter global envyfreeness implies local envyfreeness global proportionality imply local proportionality vice versa general result two distinct graphs set nodes allocation exists set valuation functions allocation locally proportional one fully characterize set graphs oblivious singlecutter protocol protocol uses single agent cut cake pieces admits bounded protocol on query complexity locally envyfree allocations robertsonwebb model also consider price envyfreeness compares total utility optimal allocation best utility allocation envyfree show lower bound omegasqrtn price envyfreeness global allocations fact holds local envyfreeness connected undirected graph thus sparse graphs surprisingly provide flexibility respect quality envyfree allocations "
12,"balancing lexicographic fairness utilitarian objective application kidney exchange"," balancing fairness efficiency resource allocation classical economic computational problem price fairness measures worstcase loss economic efficiency using inefficient fair allocation rule indivisible goods many settings price unacceptably high one setting kidney exchange needy patients swap willing incompatible kidney donors work close open problem regarding theoretical price fairness modern kidney exchanges propose general hybrid fairness rule balances strict lexicographic preference ordering classes agents utilitarian objective maximizes economic efficiency develop utility function rule favors disadvantaged groups lexicographically cost overall efficiency becomes high switches utilitarian objective rule one parameter proportional bound price fairness can adjusted policymakers apply rule real data large kidney exchange show hybrid rule produces reliable outcomes fairness rules "
13,"fairjudge trustworthy user prediction rating platforms"," rating platforms enable largescale collection user opinion items products users etc however many untrustworthy users give fraudulent ratings excessive monetary gains paper present fairjudge system identify fraudulent users propose three metrics fairness user quantifies trustworthy user rating products ii reliability rating measures reliable rating iii goodness product measures quality product intuitively user fair provides reliable ratings close goodness product formulate mutually recursive definition metrics address cold start problems incorporate behavioral properties users products formulation propose iterative algorithm fairjudge predict values three metrics prove fairjudge guaranteed converge bounded number iterations linear time complexity conducting five different experiments five rating platforms show fairjudge significantly outperforms nine existing algorithms predicting fair unfair users reported unfair users flipkart network review fraud investigators users correctly identified accuracy fairjudge algorithm already deployed flipkart "
14,"beyond parity fairness objectives collaborative filtering"," study fairness collaborativefiltering recommender systems sensitive discrimination exists historical data biased data can lead collaborativefiltering methods make unfair predictions users minority groups identify insufficiency existing fairness metrics propose four new metrics address different forms unfairness fairness metrics can optimized adding fairness terms learning objective experiments synthetic real data show new metrics can better measure fairness baseline fairness objectives effectively help reduce unfairness "
15,"new fairness metrics recommendation embrace differences"," study fairness collaborativefiltering recommender systems sensitive discrimination exists historical data biased data can lead collaborative filtering methods make unfair predictions minority groups users identify insufficiency existing fairness metrics propose four new metrics address different forms unfairness fairness metrics can optimized adding fairness terms learning objective experiments synthetic real data show new metrics can better measure fairness baseline fairness objectives effectively help reduce unfairness "
16," impossibility fairness generalized impossibility result decisions"," various measures can used estimate bias unfairness predictor previous work already established measures incompatible show groups differ prevalence predicted event several intuitive reasonable measures fairness probability positive prediction given occurrence nonoccurrence probability occurrence given prediction nonprediction ratio predictions occurrences group mutually exclusive one equal among groups two must differ exceptions perfect trivial alwayspositive alwaysnegative predictors consequence nonperfect nontrivial predictor must necessarily unfair two three reasonable sets criteria result readily generalizes wide range wellknown statistical quantities sensitivity specificity false positive rate precision etc can divided three mutually exclusive groups importantly results applies predictors whether algorithmic human conclude possible ways handle effect assessing designing prediction methods "
17,"networked fairness cake cutting"," introduce graphical framework fair division cake cutting comparisons agents limited underlying network structure generalize classical fairness notions envyfreeness proportionality graphical setting given simple undirected graph g allocation envyfree g agent envies neighbors share proportional g every agent values share less average among neighbors respect measure generalizations open new research directions developing simple efficient algorithms can produce fair allocations specific graph structures algorithmic frontier first propose movingknife algorithm outputs envyfree allocation trees algorithm significantly simpler discrete bounded envyfree algorithm recently designed aziz mackenzie complete graphs next give discrete bounded algorithm computing proportional allocation descendant graphs class graphs taking rooted tree connecting ancestordescendant pairs "
18,"fairnessaware machine learning perspective"," algorithms learned data increasingly used deciding many aspects life movies see prices pay medicine get yet growing evidence decision making inappropriately trained algorithms may unintentionally discriminate people example automated matching candidate cvs job descriptions algorithms may capture propagate ethnicity related biases several repairs selected algorithms already proposed underlying mechanisms discrimination happens computational perspective yet scientifically understood need develop theoretical understanding algorithms may become discriminatory establish fundamental machine learning principles prevention need analyze machine learning process whole systematically explain roots discrimination occurrence will allow devise global machine learning optimization criteria guaranteed prevention opposed pushing empirical constraints existing algorithms casebycase result stateoftheart will advance heuristic repairing proactive theoretically supported prevention needed law requires protect vulnerable people penetration big data initiatives will increase computer science needs provide solid explanations accountability public public concerns lead unnecessarily restrictive regulations machine learning "
19,"fairness testing testing software discrimination"," paper defines software fairness discrimination develops testingbased method measuring much software discriminates focusing causality discriminatory behavior evidence software discrimination found modern software systems recommend criminal sentences grant access financial products determine allowed participate promotions approach themis generates efficient test suites measure discrimination given schema describing valid system inputs themis generates discrimination tests automatically require oracle evaluate themis software systems come prior work explicit focus avoiding discrimination find themis effective discovering software discrimination stateoftheart techniques removing discrimination algorithms fail many situations times discriminating much input subdomain themis optimizations effective producing efficient test suites measuring discrimination themis efficient systems exhibit discrimination thus demonstrate fairness testing critical aspect software development cycle domains possible discrimination provide initial tools measuring software discrimination "
20," formalizing fairness prediction machine learning"," machine learning algorithms prediction increasingly used critical decisions affecting human lives various fairness formalizations firm consensus yet employed prevent algorithms systematically discriminating people based certain attributes protected law aim article survey fairness formalized machine learning literature task prediction present formalizations corresponding notions distributive justice social sciences literature provide theoretical well empirical critiques notions social sciences literature explain critiques limit suitability corresponding fairness formalizations certain domains also suggest two notions distributive justice address critiques discuss avenues prospective fairness formalizations "
21,"twostage algorithm fairnessaware machine learning"," algorithmic decision making process now affects many aspects lives standard tools machine learning classification regression subject bias data thus direct application offtheshelf tools lead specific group unfairly discriminated removing sensitive attributes data solve problem textitdisparate impact can arise nonsensitive attributes sensitive attributes correlated study fair machine learning algorithm avoids disparate impact making decision inspired twostage least squares method widely used field economics propose twostage algorithm removes bias training data proposed algorithm conceptually simple unlike existing fair algorithms designed classification tasks proposed method able deal regression tasks ii combine explanatory attributes remove reverse discrimination iii deal numerical sensitive attributes performance fairness proposed algorithm evaluated simulations synthetic realworld datasets "
22,"multiwinner voting fairness constraints"," multiwinner voting rules used select small representative subset candidates items larger set given preferences voters however candidates sensitive attributes gender ethnicity selecting committee specified types political leaning selecting subset news items algorithm chooses subset optimizing multiwinner voting rule may unbalanced selection may represent particular gender political orientation examples introduce algorithmic framework multiwinner voting problems additional requirement selected subset fair respect given set attributes framework provides flexibility specify fairness respect multiple nondisjoint attributes eg ethnicity gender specify score function study computational complexity constrained multiwinner voting problem monotone submodular score functions present several approximation algorithms matching hardness approximation results various attribute group structure types score functions also present simulations suggest adding fairness constraints may affect scores significantly compared unconstrained case "
23,"groupwise maximin fair allocation indivisible goods"," study problem allocating indivisible goods among n agents fair manner problem maximin share mms wellstudied solution concept provides fairness threshold specifically maximin share defined minimum utility agent can guarantee asked partition set goods n bundles remaining n agents pick bundles adversarially allocation deemed fair every agent gets bundle whose valuation least maximin share even though maximin shares provide natural benchmark fairness drawbacks particular sufficient rule unsatisfactory allocations motivated considerations work define stronger notion fairness called groupwise maximin share guarantee gmms gmms require maximin share guarantee achieved just respect grand bundle also among subgroups agents hence solution concept strengthens mms provides expost fairness guarantee show specific settings gmms allocations always exist also establish existence approximate gmms allocations additive valuations develop polynomialtime algorithm find allocations moreover establish scale fairness wherein show gmms implies approximate envy freeness finally empirically demonstrate existence gmms allocations large set randomly generated instances set instances additionally show algorithm achieves approximation factor better established worstcase bound "
24,"fairness supervised learning information theoretic approach"," automated decision making systems increasingly used realworld applications systems part decision rules derived minimizing training error available historical data therefore bias related sensitive attribute gender race religion etc data say due culturalhistorical discriminatory practices certain demographic system continue discrimination decisions including said bias decision rule present information theoretic framework designing fair predictors data aim prevent discrimination specified sensitive attribute supervised learning setting use equalized odds criterion discrimination demands prediction independent protected attribute conditioned actual label ensure fairness generalization simultaneously compress data auxiliary variable used prediction task auxiliary variable chosen decontaminated discriminatory attribute sense equalized odds final predictor obtained applying bayesian decision rule auxiliary variable "
25,"value alignment fair play rights service robots"," ethics safety research artificial intelligence increasingly framed terms alignment human values interests argue turings call fair play machines early often overlooked contribution alignment literature turings appeal fair play suggests need correct human behavior accommodate machines surprising inversion value alignment treated today reflections fair play motivate novel interpretation turings notorious imitation game condition intelligence instead value alignment machine demonstrates minimal degree alignment norms conversation instance can go undetected interrogated human carefully distinguish interpretation moral turing test motivated principle fair play instead depends imitation human moral behavior finally consider framework fair play can used situate debate robot rights within alignment literature argue extending rights service robots operating public spaces fair precisely sense encourages alignment interests humans machines "
26,"reinforcement learning fair dynamic pricing"," unfair pricing policies shown one negative perceptions customers can concerning pricing may result longterm losses company despite fact dynamic pricing models help companies maximize revenue fairness equality taken account order avoid unfair price differences groups customers paper shows solve dynamic pricing using reinforcement learning rl techniques prices maximized keeping balance revenue fairness demonstrate rl provides two main features support fairness dynamic pricing one hand rl able learn recent experience adapting pricing policy complex market environments hand provides tradeoff short longterm objectives hence integrating fairness models core considering two features propose application rl revenue optimization additional integration fairness part learning procedure using jains index metric results simulated environment show significant improvement fairness time maintaining optimisation revenue "
27,"deep bayesian trust dominant fair incentive mechanism crowd"," important class gametheoretic incentive mechanisms eliciting effort crowd peer based mechanisms workers paid matching answers one another classic mechanism workers solve gold standard tasks pay according accuracy gold tasks mechanism ensures stronger incentive compatibility peer based mechanisms assigning gold tasks workers becomes inefficient large scale propose novel mechanism assigns gold tasks workers exploits transitivity derive accuracy rest workers peers accuracy show resulting mechanism ensures dominant notion incentive compatibility fairness "
28,"fair division cardinality constraints"," consider problem fairly allocating indivisible goods among agents cardinality constraints additive valuations setting given partition entire set goodsie goods categorizedand limit specified number goods can allocated category agent objective find fair allocation subset goods assigned agent satisfies given cardinality constraints problem naturally captures number resourceallocation applications generalization wellstudied unconstrained fair division problem two central notions fairness context fair division indivisible goods envy freeness one good ef approximate maximin share guarantee mms show existence algorithmic guarantees established solution concepts unconstrained setting can essentially achieved cardinality constraints specifically develop efficient algorithms compute ef approximately mms allocations constrained setting furthermore focusing case wherein agents additive valuation establish ef allocations exist can computed efficiently even matroid constraints "
29,"claudette automated detector potentially unfair clauses online terms service"," terms service online platforms often contain clauses potentially unfair consumer present experimental study machine learning employed automatically detect potentially unfair clauses results show proposed system provide valuable tool lawyers consumers alike "
30,"causal reasoning algorithmic fairness"," work argue importance causal reasoning creating fair algorithms decision making give review existing approaches fairness describe work causality necessary understanding causal approaches argue causality necessary approach wishes fair give detailed analysis many recent approaches causalitybased fairness "
31,"pooling causal models counterfactual fairness via causal judgement aggregation"," paper consider problem combining multiple probabilistic causal models provided different experts requirement aggregated model satisfy criterion counterfactual fairness build upon work causal models fairness machine learning express problem combining multiple models within framework opinion pooling propose two simple algorithms grounded theory counterfactual fairness causal judgment aggregation guaranteed generate aggregated probabilistic causal models respecting criterion fairness compare behaviors toy case study "
32,"causal interventions fairness"," approaches algorithmic fairness constrain machine learning methods resulting predictions satisfy one several intuitive notions fairness may help private companies comply nondiscrimination laws avoid negative publicity believe often little late time training data collected individuals disadvantaged groups already suffered discrimination lost opportunities due factors control present work focus instead interventions new public policy particular maximize positive effects improving fairness overall system use causal methods model effects interventions allowing potential interferenceeach individuals outcome may depend else receives intervention demonstrate example allocating budget teaching resources using dataset schools new york city "
33,"lecture notes fair division"," fair division problem dividing one several goods amongst two agents way satisfies suitable fairness criterion notes provide succinct introduction field cover three main topics first need define understood fair allocation goods individuals present overview important fairness criteria well closely related criteria economic efficiency developed literature together short discussion axiomatic foundations second give introduction cakecutting procedures example methods fairly dividing single divisible resource amongst group individuals third discuss combinatorial optimisation problem fairly allocating set indivisible goods group agents covering centralised algorithms similar auctions distributed approach based negotiation classical literature fair division largely developed within economics notes specifically written readers background computer science similar may may wish engaged research artificial intelligence multiagent systems computational social choice references reading well small number exercises included notes prepared tutorial th european agent systems summer school easss torino italy august september updated tutorial costadt doctoral school computational social choice estoril portugal april "
34,"fairness behind veil ignorance welfare analysis automated decision making"," draw attention important yet largely overlooked aspect evaluating fairness automated decision making systemsnamely risk welfare considerations proposed family measures corresponds longestablished formulations cardinal social welfare economics justified rawlsian conception fairness behind veil ignorance convex formulation welfarebased measures fairness allows us integrate constraint convex loss minimization pipeline empirical analysis reveals interesting tradeoffs proposal prediction accuracy b group discrimination c dwork et als notion individual fairness furthermore perhaps importantly work provides heuristic justification empirical evidence suggesting lowerbound measures often leads bounded inequality algorithmic outcomes hence presenting first computationally feasible mechanism bounding individuallevel inequality "
35,"comparing fairness criteria based social outcome"," fairness algorithmic decisionmaking processes attracting increasing concern algorithm applied humanrelated decisionmaking estimator solely optimizing predictive power can learn biases existing data motivates us notion fairness machine learning several different notions studied literature little studies done notions affect individuals demonstrate comparison several policies induced wellknown fairness criteria including colorblind cb demographic parity dp equalized odds eo show eo criterion among removes grouplevel disparity empirical studies social welfare disparity policies conducted "
36," applied fairness"," machine learning practitioners often ambivalent ethical aspects products believe anything gets us current state one systems achieving degree fairness improvement welcomed true even progress get us way goal complete fairness perfectly align personal belief measure fairness used measure fairness built still put us better position status quo impediments getting fairness ethical concerns applied real applications whether abstruse philosophical debates technical overhead introduction ever hyperparameters avoided paper elaborate argument viewpoint importance "
37,"classification fairness constraints metaalgorithm provable guarantees"," developing classification algorithms fair respect sensitive attributes data become important problem due growing deployment classification algorithms various social contexts several recent works focused fairness respect specific metric modeled corresponding fair classification problem constrained optimization problem developed tailored algorithms solve despite still remain important metrics fair classifiers many aforementioned algorithms come theoretical guarantees perhaps resulting optimization problem nonconvex main contribution paper new metaalgorithm classification takes input large class fairness constraints respect multiple nondisjoint sensitive attributes comes provable guarantees achieved first developing metaalgorithm large family classification problems convex constraints showing classification problems general types fairness constraints can reduced family present empirical results show algorithm can achieve nearperfect fairness respect various fairness metrics loss accuracy due imposed fairness constraints often small overall work unifies several prior works fair classification presents practical algorithm theoretical guarantees can handle fairness metrics previously possible "
38,"automated directed fairness testing"," fairness critical trait decision making machinelearning models increasingly used sensitive application domains eg education employment decision making crucial decisions computed models free unintended bias can automatically validate fairness arbitrary machinelearning models given machinelearning model set sensitive input parameters aequitas approach automatically discovers discriminatory inputs highlight fairness violation core aequitas three novel strategies employ probabilistic search input space objective uncovering fairness violation aequitas approach leverages inherent robustness property common machinelearning models design implement scalable test generation methodologies appealing feature generated test inputs can systematically added training set underlying model improve fairness end design fully automated module guarantees improve fairness underlying model implemented aequitas evaluated six stateoftheart classifiers including classifier designed fairness constraints show aequitas effectively generates inputs uncover fairness violation subject classifiers systematically improves fairness respective models using generated test inputs evaluation aequitas generates discriminatory inputs wrt total number inputs generated leverages inputs improve fairness "
39,"fairly allocating many goods queries"," investigate query complexity fair allocation indivisible goods two agents arbitrary monotonic valuations design algorithm computes allocation satisfying envyfreeness one good ef relaxation envyfreeness using logarithmic number queries show logarithmic query complexity bound also holds three agents additive valuations results suggest possible fairly allocate goods practice even number goods extremely large contrast prove computing allocation satisfying envyfreeness another relaxations envyfreeness good efx requires linear number queries even two agents identical additive valuations "
40,"efficiency sequenceability dealoptimality fair division indivisible goods"," fair division indivisible goods using sequences sincere choices picking sequences natural way allocate objects idea follows stage designated agent picks one object among remain another intuitive way obtain allocation give objects agents first place let agents exchange long deals beneficial paper investigates notions agents additive preferences objects unveils surprising connections efficiency fairness notions particular show allocation sequenceable iff optimal certain type deals namely cycle deals involving single object furthermore paretooptimal allocation sequenceable converse regarding fairness show allocation can envyfree nonsequenceable every competitive equilibrium equal incomes sequenceable complete picture show domain restrictions may affect relations notions finally experimentally explore links scales efficiency fairness "
41,"optimization nondifferentiable constraints applications fairness recall churn goals"," show many machine learning goals improved fairness metrics can expressed constraints models predictions call rate constraints study problem training nonconvex models subject rate constraints nonconvex nondifferentiable constraints nonconvex setting standard approach lagrange multipliers may fail furthermore constraints nondifferentiable one optimize lagrangian gradientbased methods solve issues introduce proxylagrangian formulation new formulation leads algorithm produces stochastic classifier playing twoplayer nonzerosum game solving call semicoarse correlated equilibrium turn corresponds approximately optimal feasible solution constrained optimization problem give procedure shrinks randomized solution one mixture m deterministic solutions given m constraints culminates algorithms can solve nonconvex constrained optimization problems possibly nondifferentiable nonconvex constraints theoretical guarantees provide extensive experimental results enforcing wide range policy goals including different fairness metrics goals accuracy coverage recall churn "
42,"fair lending needs explainable models responsible recommendation"," financial services industry unique explainability fairness challenges arising compliance ethical considerations credit decisioning challenges complicate use model machine learning artificial intelligence methods business decision processes "
43,"fairnessaware classification criterion convexity bounds"," fairnessaware classification receiving increasing attention machine learning fields recently research proposes formulate fairnessaware classification constrained optimization problems however several limitations exist previous works due lack theoretical framework guiding formulation paper propose general framework learning fair classifiers addresses previous limitations framework formulates various commonlyused fairness metrics convex constraints can directly incorporated classic classification models within framework propose constraintfree criterion training data ensures classifier learned data fair also derive constraints ensure real fairness metric satisfied surrogate functions used achieve convexity framework can used formulating fairnessaware classification fairness guarantee computational efficiency experiments using realworld datasets demonstrate theoretical results show effectiveness proposed framework methods "
44,"active fairness algorithmic decision making"," society increasingly relies machine learning models automated decision making yet efficiency gains automation come paired concern algorithmic discrimination can systematize inequality recent work proposed optimal postprocessing methods randomize classification decisions fraction individuals order achieve fairness measures related parity errors calibration methods however raised concern due information inefficiency intragroup unfairness pareto suboptimality entail present work proposes alternative active framework fair classification deployment decisionmaker adaptively acquires information according needs different groups individuals towards balancing disparities classification performance propose two methods information collection adapted group individuallevel needs respectively show realworld datasets can achieve calibration single error parity eg equal opportunity parity false positive false negative rates ie equal odds moreover show leveraging additional degree freedom active approaches can substantially outperform randomizationbased classifiers previously considered optimal avoiding limitations intragroup unfairness "
45,"counterfactually fair prediction using multiple causal models"," paper study problem making predictions using multiple structural casual models defined different agents constraint prediction satisfies criterion counterfactual fairness relying frameworks causality fairness opinion pooling build upon extend previous work focusing qualitative aggregation causal bayesian networks causal models order complement previous qualitative results devise method based monte carlo simulations method enables decisionmaker aggregate outputs causal models provided different experts guaranteeing counterfactual fairness result demonstrate approach simple yet illustrative toy case study "
46,"can everyday ai ethical fairness machine learning algorithms"," combining big data machine learning algorithms power automatic decision tools induces much hope fear many recently enacted european legislation gdpr french laws attempt regulate use tools leaving aside wellidentified problems data confidentiality impediments competition focus risks discrimination problems transparency quality algorithmic decisions detailed perspective legal texts faced complexity opacity learning algorithms reveals need important technological disruptions detection reduction discrimination risk addressing right obtain explanation auto matic decision since trust developers users citizens litigants customers essential algorithms exploiting personal data must deployed strict ethical framework conclusion answer need list ways controls developed institutional control ethical charter external audit attached issue label "
47,"ai fairness extensible toolkit detecting understanding mitigating unwanted algorithmic bias"," fairness increasingly important concern machine learning models used support decision making highstakes applications mortgage lending hiring prison sentencing paper introduces new open source python toolkit algorithmic fairness ai fairness aif released apache v license httpsgithubcomibmaif main objectives toolkit help facilitate transition fairness research algorithms use industrial setting provide common framework fairness researchers share evaluate algorithms package includes comprehensive set fairness metrics datasets models explanations metrics algorithms mitigate bias datasets models also includes interactive web experience httpsaifmybluemixnet provides gentle introduction concepts capabilities lineofbusiness users well extensive documentation usage guidance industryspecific tutorials enable data scientists practitioners incorporate appropriate tool problem work products architecture package engineered conform standard paradigm used data science thereby improving usability practitioners architectural design abstractions enable researchers developers extend toolkit new algorithms improvements use performance benchmarking builtin testing infrastructure maintains code quality "
48," general framework fair regression"," fairness many forms definitions become important issue facing machine learning community work consider incorporate group fairness constraints kernel regression methods applicable gaussian processes support vector machines neural network regression decision tree regression focus examining effect incorporating constraints decision tree regression direct applications random forests boosted trees amongst widespread popular inference techniques show order complexity memory computation preserved models tightly bound expected perturbations model terms number leaves trees importantly approach works trained models hence can easily applied models current use group labels required training data "
49,"fairness critically reframing fairness nash welfare product"," recent studies disparate impact machine learning applications sparked debate around concept fairness along attempts formalize different criteria many approaches focus reducing prediction errors maximizing sole utility institution work seeks reconceptualize critically frame existing discourse fairness underlining implicit biases embedded common understandings fairness literature contrast corresponding economic legal definitions paper expands concept utility fairness bringing concepts established literature welfare economics game theory translate concepts algorithmic prediction domain defining formalization nash welfare product seeks expand utility collapsing institution using prediction tool individual subject prediction one function apply modulating function makes fairness welfare tradeoffs explicit based designated policy goals apply temporal model take account effects decisions beyond scope oneshot predictions apply binary classification problem present results multiepoch simulation based uci adult income dataset test case analysis propublica recidivism dataset show expanding concept utility results fairer distribution correcting embedded biases dataset without sacrificing classifier accuracy "
50,"crowdsourcing fairness diversity budget constraints"," recent studies shown labels collected crowdworkers can discriminatory respect sensitive attributes gender race raises questions suitability using crowdsourced data use training machine learning algorithms work address problem fair diverse data collection crowd budget constraints propose novel algorithm maximizes expected accuracy collected data ensuring errors satisfy desired notions fairness provide guarantees performance algorithm show algorithm performs well practice experiments real dataset "
51,"fairmod making predictive models discrimination aware"," predictive models decision trees neural networks may produce discrimination predictions paper proposes method postprocess predictions predictive model make processed predictions nondiscriminatory method considers multiple protected variables together multiple protected variables make problem challenging simple protected variable method uses wellcited discrimination metric adapts allow specification explanatory variables position profession education describe contexts applications models postprocessing predictions problem nonlinear optimization problem find best adjustments predictions discrimination constraints protected variables met time proposed method independent classification methods can handle cases existing methods handle satisfying multiple protected attributes time allowing multiple explanatory attributes independent classification model types evaluation using four real world data sets shows proposed method effectively existing methods addition extra power "
52," fairness definitions fare examining public attitudes towards algorithmic definitions fairness"," best way define algorithmic fairness many definitions fairness proposed computer science literature clear agreement particular definition work investigate ordinary peoples perceptions three fairness definitions across two online experiments test definitions people perceive fairest context loan decisions whether fairness perceptions change addition sensitive information ie race loan applicants overall one definition calibrated fairness tends preferred others results also provide support principle affirmative action "
53,"aequitas bias fairness audit toolkit"," recent work raised concerns risk unintended bias ai systems used nowadays can affect individuals unfairly based race gender religion among possible characteristics lot bias metrics fairness definitions proposed recent years consensus metricdefinition used available resources operationalize therefore despite recent awareness auditing bias fairness developing deploying ai systems yet standard practice present aequitas open source bias fairness audit toolkit intuitive easy use addition machine learning workflow enabling users seamlessly test models several bias fairness metrics relation multiple population subgroups aequitas facilitates informed equitable decisions around developing deploying algorithmic decision making systems data scientists machine learning researchers policymakers "
54,"bayesian modeling intersectional fairness variance bias"," intersectionality framework analyzes interlocking systems power oppression affect individuals along overlapping dimensions including race gender sexual orientation class disability intersectionality theory therefore implies important fairness artificial intelligence systems protected regard multidimensional protected attributes however measurement fairness becomes statistically challenging multidimensional setting due data sparsity increases rapidly number dimensions values per dimension present bayesian probabilistic modeling approach reliable dataefficient estimation fairness multidimensional protected attributes apply two existing intersectional fairness metrics experimental results census data compas criminal justice recidivism dataset demonstrate utility methodology show bayesian methods valuable modeling measurement fairness intersectional context "
55,"intersectionality multiple group fairness expectation constraints"," group fairness important concern machine learning researchers developers regulators however strictness models must constrained considered fair still debate focus work constraining expected outcome subpopulations kernel regression particular decision tree regression application random forests boosted trees ensemble models individual constraints previously addressed work addresses concerns incorporating multiple constraints simultaneously proposed solution affect order computational memory complexity decision trees easily integrated models post training "
56," years test unfairness lessons machine learning"," quantitative definitions unfair fair introduced multiple disciplines well years including education hiring machine learning trace notion fairness defined within testing communities education hiring past half century exploring cultural social context different fairness definitions emerged cases earlier definitions fairness similar identical definitions fairness current machine learning research foreshadow current formal work cases insights fairness means measure largely gone overlooked compare past current notions fairness along several dimensions including fairness criteria focus criteria eg test model use relationship fairness individuals groups subgroups mathematical method measuring fairness eg classification regression work points way towards future research measurement unfairness builds modern understanding fairness incorporating insights past "
57,"ai fairness people disabilities point view"," consider fair treatment society people disabilities might impacted rise use artificial intelligence especially machine learning methods argue fairness people disabilities different fairness protected attributes age gender race one major difference extreme diversity ways disabilities manifest people adapt secondly disability information highly sensitive always shared precisely potential discrimination given differences explore definitions fairness well work disability space finally suggest ways approaching fairness people disabilities ai applications "
58,"probabilistic verification fairness properties via concentration"," machine learning systems increasingly used make real world legal financial decisions paramount importance develop algorithms verify systems discriminate minorities design scalable algorithm verifying fairness specifications algorithm obtains strong correctness guarantees based adaptive concentration inequalities inequalities enable algorithm adaptively take samples enough data make decision implement algorithm tool called verifair show scales large machine learning models including deep recurrent neural network five orders magnitude larger largest previouslyverified neural network technique gives probabilistic guarantees due use random samples show can choose probability error extremely small "
59,"learning controllable fair representations"," learning data representations transferable fair respect certain protected attributes crucial reducing unfair decisions preserving utility data propose informationtheoretically motivated objective learning maximally expressive representations subject fairness constraints demonstrate range existing approaches optimize approximations lagrangian dual objective contrast existing approaches objective allows user control fairness representations specifying limits unfairness exploiting duality introduce method optimizes model parameters well expressivenessfairness tradeoff empirical evidence suggests proposed method can balance tradeoff multiple notions fairness achieves higher expressiveness lower computational cost "
60,"putting fairness principles practice challenges metrics improvements"," researchers become aware passionate algorithmic fairness explosion papers laying new metrics suggesting algorithms address issues calling attention issues existing applications machine learning research greatly expanded understanding concerns challenges deploying machine learning much less work seeing rubber meets road paper provide casestudy application fairness machine learning research production classification system offer new insights measure address algorithmic fairness issues discuss open questions implementing equality opportunity describe fairness metric conditional equality takes account distributional differences provide new approach improve fairness metric model training demonstrate efficacy improving performance realworld product "
61,"noisetolerant fair classification"," fairnessaware learning involves designing algorithms discriminate respect sensitive feature eg race gender existing work problem operates assumption sensitive feature available ones training sample perfectly reliable assumption may violated many realworld cases example respondents survey may choose conceal obfuscate group identity fear potential discrimination poses question whether one can still learn fair classifiers given noisy sensitive features paper answer question affirmative show one measures fairness using meandifference score sensitive features subject noise mutually contaminated learning model owing simple identity need change desired fairnesstolerance requisite tolerance can estimated leveraging existing noiserate estimators label noise literature finally show procedure empirically effective two casestudies involving sensitive feature censoring "
62,"stable fair classification"," fair classification topic intense study machine learning several algorithms proposed towards important task however recent study friedler et al observed fair classification algorithms may stable respect variations training dataset crucial consideration several realworld applications motivated work study problem designing classification algorithms fair stable propose extended framework based fair classification algorithms formulated optimization problems introducing stabilityfocused regularization term theoretically prove stability guarantee lacking fair classification algorithms also provide accuracy guarantee extended framework accuracy guarantee can used inform selection regularization parameter framework best knowledge first work combines stability fairness automated decisionmaking tasks assess benefits approach empirically extending several fair classification algorithms shown achieve best balance fairness accuracy adult dataset empirical results show framework indeed improves stability slight sacrifice accuracy "
63,"capuchin causal database repair algorithmic fairness"," fairness increasingly recognized critical component machine learning systems however underlying data systems trained often reflect discrimination suggesting database repair problem existing treatments fairness rely statistical correlations can fooled statistical anomalies simpsons paradox proposals causalitybased definitions fairness can correctly model situations require specification underlying causal models paper formalize situation database repair problem proving sufficient conditions fair classifiers terms admissible variables opposed complete causal model show conditions correctly capture subtle fairness violations use conditions basis database repair algorithms provide provable fairness guarantees classifiers trained training labels evaluate algorithms real data demonstrating improvement state art multiple fairness metrics proposed literature retaining high utility "
64,"fairness recommendation ranking pairwise comparisons"," recommender systems one pervasive applications machine learning industry many services using match users products information important ask possible fairness risks can quantify address paper offer set novel metrics evaluating algorithmic fairness concerns recommender systems particular show measuring fairness based pairwise comparisons randomized experiments provides tractable means reason fairness rankings recommender systems building metric offer new regularizer encourage improving metric model training thus improve fairness resulting rankings apply pairwise regularization largescale production recommender system show able significantly improve systems pairwise fairness "
65," longterm impact algorithmic decision policies effort unfairness feature segregation social learning"," existing notions algorithmic fairness oneshot ensure form allocative equality time decision making account adverse impact algorithmic decisions today longterm welfare prosperity certain segments population take broader perspective algorithmic fairness propose effortbased measure fairness present datadriven framework characterizing longterm impact algorithmic policies reshaping underlying population motivated psychological literature emphsocial learning economic literature equality opportunity propose microscale model individuals may respond decisionmaking algorithms employ existing measures segregation sociology economics quantify resulting macroscale populationlevel change importantly observe different models may shift groupconditional distribution qualifications different directions findings raise number important questions regarding formalization fairness decisionmaking models "
66,"leveling playing field fairness ai versus human game benchmarks"," beginning history ai interest games platform research field developed humanlevel competence complex games became target researchers worked reach relatively recently target finally met traditional tabletop games backgammon chess go current research focus shifted electronic games provide unique challenges often case ai research results liable exaggerated misrepresented either authors third parties extent games benchmark consist fair competition human ai also matter debate work review statements made authors third parties general media academic circle game benchmark results discuss factors can impact perception fairness contest humans machines "
67,"fair classification social welfare"," now machine learning algorithms lie center many resource allocation pipelines computer scientists unwittingly cast partial social planners given state affairs important questions follow relationship fairness defined computer scientists notions social welfare paper present welfarebased analysis classification fairness regimes translate loss minimization program social welfare maximization problem set implied welfare weights individuals groupsweights can analyzed distribution justice lens converse direction ask space possible labelings given dataset hypothesis class provide algorithm answers question respect linear hyperplanes mathbbrd runs ondd main findings relationship fairness criteria welfare center sensitivity analyses fairnessconstrained empirical risk minimization programs characterize ranges delta epsilon perturbations fairness parameter epsilon yield better worse neutral outcomes utility individuals extension groups show applying strict fairness criteria codified parity constraints can worsen welfare outcomes groups generally always preferring fair classifiers abide pareto principlea fundamental axiom social choice theory welfare economics recent work machine learning rallied around notions fairness critical ensuring algorithmic systems disparate negative impact disadvantaged social groups showing constraints often fail translate improved outcomes groups cast doubt effectiveness means ensure justice "
68,"fairness machine learning tractable models"," machine learning techniques become pervasive across range different applications now widely used areas disparate recidivism prediction consumer creditrisk analysis insurance pricing prevalence machine learning techniques raised concerns potential learned algorithms become biased certain groups many definitions proposed literature fundamental task reasoning probabilistic events challenging one owing intractability inference focus paper taking steps towards application tractable models fairness tractable probabilistic models emerged guarantee conditional marginal can computed time linear size model particular show sum product networks spns enable effective technique determining statistical relationships protected attributes training variables subset training variables found spn independent training attribute can considered safe variables can train classification model without concern resulting classifier will result disparate outcomes different demographic groups initial experiments german credit data set indicate processing technique significantly reduces disparate treatment male female credit applicants small reduction classification accuracy compared state art will also motivate concept fairness percentile equivalence new definition predicated notion individuals percentile respective distributions treated equivalently prevents unfair penalisation individuals lie extremities respective distributions "
69,"compositional fairness constraints graph embeddings"," learning highquality node embeddings key building block machine learning models operate graph data social networks recommender systems however existing graph embedding techniques unable cope fairness constraints eg ensuring learned representations correlate certain attributes age gender introduce adversarial framework enforce fairness constraints graph embeddings approach compositionalmeaning can flexibly accommodate different combinations fairness constraints inference instance context social recommendations framework allow one user request recommendations invariant age gender also allowing another user request invariance just age experiments standard knowledge graph recommender system benchmarks highlight utility proposed framework "
70,"fairness missing values"," causes underlying unfair decision making complex internalised different ways decision makers actors dealing data models ultimately individuals affected decisions one frequent manifestation latent causes arises form missing values protected groups reluctant give information used delicate information groups can erased human operators data acquisition may simply less complete systematic minority groups result missing values bias data two phenomena tightly coupled however recent techniques libraries experimental results dealing fairness machine learning simply ignored missing data paper claim fairness research miss opportunity deal properly missing data support claim analyse sources missing data bias map common causes find rows containing missing values usually fairer rest treated uncomfortable ugly data different techniques libraries get rid first occasion study tradeoff performance fairness rows missing values used either technique deals directly imputation methods end paper series recommended procedures missing data aiming fair decision making "
71,"achieving fairness determining medicaid eligibility fairgroup construction"," effective complements human judgment artificial intelligence techniques started aid human decisions complicated social problems across world context united states instance automated mldl classification models offer complements human decisions determining medicaid eligibility however given limitations mldl model design algorithms may fail leverage various factors decision making resulting improper decisions allocate resources individuals may need view issue propose paper method textitfairgroup construction based legal doctrine textitdisparate impact improve fairness regressive classifiers experiments american community survey dataset demonstrate method easily adapted variety regressive classification models boost fairness deciding medicaid eligibility maintaining high levels classification accuracy "
72,"towards fair privacypreserving federated deep models"," current standalone deep learning framework tends result overfitting low utility problem can addressed either centralized framework deploys central server train global model joint data parties distributed framework leverages parameter server aggregate local model updates serverbased solutions prone problem singlepointoffailure respect collaborative learning frameworks federated learning fl robust existing federated learning frameworks overlook important aspect participation fairness parties given final model without regard contributions address issues propose decentralized fair privacypreserving deep learning fppdl framework incorporate fairness federated deep learning models particular design local credibility mutual evaluation mechanism guarantee fairness threelayer onionstyle encryption scheme guarantee accuracy privacy different existing fl paradigm fppdl participant receives different version fl model performance commensurate contributions experiments benchmark datasets demonstrate fppdl balances fairness privacy accuracy enables federated learning ecosystems detect isolate lowcontribution parties thereby promoting responsible participation "
73,"flexibly fair representation learning disentanglement"," consider problem learning representations achieve group subgroup fairness respect multiple sensitive attributes taking inspiration disentangled representation learning literature propose algorithm learning compact representations datasets useful reconstruction prediction also emphflexibly fair meaning can easily modified test time achieve subgroup demographic parity respect multiple sensitive attributes conjunctions show empirically resulting encoderwhich require sensitive attributes inferenceenables adaptation single representation variety fair classification tasks new target labels subgroup definitions "
74,"fair division without disparate impact"," consider problem dividing items individuals way fair sense distributional fairness sense disparate impact across protected classes important existing mechanism distributionally fair division competitive equilibrium equal incomes ceei unfortunately ceei will general respect disparate impact constraints consider two types disparate impact measures requiring allocations similar across protected classes requiring average utility levels similar across protected classes modify standard ceei algorithm two ways equitable equilibrium equal incomes removes disparate impact allocations competitive equilibrium equitable incomes removes disparate impact attained utility levels show analytically removing disparate impact outcomes breaks several ceeis desirable properties envy regret pareto optimality incentive compatibility contrast can remove disparate impact attained utility levels without affecting properties finally experimentally evaluate tradeoffs efficiency equity disparate impact recommendersystem based market "
75,"learning fair naive bayes classifiers discovering eliminating discrimination patterns"," machine learning increasingly used make realworld decisions recent research efforts aim define ensure fairness algorithmic decision making existing methods often assume fixed set observable features define individuals lack discussion certain features observed test time paper study fairness naive bayes classifiers allow partial observations particular introduce notion discrimination pattern refers individual receiving different classifications depending whether sensitive attributes observed model considered fair pattern propose algorithm discover mine discrimination patterns naive bayes classifier show learn maximum likelihood parameters subject fairness constraints approach iteratively discovers eliminates discrimination patterns fair model learned empirical evaluation three realworld datasets demonstrates can remove exponentially many discrimination patterns adding small fraction constraints "
76,"farm fair reward mechanism information aggregation spontaneous localized settings extended version"," although peer prediction markets widely used crowdsourcing aggregate information agents often fail reward participating agents equitably honest agents can wrongly penalized randomly paired dishonest ones work introduce emphselective emphcumulative fairness characterize mechanism fair satisfies notions present farm representative mechanism designed farm nash incentive mechanism focuses information aggregation spontaneous local activities accessible limited number agents without assuming prior knowledge event agents vicinity observe information farm uses textiti emphreport strength score remove risk random pairing dishonest reporters textitii emphconsistency score measure agents history accurate reports distinguish valuable reports textitiii emphreliability score estimate probability agent collude nearby agents prevents agents getting swayed textitiv emphlocation robustness score filter agents try participate without present considered setting together report strength consistency reliability represent fair reward given agents based reports "
77,"inherent tradeoffs learning fair representations"," prevalence machine learning highstakes applications especially ones regulated antidiscrimination laws societal norms crucial ensure predictive models propagate existing bias discrimination due ability deep neural nets learn rich representations recent advances algorithmic fairness focused learning fair representations adversarial techniques reduce bias data preserving utility simultaneously paper lens information theory provide first result quantitatively characterizes tradeoff demographic parity joint utility across different population groups specifically base rates differ groups show method aiming learn fair representations admits informationtheoretic lower bound joint error across groups complement negative results also prove optimal decision functions across different groups close learning fair representations leads alternative notion fairness known accuracy parity states error rates close groups finally theoretical findings also confirmed empirically realworld datasets "
78,"fairness criteria lens directed acyclic graphical models"," substantial portion literature fairness algorithms proposes analyzes operationalizes simple formulaic criteria assessing fairness two criteria equalized odds calibration group gained significant attention simplicity intuitive appeal also incompatibility chapter provides perspective meaning consequences fairness criteria using graphical models reveals equalized odds related criteria ultimately misleading assessment various graphical models suggests fairness criteria ultimately casespecific sensitive nature information algorithm processes "
79,"reinforcement learning fairness constraints resource distribution humanrobot teams"," much work robotics operations research focused optimal resource distribution agent dynamically decides sequentially distribute resources among different candidates however work ignores notion fairness candidate selection case robot distributes resources human team members disproportionately favoring highest performing teammate can negative effects team dynamics system acceptance introduce multiarmed bandit algorithm fairness constraints robot distributes resources human teammates different skill levels problem robot know skill level human teammate learns observing performance time define fairness constraint minimum rate human teammate selected throughout task provide theoretical guarantees performance perform largescale user study adjust level fairness algorithm results show fairness resource distribution significant effect users trust system "
80,"fairnas rethinking evaluation fairness weight sharing neural architecture search"," one critical problems twostage weightsharing neural architecture search evaluation candidate models faithful ranking certainly leads accurate searching results however current methods prone making misjudgments paper prove inevitably give biased evaluations due inherent unfairness supernet training view propose two levels constraints expectation fairness strict fairness particularly strict fairness ensures equal optimization opportunities choice blocks throughout training neither overestimates underestimates capacity demonstrate crucial improving confidence models ranking incorporating supernet trained fairness constraints multiobjective evolutionary search algorithm obtain various stateoftheart models imagenet especially fairnasa attains top accuracy models evaluation codes made publicly available online httpgithubcomfairnasfairnas "
81,"toward fairness ai people disabilities research roadmap"," ai technologies potential dramatically impact lives people disabilities pwd indeed improving lives pwd motivator many stateoftheart ai systems automated speech recognition tools can caption videos people deaf hard hearing language prediction algorithms can augment communication people speech cognitive disabilities however widely deployed ai systems may work properly pwd worse may actively discriminate considerations regarding fairness ai pwd thus far received little attention position paper identify potential areas concern regarding several ai technology categories may impact particular disability constituencies care taken design development testing intend risk assessment various classes ai might interact various classes disability provide roadmap future research needed gather data test hypotheses build inclusive algorithms "
82,"fairst equitable spatial temporal demand prediction new mobility systems"," emerging transportation modes including carsharing bikesharing ridehailing transforming urban mobility shown reinforce socioeconomic inequities spatiotemporal demand prediction models new mobility regimes must therefore consider fairness firstclass design requirement present fairst fairnessaware model predicting demand new mobility systems approach utilizes d d d convolutions integrate various urban features learn spatialtemporal dynamics mobility system include fairness metrics form regularization make predictions equitable across demographic groups propose two novel spatiotemporal fairness metrics regionbased fairness gap rfg individualbased fairness gap ifg quantify equity spatiotemporal context vary whether demographics labeled region level rfg whether population distribution information available ifg experimental results real bike share ride share datasets demonstrate effectiveness proposed model fairst reduces fairness gap can surprisingly achieve better accuracy stateoftheart yet fairnessoblivious methods including lstms convlstms d cnn "
83,"fairnessenhancing interventions stream classification"," wide spread usage automated datadriven decision support systems raised lot concerns regarding accountability fairness employed models absence human supervision existing fairnessaware approaches tackle fairness batch learning problem aim learning fair model can applied future instances problem many applications however data comes sequentially characteristics might evolve time setting counterintuitive fix fair model data stream changes data might incur changes underlying model therefore affecting fairness work propose fairnessenhancing interventions modify input data outcome stream classifier applied data will fair experiments real synthetic data show approach achieves good predictive performance low discrimination scores course stream "
84,"faht adaptive fairnessaware decision tree classifier"," automated datadriven decisionmaking systems ubiquitous across wide spread online well offline services systems depend sophisticated learning algorithms available data optimize service function decision support assistance however growing concern accountability fairness employed models fact often available historic data intrinsically discriminatory ie proportion members sharing one sensitive attributes higher proportion population whole receiving positive classification leads lack fairness decision support system number fairnessaware learning methods proposed handle concern however methods tackle fairness static problem take evolution underlying stream population consideration paper introduce learning mechanism design fair classifier online stream based decisionmaking learning model faht fairnessaware hoeffding tree extension wellknown hoeffding tree algorithm decision tree induction streams also accounts fairness experiments show algorithm able deal discrimination streaming environments maintaining moderate predictive performance stream "
85,"fairness reinforcement learning"," decision support systems eg ecological conservation autonomous systems eg adaptive controllers smart cities start deployed real applications although operations often impact many users stakeholders fairness consideration generally taken account design lead completely unfair outcomes users stakeholders tackle issue advocate use social welfare functions encode fairness present general novel problem context deep reinforcement learning although possibly extended machine learning tasks "
86," point fairness disability ai complexity justice"," work integrating conversations around ai disability vital valued particularly done lens fairness yet time analyzing ethical implications ai disabled people solely lens singular idea fairness risks reinforcing existing power dynamics either reinforcing position existing medical gatekeepers promoting tools techniques benefit otherwiseprivileged disabled people harming rendered outliers multiple ways paper present two case studies within computer vision subdiscipline ai focused training algorithms can see technologies putatively intended help disabled people failures consider structural injustices design likely result harms addressed fairness framing ethics drawing disability studies critical data science call researchers ai ethics disability move beyond simplistic notions fairness towards notions justice "
87,"fairness deep learning computational perspective"," deep learning increasingly used highstake decision making applications affect individual lives however deep learning models might exhibit algorithmic discrimination behaviors respect protected groups potentially posing negative impacts individuals society therefore fairness deep learning attracted tremendous attention recently provide review covering recent progresses tackle algorithmic fairness problems deep learning computational perspective specifically show interpretability can serve useful ingredient diagnose reasons lead algorithmic discrimination also discuss fairness mitigation approaches categorized according three stages deep learning lifecycle aiming push forward area fairness deep learning build genuinely fair reliable deep learning systems "
88,"fairnessaware process mining"," process mining multipurpose tool enabling organizations improve processes one primary purposes process mining finding root causes performance compliance problems processes usual way gathering data process event log sources applying data mining machine learning techniques however results applying techniques always acceptable many situations approach prone making obvious unfair diagnoses applying may result conclusions unsurprising even discriminating eg blaming overloaded employees delays paper present solution problem creating fair classifier situations undesired effects removed expense reduction accuracy resulting classifier implemented method plugin prom using implemented plugin two real event logs decreased discrimination caused classifier losing small fraction accuracy "
89,"quantifying inframarginality tradeoff group fairness"," critical decisionmaking scenarios optimizing accuracy can lead biased classifier hence past work recommends enforcing groupbased fairness metrics addition maximizing accuracy however exposes classifier another kind bias called inframarginality refers individuallevel bias individualssubgroups can worse simply optimizing accuracy instance classifier implementing racebased parity may significantly disadvantage women advantaged race quantify bias propose general notion etainframarginality can used evaluate extent bias prove theoretically unlike fairness metrics inframarginality tradeoff accuracy high accuracy directly leads low inframarginality observation confirmed empirical analysis multiple simulated realworld datasets find maximizing group fairness often increases inframarginality suggesting consideration grouplevel fairness individuallevel inframarginality however measuring inframarginality requires knowledge true distribution individuallevel outcomes correctly explicitly propose practical method measure inframarginality simple algorithm maximize groupwise accuracy avoid inframarginality "
90,"avoiding resentment via monotonic fairness"," classifiers achieve demographic balance explicitly using protected attributes race gender often politically culturally controversial due lack individual fairness ie individuals similar qualifications will receive different outcomes individually group fair decision criteria can produce counterintuitive results eg optimal constrained boundary may reject intuitively better candidates due demographic imbalance similar candidates approaches can seen introducing individual resentment individuals received better outcome either belonged different demographic class qualifications remained class objectively worse qualifications eg lower test scores show forms resentment can avoided using monotonically constrained machine learning models create individually fair demographically balanced classifiers "
91,"fat forensics python toolbox algorithmic fairness accountability transparency"," machine learning algorithms can take important decisions sometimes legally binding everyday life cases however systems decisions neither regulated certified given potential harm algorithms can cause qualities fairness accountability transparency predictive systems paramount importance recent literature suggested voluntary selfreporting aspects predictive systems eg data sheets data sets scope often limited single component machine learning pipeline producing requires manual labour resolve impasse ensure highquality fair transparent reliable machine learning systems developed open source toolbox can inspect selected fairness accountability transparency aspects systems automatically objectively report back engineers users describe design scope usage examples python toolbox paper toolbox provides functionality inspecting fairness accountability transparency aspects machine learning process data features models predictions available public bsd clause open source licence "
92,"causal modeling fairness dynamical systems"," many application areaslending education online recommenders examplefairness equity concerns emerge machine learning system interacts dynamically changing environment produce immediate longterm effects individuals demographic groups discuss causal directed acyclic graphs dags unifying framework recent literature fairness dynamical systems show formulation affords several new directions inquiry modeler causal assumptions can expressed manipulated emphasize importance computing interventional quantities dynamical fairness setting show causal assumptions enable simulation environment dynamics known offpolicy estimation dynamics unknown intervention short longterm outcomes group individual levels "
93,"groupbased fair learning leads counterintuitive predictions"," number machine learning ml methods proposed recently maximize model predictive accuracy enforcing notions group parity fairness across subpopulations propose desirable property procedures slackconsistency individual predictions model monotonic respect allowed slack ie maximum allowed groupparity violation monotonicity can useful individuals understand impact enforcing fairness predictions surprisingly find standard ml methods enforcing fairness violate basic property moreover undesirable behavior arises situations agnostic complexity underlying model approximate optimizations suggesting simple act incorporating constraint can lead drastically unintended behavior ml present simple theoretical method enforcing slackconsistency encouraging discussions unintended behaviors potentially induced enforcing groupbased parity "
94," impact data preparation fairness software systems"," machine learning models widely adopted scenarios directly affect people development software systems based models raises societal legal concerns decisions may lead unfair treatment individuals based attributes like race gender data preparation key machine learning pipeline effect fairness yet studied detail paper evaluate fairness effectiveness learned models affected removal sensitive attribute encoding categorical attributes instance selection methods including crossvalidators random undersampling used adult income german credit data datasets widely studied known fairness concerns applied data preparation technique individually analyse difference predictive performance fairness using statistical parity difference disparate impact normalised prejudice index results show fairness affected transformations made training data particularly imbalanced datasets removing sensitive attribute insufficient eliminate unfairness predictions expected key achieve fairer models additionally standard random undersampling respect true labels sometimes prejudicial performing random undersampling "
95,"fairness clustering multiple sensitive attributes"," clustering may considered fair prespecified sensitive attributes proportions sensitive attribute groups cluster reflect dataset paper consider task fair clustering scenarios involving multiple multivalued numeric sensitive attributes propose fair clustering method textitfairkm fair kmeans inspired popular kmeans clustering formulation outline computational notion fairness used along cluster coherence objective yield fairkm clustering method empirically evaluate approach wherein quantify quality fairness clusters realworld datasets experimental evaluation illustrates clusters generated fairkm fare significantly better clustering quality fair representation sensitive attribute groups compared clusters stateoftheart baseline fair clustering method "
96,"conditional learning fair representations"," propose novel algorithm learning fair representations can simultaneously mitigate two notions disparity among different demographic subgroups classification setting two key components underpinning design algorithm balanced error rate conditional alignment representations show two components contribute ensuring accuracy parity equalized falsepositive falsenegative rates across groups without impacting demographic parity furthermore also demonstrate theory two realworld experiments proposed algorithm leads better utilityfairness tradeoff balanced datasets compared existing algorithms learning fair representations classification "
97," empirical study learning fairness metrics compas data human supervision"," notion individual fairness requires similar people receive similar treatment however hard achieve practice since difficult specify appropriate similarity metric work attempt learn similarity metric human annotated data gather new dataset human judgments criminal recidivism prediction compas task assuming human supervision obeys principle individual fairness leverage prior work metric learning evaluate performance several metric learning methods dataset show learned metrics outperform euclidean precision metric various criteria provide way directly learn similarity metric satisfying individual fairness provide empirical study derive similarity metric human supervisors future work can use tool understand human supervision "
98," gender matter towards fairness dialogue systems"," recently increasing concerns fairness artificial intelligence ai realworld applications computer vision recommendations example recognition algorithms computer vision unfair black people poorly detecting faces inappropriately identifying gorillas one crucial application ai dialogue systems extensively applied society usually built real human conversational data thus inherit fairness issues held real world however fairness dialogue systems investigated paper perform initial study fairness issues dialogue systems particular construct first dataset propose quantitative measures understand fairness dialogue models studies demonstrate popular dialogue models show significant prejudice towards different genders races besides mitigate bias exhibited dialogue systems propose two effective debiasing methods experiments show methods can reduce biases dialogue systems significantly will release dataset measurement code later foster fairness research dialogue systems "
99,"preventing adversarial use datasets fair coreset construction"," propose improving privacy properties dataset publishing strategically chosen coreset data containing subset instances coreset allows strong performance primary tasks forces poor performance unwanted tasks give methods linear models neural networks demonstrate efficacy data "
100,"pcfairness unified framework measuring causalitybased fairness"," recent trend fair machine learning define fairness causalitybased notions concern causal connection protected attributes decisions however one common challenge causalitybased fairness notions identifiability ie whether can uniquely measured observational data critical barrier applying notions realworld situations paper develop framework measuring different causalitybased fairness propose unified definition covers previous causalitybased fairness notions namely pathspecific counterfactual fairness pc fairness based propose general method form constrained optimization problem bounding pathspecific counterfactual fairness unidentifiable situations experiments synthetic realworld datasets show correctness effectiveness method "
101,"learning fairness multiagent systems"," fairness essential human society contributing stability productivity similarly fairness also key many multiagent systems taking fairness multiagent learning help multiagent systems become efficient stable however learning efficiency fairness simultaneously complex multiobjective jointpolicy optimization tackle difficulties propose fen novel hierarchical reinforcement learning model first decompose fairness agent propose fairefficient reward agent learns policy optimize avoid multiobjective conflict design hierarchy consisting controller several subpolicies controller maximizes fairefficient reward switching among subpolicies provides diverse behaviors interact environment fen can trained fully decentralized way making easy deployed realworld applications empirically show fen easily learns fairness efficiency significantly outperforms baselines variety multiagent scenarios "
102,"auditing achieving intersectional fairness classification problems"," machine learning algorithms extensively used make increasingly consequential decisions people achieving optimal predictive performance can longer focus particularly important consideration fairness respect race gender sensitive attribute paper studies intersectional fairness intersections multiple sensitive attributes considered prior research mainly focused fairness respect single sensitive attribute intersectional fairness comparatively less studied despite critical importance safety modern machine learning systems present comprehensive framework auditing achieving intersectional fairness classification problems define suite metrics assess intersectional fairness data model outputs extending known singleattribute fairness metrics propose methods robustly estimating even intersectional subgroups underrepresented furthermore develop postprocessing techniques mitigate detected intersectional bias classification model techniques rely assumptions regarding underlying model preserve predictive performance guaranteed level fairness finally give guidance practical implementation showing proposed methods perform realworld dataset "
103," humanintheloop framework construct contextdependent mathematical formulations fairness"," despite recent surge interest designing guaranteeing mathematical formulations fairness virtually existing notions algorithmic fairness fail adaptable intricacies nuances decisionmaking context hand argue capturing factors inherently human task requires knowledge social background machine learning tools impact real peoples outcomes deep understanding ramifications automated decisions decision subjects society work present framework construct contextdependent mathematical formulation fairness utilizing peoples judgment fairness utilize theoretical model heidari et al which shows existing formulations algorithmic fairness special cases economic models equality opportunity eopand present practical humanintheloop approach pinpoint fairness notion eop family best captures peoples perception fairness given context illustrate framework run humansubject experiments designed learn parameters heidari et als eop model including circumstance desert utility hypothetical recidivism decisionmaking scenario work takes initial step toward democratizing formulation fairness utilizing humanjudgment tackle fundamental shortcoming automated decisionmaking systems machine incapable understanding processing human aspects social context decisions "
104,"fairnessaware neural reyni minimization continuous features"," past years seen dramatic rise academic societal interest fair machine learning plenty fair algorithms proposed recently tackle challenge discrete variables ideas exist continuous ones objective paper ensure independence level outputs regression models given continuous sensitive variables purpose use hirschfeldgebeleinrenyi hgr maximal correlation coefficient fairness metric propose two approaches minimize hgr coefficient first reducing upper bound hgr neural network estimation chi divergence second minimizing hgr directly adversarial neural network architecture idea predict output y minimizing ability adversarial neural network find estimated transformations required predict hgr coefficient empirically assess compare approaches demonstrate significant improvements previously presented work field "
105,"fair adversarial gradient tree boosting"," fair classification become important topic machine learning research bias mitigation strategies focus neural networks noticed lack work fair classifiers based decision trees even though proven efficient uptodate comparison stateoftheart classification algorithms tabular data tree boosting outperforms deep learning reason developed novel approach adversarial gradient tree boosting objective algorithm predict output y gradient tree boosting minimizing ability adversarial neural network predict sensitive attribute s approach incorporates iteration gradient neural network directly gradient tree boosting empirically assess approach popular data sets compare stateoftheart algorithms results show algorithm achieves higher accuracy obtaining level fairness measured using set different common fairness definitions "
106,"fair data adaptation quantile preservation"," fairness classification regression received much attention recently various partially noncompatible criteria proposed fairness criteria can enforced given classifier alternatively data can adapated ensure every classifier trained data will adhere desired fairness criteria present practical data adaption method based quantile preservation causal structural equation models data adaptation based presumed counterfactual model data counterfactual model verified experimentally show certain population notions fairness still guaranteed even counterfactual model misspecified precise nature fulfilled noncausal fairness notion demographic parity separation sufficiency depends structure underlying causal model choice resolving variables describe implementation proposed data adaptation procedure based random forests demonstrate practical use simulated realworld data "
107,"fairness equality effort"," fair machine learning receiving increasing attention machine learning fields researchers fair learning developed correlation associationbased measures demographic disparity mistreatment disparity calibration causalbased measures total effect direct indirect discrimination counterfactual fairness fairness notions equality opportunity equal odds consider decisions training data decisions made predictive models paper develop new causalbased fairness notation called equality effort different existing fairness notions mainly focus discovering disparity decisions two groups individuals proposed equality effort notation helps answer questions like extend legitimate variable change make particular individual achieve certain outcome level addresses concerns whether efforts made achieve outcome level individuals protected group unprotected group different develop algorithms determining whether individual group individuals discriminated terms equality effort also develop optimizationbased method removing discriminatory effects data discrimination detected conduct empirical evaluations compare equality effort existing fairness notion show effectiveness proposed algorithms "
108,"online fair division survey"," survey burgeoning promising new research area considers online nature many practical fair division problems identify wide variety online fair division problems well discuss new mechanisms normative properties apply online setting online nature fair division problems provides opportunities challenges possibility develop new online mechanisms well difficulty dealing uncertain future "
109,"towards fair protocols workflows openpredict case study"," essential advancement science scientists researchers share reuse reproduce workflows protocols used others fair principles set guidelines aim maximize value usefulness research data emphasize number important points regarding means digital objects found reused others question apply principles just static input output data also dynamic workflows protocols consume produce still debate poses number challenges paper describe inclusive overarching approach apply fair principles workflows protocols demonstrate benefits apply evaluate approach case study consists making predict workflow highly cited drug repurposing workflow open fair includes fairification involved datasets well applying semantic technologies represent store data detailed versions general protocol concrete workflow instructions execution traces semantic model proposed better address specific requirements evaluated answering competency questions semantic model consists classes relations number existing ontologies including workflowever prov edam bpmn allowed us formulate answer new kinds competency questions evaluation shows high degree fairified openpredict workflow now adheres fair principles practicality usefulness able answer new competency questions "
110,"greedy algorithms fair division mixed manna"," consider multiagent model fair division mixed manna ie items agents can positive zero negative utilities agents additive utilities bundles items model give several general impossibility results special possibility results three common fairness concepts ie ef efx efx one popular efficiency concept ie po also study interact common welfare objectives nash disutility nash egalitarian welfares example show maximizing nash welfare mixed manna minimizing disutility nash welfare ensure ef allocation whereas goods nash welfare also prove efx allocation may exist even identical utilities comparison tertiary utilities efx po allocations efx po allocations always exist also identical utilities efx po allocations always exist cases give polynomialtime algorithms returning allocations approximating nash disutility nash egalitarian welfares special cases "
111,"fair eyes others"," envyfreeness widely studied notion resource allocation capturing aspects fairness notion envy inherently subjective though might case agent envies another agent objectively reason difficulty define notion objectivity since groundtruth can properly serve basis definition natural approach consider judgement agents proxy objectivity building previous work parijs introduced unanimous envy propose notion approval envy agent ai experiences approval envy towards aj envious aj sufficiently many agents agree case perspectives interesting properties notion put forward computing minimal threshold guaranteeing approval envy clearly inherits wellknown intractable results envyfreeness identify tractable cases house allocation ii provide general method based mixed integer programming encoding problem proves efficient practice allows us particular show experimentally existence allocations rather small threshold often observed "
112,"fair darts eliminating unfair advantages differentiable architecture search"," differentiable architecture search darts now widely disseminated weightsharing neural architecture search method however suffers wellknown performance collapse due inevitable aggregation skip connections paper first disclose root cause lies unfair advantage exclusive competition experiments show either two conditions broken collapse disappears thereby present novel approach called fair darts exclusive competition relaxed collaborative specifically let operations architectural weight independent others yet still important issue discretization discrepancy propose zeroone loss push architectural weights towards zero one approximates expected multihot solution experiments performed two mainstream search spaces derive new stateoftheart results cifar imagenet code available httpsgithubcomxiaomiautomlfairdarts "
113," legal compatibility fairness definitions"," past literature effective demonstrating ideological gaps machine learning ml fairness definitions considering use complex sociotechnical systems however go demonstrate definitions often misunderstand legal concepts purport inspired consequently inappropriately coopt legal language paper demonstrate examples misalignment discuss differences ml terminology legal counterparts well legal ml fairness communities can learn tensions focus paper us antidiscrimination law since ml fairness research community regularly references terms body law "
114,"recovering biased data can fairness constraints improve accuracy"," multiple fairness constraints proposed literature motivated range concerns demographic groups might treated unfairly machine learning classifiers work consider different motivation learning biased training data posit several ways training data may biased including noisy negatively biased labeling process members disadvantaged group decreased prevalence positive negative examples disadvantaged group given biased training data empirical risk minimization erm may produce classifier biased also suboptimal accuracy true data distribution examine ability fairnessconstrained erm correct problem particular find equal opportunity fairness constraint hardt price srebro combined erm will provably recover bayes optimal classifier range bias models also consider recovery methods including reweighting training data equalized odds demographic parity theoretical results provide additional motivation considering fairness interventions even actor cares primarily accuracy "
115,"group fairness bandit arm selection"," propose novel formulation group fairness contextual multiarmed bandit cmab setting cmab setting sequential decision maker must time step choose arm pull finite set arms observing context potential arm pulls model arms partitioned two sensitive groups based protected feature eg age race socioeconomic status despite fact may differences expected payout groups may wish ensure form fairness picking arms various groups work explore two definitions fairness equal group probability wherein probability pulling arm protected groups proportional parity wherein probability choosing arm particular group proportional size group provide novel algorithm can accommodate notions fairness arbitrary number groups provide bounds regret algorithm validate algorithm using synthetic data well two realworld datasets intervention settings wherein want allocate resources fairly across protected groups "
116,"ltlf synthesis fairness stability assumptions"," synthesis assumptions constraints environment rule certain environment behaviors key observation even consider systems ltlf goals finite traces environment assumptions need expressed infinite traces since accomplishing agent goals may require unbounded number environment action solve synthesis respect finitetrace ltlf goals infinitetrace assumptions reduce problem ltl synthesis unfortunately synthesis ltlf ltl worstcase complexity exptimecomplete algorithms available ltl synthesis much difficult practice ltlf synthesis work show interesting cases can avoid detour ltl synthesis keep simplicity ltlf synthesis specifically develop bddbased fixpointbased technique handling basic forms fairness stability assumptions show empirically technique performs much better standard ltl synthesis "
117,"fair contextual multiarmed bandits theory experiments"," ai system interacts multiple users frequently needs make allocation decisions instance virtual agent decides pay attention group setting factory robot selects worker deliver part demonstrating fairness decision making essential systems broadly accepted introduce multiarmed bandit algorithm fairness constraints fairness defined minimum rate task resource assigned user proposed algorithm uses contextual information users task makes assumptions losses capturing performance different users generated provide theoretical guarantees performance empirical results simulation online user study results highlight benefit accounting contexts fair decision making especially users perform better contexts worse others "
118,"balancing tradeoff profit fairness rideshare platforms highdemand hours"," rideshare platforms assigning requests drivers tend maximize profit system andor minimize waiting time riders platforms can exacerbate biases drivers may certain types requests consider case peak hours demand rides supply drivers drivers well aware advantage peak hours can choose selective rides accept moreover scenario assignment requests drivers platform made maximize profit andor minimize wait time riders requests certain type eg nonpopular pickup location nonpopular dropoff location might never assigned driver system can highly unfair riders however increasing fairness might come cost overall profit made rideshare platform balance conflicting goals present flexible nonadaptive algorithm lpalg allows platform designer control profit fairness system via parameters alpha beta respectively model matching problem online bipartite matching set drivers offline requests arrive online upon arrival request use lpalg assign driver driver might choose accept reject reject request formalize measures profit fairness setting show using lpalg competitive ratios profit fairness measures worse alphae betae respectively extensive experimental results realworld synthetic datasets confirm validity theoretical lower bounds additionally show lpalg choice alpha beta can beat two natural heuristics greedy uniform emphboth fairness profit "
119,"stochastic fairness languagetheoretic fairness planning nondeterministic domains"," address two central notions fairness literature planning nondeterministic fully observable domains first call stochastic fairness classical assumes environment operates probabilistically using possibly unknown probabilities second languagetheoretic assumes action taken given state infinitely often possible outcomes appear infinitely often call stateaction fairness two notions coincide standard reachability goals diverge temporally extended goals important difference overlooked planning literature argue led confusion number published algorithms use reductions stated stateaction fairness incorrect correct stochastic fairness remedy provide optimal sound complete algorithm solving stateaction fair planning ltlltlf goals well correct proof lower bound goalcomplexity proof general enough provides new proofs also nofairness stochasticfairness cases overall show stochastic fairness better behaved stateaction fairness "
120,"leveraging semisupervised learning fairness using neural networks"," growing concern fairness decisionmaking systems based machine learning shortage labeled data always challenging problem facing machine learning based systems scenarios semisupervised learning shown effective way exploiting unlabeled data improve upon performance model notably unlabeled data contain label information can significant source bias training machine learning systems inspired us tackle challenge fairness formulating problem semisupervised framework paper propose semisupervised algorithm using neural networks benefiting unlabeled data just improve performance also improve fairness decisionmaking process proposed model called ssfair exploits information unlabeled data mitigate bias training data "
121,"measuring nonexpert comprehension machine learning fairness metrics"," bias machine learning manifested injustice several areas medicine hiring criminal justice response computer scientists developed myriad definitions fairness correct bias fielded algorithms definitions based established legal ethical norms others largely mathematical unclear whether general public agrees fairness definitions perhaps importantly whether understand definitions take initial steps toward bridging gap ml researchers public addressing question lay audience understand basic definition ml fairness develop metric measure comprehension three definitionsdemographic parity equal opportunity equalized odds evaluate metric using online survey investigate relationship comprehension sentiment demographics definition "
122," consequentialism fairness"," recent work fairness machine learning primarily emphasized define quantify encourage fair outcomes less attention paid however ethical foundations underlie efforts among ethical perspectives taken consideration consequentialism position roughly speaking outcomes matter although consequentialism free difficulties although necessarily provide tractable way choosing actions combined problems uncertainty subjectivity aggregation nevertheless provides powerful foundation critique existing literature machine learning fairness moreover brings fore tradeoffs involved including problem counts pros cons using policy relative value distant future paper provide consequentialist critique common definitions fairness within machine learning well machine learning perspective consequentialism conclude broader discussion issues learning randomization important implications ethics automated decision making systems "
123,"fairness learningbased sequential decision algorithms survey"," algorithmic fairness decisionmaking studied extensively static settings oneshot decisions made tasks classification however practice decisionmaking processes sequential nature decisions made past may impact future data particularly case decisions affect individuals users generating data used future decisions survey review existing literature fairness datadriven sequential decisionmaking will focus two types sequential decisions past decisions impact underlying user population thus impact future data past decisions impact underlying user population therefore future data can impact future decisions case impact various fairness interventions underlying population examined "
124,"fair transfer multiple style attributes text"," preserve anonymity obfuscate identity online platforms users may morph text portray different gender demographic similarly chatbot may need customize communication style improve engagement audience manner changing style written text gained significant attention recent years yet past research works largely cater transfer single style attributes disadvantage focusing single style alone often results target text existing style attributes behave unpredictably unfairly dominated new style counteract behavior nice style transfer mechanism can transfer control multiple styles simultaneously fairly approach one obtain obfuscated written text incorporated desired degree multiple soft styles femalequality politeness formalness work demonstrate transfer multiple styles achieved sequentially performing multiple singlestyle transfers single styletransfer step often reverses dominates style incorporated previous transfer step propose neural network architecture fairly transferring multiple style attributes given text test architecture yelp data set demonstrate superior performance compared existing onestyle transfer steps performed sequence "
125,"adequate fair explanations"," explaining sophisticated machinelearning based systems important issue foundations ai recent efforts shown various methods providing explanations approaches can broadly divided two schools provide local human interpreatable approximation machine learning algorithm logical approaches exactly characterise one aspect decision paper focus upon second school exact explanations rigorous logical foundation epistemological problem exact methods can furnish complete explanations explanations may complex humans understand even write human readable form interpretability requires epistemically accessible explanations explanations humans can grasp yet sufficiently complete epistemically accessible explanation still needs clarification terms counterfactuals following wachter et al counterfactual explanations many assumptions needed provide complete explanation left implicit counterfactual explanations exploit properties particular data point sample also local well partial explanations explore move local partial explanations call complete local explanations global ones preserve accessibility argue need partiality partiality makes possible hide explicit biases present algorithm may injurious unfairwe investigate easy uncover biases providing complete fair explanations exploiting structure set counterfactuals providing complete local explanation "
126,"algorithmic fairness nonideal perspective"," inspired recent breakthroughs predictive modeling practitioners industry government turned machine learning hopes operationalizing predictions drive automated decisions unfortunately many social desiderata concerning consequential decisions justice fairness natural formulation within purely predictive framework efforts mitigate problems researchers proposed variety metrics quantifying deviations various statistical parities might expect observe fair world offered variety algorithms attempts satisfy subsets parities trade degree satisfied utility paper connect approach emphfair machine learning literature ideal nonideal methodological approaches political philosophy ideal approach requires positing principles according just world operate straightforward application ideal theory one supports proposed policy arguing closes discrepancy real perfectly just world however failing account mechanisms nonideal world arose responsibilities various decisionmakers impacts proposed policies naive applications ideal thinking can lead misguided interventions paper demonstrate connection fair machine learning literature ideal approach political philosophy argue increasingly apparent shortcomings proposed fair machine learning algorithms reflect broader troubles faced ideal approach conclude critical discussion harms misguided solutions reinterpretation impossibility results directions future research "
127,"algorithmic fairness"," increasing number decisions regarding daily lives human beings controlled artificial intelligence ai algorithms spheres ranging healthcare transportation education college admissions recruitment provision loans many realms since now touch many aspects lives crucial develop ai algorithms accurate also objective fair recent studies shown algorithmic decisionmaking may inherently prone unfairness even intention paper presents overview main concepts identifying measuring improving algorithmic fairness using ai algorithms paper begins discussing causes algorithmic bias unfairness common definitions measures fairness fairnessenhancing mechanisms reviewed divided preprocess inprocess postprocess mechanisms comprehensive comparison mechanisms conducted towards better understanding mechanisms used different scenarios paper describes commonly used fairnessrelated datasets field finally paper ends reviewing several emerging research subfields algorithmic fairness "
128,"fae fairnessaware ensemble framework"," automated decision making based big data machine learning ml algorithms can result discriminatory decisions certain protected groups defined upon personal data like gender race sexual orientation etc algorithms designed discover patterns big data might pick encoded societal biases training data even worse might reinforce biases resulting severe discrimination majority thus far proposed fairnessaware machine learning approaches focus solely pre postprocessing steps machine learning process input data learning algorithms derived models respectively however fairness problem isolated single step ml process rather discrimination often result complex interactions big data algorithms therefore holistic approach required proposed fae fairnessaware ensemble framework combines fairnessrelated interventions pre postprocessing steps data analysis process preprocessing step tackle problems underrepresentation protected group group imbalance classimbalance generating balanced training samples postprocessing step tackle problem class overlapping shifting decision boundary direction fairness "
129,"joint optimization ai fairness utility humancentered approach"," today ai increasingly used many highstakes decisionmaking applications fairness important concern already many examples ai biased making questionable unfair decisions ai research community proposed many methods measure mitigate unwanted biases involve inputs human policy makers argue different fairness criteria sometimes simultaneously satisfied achieving fairness often requires sacrificing objectives model accuracy key acquire adhere human policy makers preferences make tradeoff among objectives paper propose framework exemplar methods eliciting preferences optimizing ai model according preferences "
130,"fair correlation clustering"," paper study correlation clustering fairness constraints fair variants kmedian kcenter clustering studied recently approximation algorithms using notion called fairlet decomposition proposed obtain approximation algorithms fair correlation clustering several important types fairness constraints results hinge obtaining fairlet decomposition correlation clustering introducing novel combinatorial optimization problem define fairlet decomposition cost similar kmedian cost allows us obtain approximation algorithms wide range fairness constraints complement theoretical results indepth analysis algorithms real graphs show fair solutions correlation clustering can obtained limited increase cost compared stateoftheart unfair algorithms "
131,"fair correlation clustering"," paper study problem correlation clustering fairness constraints classic correlation clustering problem given complete graph edge labeled positive negative goal obtain clustering vertices minimizes disagreements number negative edges trapped inside cluster plus positive edges different clusters consider two variations fairness constraint problem correlation clustering node color goal form clusters overrepresent vertices color first variant aims generate clusters minimum disagreements distribution feature eg gender cluster global distribution case two colors desired ratio number colors cluster p get mathcalopapproximation algorithm algorithm extended case multiple colors prove problem nphard second variant considers relative upper lower bounds number nodes color cluster goal avoid violating upper lower bounds corresponding color cluster minimizing total number disagreements along theoretical results show effectiveness algorithm generate fair clusters empirical evaluation real world data sets "
132,"convex fairness constrained model using causal effect estimators"," recent years seen much research fairness machine learning mean difference md demographic parity one popular measures fairness however md quantifies discrimination also explanatory bias difference outcomes justified explanatory features paper devise novel models called faircees remove discrimination keeping explanatory bias models based estimators causal effect utilizing propensity score analysis prove faircees squared loss theoretically outperform naive md constraint model provide efficient algorithm solving faircees regression binary classification tasks experiment synthetic realworld data two tasks faircees outperformed existing model considers explanatory bias specific cases "
133,"learning individually fair classifier pathspecific causaleffect constraint"," machine learning increasingly used make decisions individuals various fields require achieve good prediction accuracy ensuring fairness respect sensitive features race gender problem however remains difficult complex realworld scenarios effectively quantify unfairness scenarios existing methods utilize pathspecific causal effects however none can ensure fairness individual without making impractical assumptions specifically assumptions require us formulate true datagenerating processes causal model requires extremely deep understanding data unrealistic practice paper propose framework learning individually fair classifier without relying causal model goal define probability individual unfairness piu solve optimization problem constrains upper bound can estimated data without causal model elucidate constraint can guarantee fairness individual experimental results demonstrate method learns individually fair classifier slight cost prediction accuracy "
134,"fair prediction endogenous behavior"," increasing regulatory interest whether machine learning algorithms deployed consequential domains eg criminal justice treat different demographic groups fairly however several proposed notions fairness typically mutually incompatible using criminal justice example study model society chooses incarceration rule agents different demographic groups differ outside options eg opportunity legal employment decide whether commit crimes show equalizing type type ii errors across groups consistent goal minimizing overall crime rate popular notions fairness "
135,"designing fair ai managing employees organizations review critique design agenda"," organizations rapidly deploying artificial intelligence ai systems manage workers however ai found times unfair workers unfairness toward workers associated decreased worker effort increased worker turnover avoid problems ai systems must designed support fairness redress instances unfairness despite attention related ai unfairness theoretical systematic approach developing design agenda paper addresses issue three ways first introduce organizational justice theory three different fairness types distributive procedural interactional frameworks redressing instances unfairness retributive justice restorative justice second review design literature specifically focuses issues ai fairness organizations third propose design agenda ai fairness organizations applies fairness types organizational scenarios paper concludes implications future research "
136,"learning fairnessaware relational structures"," development fair machine learning models effectively avert bias discrimination important problem garnered attention recent years necessity encoding complex relational dependencies among features variables competent predictions require development fair yet expressive relational models work introduce fairasl fairnessaware structure learning algorithm learning relational structures incorporates fairness measures learning relational graphical model structures approach versatile able encode wide range fairness metrics statistical parity difference overestimation equalized odds equal opportunity including recently proposed relational fairness measures existing approaches employ fairness measures predetermined model structures post prediction fairasl directly learns structure optimizing fairness measures hence able remove structural bias model demonstrate effectiveness learned model structures compared stateoftheart fairness models quantitatively qualitatively datasets representing three different modeling scenarios relational dataset ii recidivism prediction dataset widely used studying discrimination iii recommender systems dataset results show fairasl can learn fair yet interpretable expressive structures capable making accurate predictions "
137,"learning certified individually fair representations"," effectively enforce fairness constraints one needs define appropriate notion fairness employ representation learning order impose notion without compromising downstream utility data consumer desirable notion individual fairness guarantees similar treatment similar individuals work introduce first method generalizes individual fairness rich similarity notions via logical constraints also enabling data consumers obtain fairness certificates models key idea learn representation provably maps similar individuals latent representations epsilon apart ellinftydistance enabling data consumers certify individual fairness proving epsilonrobustness classifier experimental evaluation six realworld datasets wide range fairness constraints demonstrates approach expressive enough capture similarity notions beyond existing distance metrics scaling realistic use cases "
138,"fairrec twosided fairness personalized recommendations twosided platforms"," investigate problem fair recommendation context twosided online platforms comprising customers one side producers traditionally recommendation services platforms focused maximizing customer satisfaction tailoring results according personalized preferences individual customers however investigation reveals customercentric design may lead unfair distribution exposure among producers may adversely impact wellbeing hand producercentric design might become unfair customers thus consider fairness issues span customers producers approach involves novel mapping fair recommendation problem constrained version problem fairly allocating indivisible goods proposed fairrec algorithm guarantees least maximin share mms exposure producers envyfree one item ef fairness every customer extensive evaluations multiple realworld datasets show effectiveness fairrec ensuring twosided fairness incurring marginal loss overall recommendation quality "
139,"counterfactual fairness removing direct effects regularization"," building machine learning models fair respect unprivileged group topical problem modern fairnessaware algorithms often ignore causal effects enforce fairness modifications applicable subset machine learning models work propose new definition fairness incorporates causality controlled direct effect cde develop regularizations tackle classical fairness measures present causal regularization satisfies new fairness definition removing impact unprivileged group variables model outcomes measured cde regularizations applicable model trained using iteratively minimizing loss differentiation demonstrate approaches using gradient boosting logistic regression synthetic dataset uci adult census dataset realworld creditrisk dataset results found mitigate unfairness predictions small reductions model performance "
140,"chexclusion fairness gaps deep chest xray classifiers"," machine learning systems received much attention recently ability achieve expertlevel performance clinical tasks particularly medical imaging examine extent stateoftheart deep learning classifiers trained yield diagnostic labels xray images biased respect protected attributes train convolution neural networks predict diagnostic labels three prominent public chest xray datasets mimiccxr chestxray chexpert evaluate tpr disparity difference true positive rates tpr underdiagnosis rate false positive rate nondiagnosis among different protected attributes patient sex age race insurance type demonstrate tpr disparities exist stateoftheart classifiers datasets clinical tasks subgroups find tpr disparities commonly significantly correlated subgroups proportional disease burden find subgroups subsection population chronically underdiagnosed performance disparities real consequences models move papers products carefully audited prior deployment "
141,"evidencebased explanation promote fairness ai systems"," artificial intelligence ai technology gets intertwined every system people using ai make decisions everyday activities simple contexts netflix recommendations complex context like judicial scenarios ai part peoples decisions people make decisions usually need explain decision others matter particularly critical contexts human expertise central decisionmaking order explain decisions ai support people need understand ai part decision considering aspect fairness role ai decisionmaking process becomes even sensitive since affects fairness responsibility people making ultimate decision exploring evidencebased explanation design approach tell story decision position paper discuss approach ai systems using fairness sensitive cases literature "
142,"getting fairness right towards toolbox practitioners"," potential risk ai systems unintentionally embedding reproducing bias attracted attention machine learning practitioners society large policy makers willing set standards algorithms ai techniques issue refine existing regulation order enforce decisions made automated systems fair nondiscriminatory critical meanwhile researchers demonstrated various existing metrics fairness statistically mutually exclusive right choice mostly depends use case definition fairness recognizing solutions implementing fair ai purely mathematical require commitments stakeholders define desired nature fairness paper proposes draft toolbox helps practitioners ensure fair ai practices based nature application available training data also legal requirements ethical philosophical cultural dimensions toolbox aims identify appropriate fairness objective approach attempts structure complex landscape fairness metrics therefore makes different available options accessible nontechnical people proven absence silver bullet solution fair ai toolbox intends produce fairest ai systems possible respect local context "
143,"finding fair efficient allocations valuations dont add "," paper present new results fair efficient allocation indivisible goods agents whose preferences correspond matroid rank functions versatile valuation class several desirable properties monotonicity submodularity naturally lends number realworld domains use properties advantage first show agent valuations matroid rank functions socially optimal ie utilitarian social welfaremaximizing allocation achieves envyfreeness one item ef exists computationally tractable also prove nash welfaremaximizing leximin allocations exhibit fairnessefficiency combination showing can achieved minimizing symmetric strictly convex function utilitarian optimal outcomes best knowledge first valuation function class subsumed additive valuations established allocation maximizing nash welfare ef moreover subclass valuation functions based maximum unweighted bipartite matching show leximin allocation can computed polynomial time additionally explore possible extensions results fairness criteria ef well generalizations valuation classes "
144,"best practices implementing fair vocabularies ontologies web"," adoption semantic web technologies increasing number vocabularies ontologies developed different domains ranging biology agronomy geosciences however many ontologies still difficult find access understand researchers due lack documentation uri resolving issues versioning problems etc chapter describe guidelines best practices creating accessible understandable reusable ontologies web using standard practices pointing existing tools frameworks developed semantic web community illustrate guidelines concrete examples order help researchers implement practices future vocabularies "
145,"games fairness interpretability"," machine learning ml systems becomes ubiquitous ensuring fair equitable application underlying algorithms paramount importance argue one way achieve proactively cultivate public pressure ml developers design develop fairer algorithms one way cultivate public pressure simultaneously serving interests objectives algorithm developers gameplay propose new class games games fairness interpretability one example incentivealigned approach producing fairer equitable algorithms games fairness interpretability carefullydesigned games mass appeal inherently engaging provide insights machine learning models work ultimately produce data helps researchers developers improve algorithms highlight several possible examples games implications fairness interpretability proliferation creative positive public pressure narrowing gap algorithm developers general public machine learning community benefit "
146,"fairness bioinspired optimization research prescription methodological guidelines comparing metaheuristics"," bioinspired optimization including evolutionary computation swarm intelligence growing research topic many competitive bioinspired algorithms proposed every year active area preparing successful proposal new bioinspired algorithm easy task given maturity research field proposing new optimization technique innovative elements longer enough apart novelty results reported authors proven achieve significant advance previous outcomes state art unfortunately new proposals deal requirement properly fail select appropriate benchmark reference algorithms compare cases validation process carried defined principled way even done consequently significance results presented studies guaranteed work review several recommendations literature propose methodological guidelines prepare successful proposal taking issues account expect guidelines useful authors also reviewers editors along assessment new contributions field "
147,"hierarchically fair federated learning"," federated learning adopted among competitive agents siloed datasets agents selfinterested participate fairly rewarded encourage application federated learning paper employs management strategy ie contributions lead rewards propose novel hierarchically fair federated learning hffl framework framework agents rewarded proportion prenegotiated contribution levels hffl extends incorporate heterogeneous models theoretical analysis empirical evaluation several datasets confirm efficacy frameworks upholding fairness thus facilitating federated learning competitive settings "
148,"jealousyfreeness common properties fair division mixed manna"," consider fair division setting indivisible items allocated agents agent setting strictly negative zero strictly positive utility item thus make distinction items good agents bad agents ie mixed good everyone ie goods bad everyone ie bads model study axiomatic concepts allocations jealousyfreeness one item envyfreeness one item paretooptimality obtain many new possibility impossibility results regard combinations properties also investigate new computational tasks related combinations thus advance stateoftheart fair division mixed manna "
149,"ensuring fairness prior probability shifts"," paper study problem fair classification presence prior probability shifts training set distribution differs test set phenomenon can observed yearly records several realworld datasets recidivism records medical expenditure surveys unaccounted shifts can cause predictions classifier become unfair towards specific population subgroups fairness notion called proportional equality pe accounts shifts procedure ensure pefairness unknown work propose method called cape provides comprehensive solution aforementioned problem cape makes novel use prevalence estimation techniques sampling ensemble classifiers ensure fair predictions prior probability shifts introduce metric called prevalence difference pd cape attempts minimize order ensure pefairness theoretically establish metric exhibits several desirable properties evaluate efficacy cape via thorough empirical evaluation synthetic datasets also compare performance cape several popular fair classifiers realworld datasets like compas criminal risk assessment meps medical expenditure panel survey results indicate cape ensures pefair predictions performing well performance metrics "
150,"fair division computer scientists perspective"," survey recent progress classic challenging problem social choice fair division indivisible items discuss computational perspective provided interesting insights understanding divide items fairly efficiently involved bringing bear tools used knowledge representation computational complexity approximation methods game theory online analysis communication complexity "
151," fairness automated bridging gap eu nondiscrimination law ai"," article identifies critical incompatibility european notions discrimination existing statistical measures fairness first review evidential requirements bring claim eu nondiscrimination law due disparate nature algorithmic human discrimination eus current requirements contextual reliant intuition open judicial interpretation automated second show legal protection offered nondiscrimination law challenged ai humans discriminate humans discriminate due negative attitudes eg stereotypes prejudice unintentional biases eg organisational practices internalised stereotypes can act signal victims discrimination occurred finally examine existing work fairness machine learning lines procedures assessing cases eu nondiscrimination law propose conditional demographic disparity cdd standard baseline statistical measurement aligns european court justices gold standard establishing standard set statistical evidence automated discrimination cases can help ensure consistent procedures assessment judicial interpretation cases involving ai automated systems proposal procedural regularity identification assessment automated discrimination clarify build considerations fairness automated systems far possible still respecting enabling contextual approach judicial interpretation practiced eu nondiscrimination law nb abridged abstract "
152,"reputation agent prompting fair reviews gig markets"," study presents new tool reputation agent promote fairer reviews requesters employers customers gig markets unfair reviews created requesters consider factors outside workers control known plague gig workers can result lost job opportunities even termination marketplace tool leverages machine learning implement intelligent interface uses deep learning automatically detect individual included unfair factors review factors outside workers control per policies market prompts individual reconsider review incorporated unfair factors study effectiveness reputation agent conducted controlled experiment different gig markets experiment illustrates across markets reputation agent contrast traditional approaches motivates requesters review gig workers performance fairly discuss tools bring transparency employers policies gig market can help build empathy thus resulting reasoned discussions around potential injustices towards workers generated interfaces vision tools promote truth transparency can bring fairer treatment gig workers "
153,"ethical adversaries towards mitigating unfairness adversarial machine learning"," machine learning integrated growing number critical systems farreaching impacts society unexpected behaviour unfair decision processes coming increasing scrutiny due widespread use theoretical considerations individuals well organisations notice test criticize unfair results hold model designers deployers accountable offer framework assists groups mitigating unfair representations stemming training datasets framework relies two interoperating adversaries improve fairness first model trained goal preventing guessing protected attributes values limiting utility losses first step optimizes models parameters fairness second framework leverages evasion attacks adversarial machine learning generate new examples will misclassified new examples used retrain improve model first step two steps iteratively applied significant improvement fairness obtained evaluated framework wellstudied datasets fairness literature including compas can surpass approaches concerning demographic parity equality opportunity also models utility also illustrate findings subtle difficulties mitigating unfairness highlight framework can assist model designers "
154,"statistical equity fairness classification objective"," machine learning systems shown propagate societal errors past light wealth research focuses designing solutions fair even abundance work singular definition fairness mainly fairness subjective context dependent propose new fairness definition motivated principle equity considers existing biases data attempts make equitable decisions account previous historical biases formalize definition fairness motivate appropriate contexts next operationalize equitable classification perform multiple automatic human evaluations show effectiveness definition demonstrate utility aspects fairness feedback loop "
155,"fair outlier detection"," outlier detection method may considered fair specified sensitive attributes results outlier detection skewed towards particular groups defined sensitive attributes task consider first time best knowledge task fair outlier detection work consider task fair outlier detection multiple multivalued sensitive attributes eg gender race religion nationality marital status etc propose fair outlier detection method fairlof inspired popular lof formulation neighborhoodbased outlier detection outline ways unfairness induced within lof develop three heuristic principles enhance fairness form basis fairlof method novel task develop evaluation framework fair outlier detection use benchmark fairlof quality fairness results extensive empirical evaluation realworld datasets illustrate fairlof able achieve significant improvements fairness sometimes marginal degradations result quality measured fairnessagnostic lof method "
156,"gender slopes counterfactual fairness computer vision models attribute manipulation"," automated computer vision systems applied many domains including security law enforcement personal devices recent reports suggest systems may produce biased results discriminating people certain demographic groups diagnosing understanding underlying true causes model biases however challenging tasks modern computer vision systems rely complex blackbox models whose behaviors hard decode propose use encoderdecoder network developed image attribute manipulation synthesize facial images varying dimensions gender race keeping signals intact use synthesized images measure counterfactual fairness commercial computer vision classifiers examining degree classifiers affected gender racial cues controlled images eg feminine faces may elicit higher scores concept nurse lower scores stemrelated concepts also report skewed gender representations online search service professionrelated keywords may explain origin biases encoded models "
157,"opportunistic multiaspect fairness personalized reranking"," recommender systems become widespread moved areas greater social impact employment housing researchers begun seek ways ensure fairness results systems produce work primarily focused developing recommendation approaches fairness metrics jointly optimized along recommendation accuracy however previous work largely ignored individual preferences may limit ability algorithm produce fair recommendations furthermore exceptions researchers considered scenarios fairness measured relative single sensitive feature attribute race gender paper present reranking approach fairnessaware recommendation learns individual preferences across multiple fairness dimensions uses enhance provider fairness recommendation results specifically show opportunistic metricagnostic approach achieves better tradeoff accuracy fairness prior reranking approaches across multiple fairness dimensions "
158,"fair classification via unconstrained optimization"," achieving bayes optimal binary classification rule subject group fairness constraints known reducible cases learning groupwise thresholding rule bayes regressor paper extend result proving broader setting bayes optimal fair learning rule remains groupwise thresholding rule bayes regressor possible randomization thresholds provides stronger justification postprocessing approach fair classification predictor learned first output adjusted remove bias show postprocessing rule twostage approach can learned quite efficiently solving unconstrained optimization problem proposed algorithm can applied blackbox machine learning model deep neural networks random forests support vector machines addition can accommodate many fairness criteria previously proposed literature equalized odds statistical parity prove algorithm bayes consistent motivate furthermore via impossibility result quantifies tradeoff accuracy fairness across multiple demographic groups finally conclude validating algorithm adult benchmark dataset "
159,"whats sex got fair machine learning"," debate fairness machine learning largely centered around competing definitions fairness nondiscrimination groups requires however little attention paid precisely group many recent approaches fairness require one specify causal model data generating process exercises make implicit ontological assumption racial sex group simply collection individuals share given trait show exploring formal assumption modularity causal models holds dependencies captured one causal pathway invariant interventions pathways causal models sex propose two substantive claims exists feature sexonitsown inherent trait individual causally brings social phenomena external world relations sex effects can modified whichever ways former feature still retain meaning sex world argue ontological picture false many effects sex purportedly causes fact constitutive features sex social status give social meaning sex features meanings precisely make sex discrimination distinctively morally problematic type action correcting conceptual error number implications models can used detect discrimination formal diagrams constitutive relations present entirely different path toward reasoning discrimination whereas causal diagrams guide construction sophisticated modular counterfactuals constitutive diagrams identify different kind counterfactual central inquiry discrimination one asks social meaning group changed nonmodular features altered "
160,"fairnessaware explainable recommendation knowledge graphs"," growing attention fairness considerations recently especially context intelligent decision making systems explainable recommendation systems particular may suffer explanation bias performance disparity paper analyze different groups users according level activity find bias exists recommendation performance different groups show inactive users may susceptible receiving unsatisfactory recommendations due insufficient training data inactive users recommendations may biased training records active users due nature collaborative filtering leads unfair treatment system propose fairness constrained approach via heuristic reranking mitigate unfairness problem context explainable recommendation knowledge graphs experiment several realworld datasets stateoftheart knowledge graphbased explainable recommendation algorithms promising results show algorithm able provide highquality explainable recommendations also reduces recommendation unfairness several respects "
161," quest fair schedule young physicists tournament"," young physicists tournament established teamoriented scientific competition high school students countries continents competition consists scientific discussions called fights three four teams participate fight presents problem rotating roles presenter opponent reviewer observer among rules countries require team announce advance problems will present national tournament task organizers choose composition fights way team presents chosen problems exactly within single fight problem presented besides formalizing feasibility conditions paper formulate several additional fairness conditions tournament schedules show fulfillment can ensured constructing suitable edge colorings bipartite graphs find fair schedules propose integer linear programs test real well randomly generated data "
162,"fair classification noisy protected attributes framework provable guarantees"," due deployment classification algorithms multitude applications directly indirectly affecting people society developing methods fair respect protected attributes gender race crucial however protected attributes datasets may inaccurate due noise data collection protected attributes imputed either whole part inaccuracies can prevent existing fair classification algorithms achieving claimed fairness guarantees motivated recent works studied fair classification problem binary protected attribute noisy protected type flipped known fixed probability either suggesting optimization using tighter statistical equalized odds constraints counter noise identifying conditions prior equalized odds postprocessing algorithms can handle noisy attributes extend study noisetolerant fair classification general setting main contribution optimization framework learning fair classifier presence noisy perturbations protected attributes can employed linear linearfractional class fairness constraints comes probabilistic guarantees accuracy fairness can handle multiple nonbinary protected attributes empirically show framework can used attain either statistical rate false positive rate fairness guarantees minimal loss accuracy even noise corruption large two realworld datasets prior existing noisy fair classification approaches hand either always achieve desired fairness levels suffer larger loss accuracy guaranteeing high fairness compared framework "
163,"deepfair deep learning improving fairness recommender systems"," lack bias management recommender systems leads minority groups receiving unfair recommendations moreover tradeoff equity precision makes difficult obtain recommendations meet criteria propose deep learning based collaborative filtering algorithm provides recommendations optimum balance fairness accuracy without knowing demographic information users experimental results show possible make fair recommendations without losing significant proportion accuracy "
164,"balancing fairness efficiency optimization model"," optimization models generally aim efficiency maximizing total benefit minimizing cost yet tradeoff fairness efficiency important element many practical decisions propose principled practical method balancing two criteria optimization model following critical assessment existing schemes define set social welfare functions swfs combine rawlsian leximax fairness utilitarianism overcome weaknesses previous approaches particular regulate equityefficiency tradeoff single parameter meaningful interpretation practical contexts formulate swfs using mixed integer constraints sequentially maximize subject constraints define problem hand providing practical stepbystep instructions implementation demonstrate method problems realistic size involving healthcare resource allocation disaster preparation solution times modest ranging fraction second seconds given value tradeoff parameter "
165,"system integrate fairness transparently industry approach"," significant research efforts address issue unintentional bias machine learning ml many wellknown companies dealt fallout deployment products due issue industrial context enterprises largescale ml solutions broad class use cases deployed different swaths customers trading cost detecting mitigating bias across landscape lifetime use case risk impact brand image key consideration propose framework industrial uses addresses methodological mechanization needs approach benefits prior experience handling security privacy concerns well past internal ml projects significant reuse bias handling ability every stage ml development lifecycle guide users can lower overall costs reducing bias "
166,"fair influence maximization welfare optimization approach"," several social interventions eg suicide hiv prevention leverage social network information maximize outreach algorithmic influence maximization techniques proposed aid choice influencers peer leaders interventions traditional algorithms influence maximization designed social interventions mind result may disproportionately exclude minority communities benefits intervention motivated research fair influence maximization existing techniques require committing single domainspecific fairness measure makes hard decision maker meaningfully compare notions resulting tradeoffs across different applications address shortcomings extending principles cardinal welfare influence maximization setting underlain complex connections members different communities generalize theory regarding principles show circumstances principles can satisfied welfare function propose family welfare functions governed single inequity aversion parameter allows decision maker study taskdependent tradeoffs fairness total influence effectively trade quantities like influence gap varying parameter use welfare functions fairness notion rule undesirable allocations show resulting optimization problem monotone submodular can solved optimality guarantees finally carry detailed experimental analysis synthetic real social networks high welfare can achieved without sacrificing total influence significantly interestingly can show exists welfare functions empirically satisfy principles "
167,"causal intersectionality fair ranking"," paper propose causal modeling approach intersectional fairness flexible taskspecific method computing intersectionally fair rankings rankings used many contexts ranging web search results college admissions causal inference fair rankings received limited attention additionally growing literature causal fairness directed little attention intersectionality bringing issues together formal causal framework make application intersectionality fair machine learning explicit connected important real world effects domain knowledge transparent technical limitations experimentally evaluate approach real synthetic datasets exploring behaviour different structural assumptions "
168,"fair kmeans clustering"," show popular kmeans clustering algorithm lloyds heuristic used variety scientific data can result outcomes unfavorable subgroups data eg demographic groups biased clusterings can deleterious implications humancentric applications resource allocation present fair kmeans objective algorithm choose cluster centers provide equitable costs different groups algorithm fairlloyd modification lloyds heuristic kmeans inheriting simplicity efficiency stability comparison standard lloyds find benchmark data sets fairlloyd exhibits unbiased performance ensuring groups balanced costs output kclustering incurring negligible increase running time thus making viable fair option wherever kmeans currently used "
169,"probabilistic fair clustering"," clustering problems central decisionmaker given complete metric graph vertices must provide clustering vertices minimizes objective function fair clustering problems vertices endowed color eg membership group features valid clustering might also include representation colors clustering prior work fair clustering assumes complete knowledge group membership paper generalize prior work assuming imperfect knowledge group membership probabilistic assignments present clustering algorithms general setting approximation ratio guarantees also address problem metric membership different groups notion order distance experiments conducted using proposed algorithms well baselines validate approach also surface nuanced concerns group membership known deterministically "
170,"verifying individual fairness machine learning models"," consider problem whether given decision model working structured data individual fairness following work dwork model individually biased unfair pair valid inputs close according appropriate metric treated differently model different class label large difference output unbiased fair pair exists objective construct verifiers proving individual fairness given model considering appropriate relaxations problem construct verifiers sound complete linear classifiers kernelized polynomialradial basis function classifiers also report experimental results evaluating proposed algorithms publicly available datasets "
171,"machine learning pipelines provenance reproducibility fair data principles"," machine learning ml increasingly important scientific tool supporting decision making knowledge generation numerous fields also becomes important results ml experiments reproducible unfortunately often case rather ml similar many disciplines faces reproducibility crisis paper describe goals initial steps supporting endtoend reproducibility ml pipelines investigate factors beyond availability source code datasets influence reproducibility ml experiments propose ways apply fair data practices ml workflows present preliminary results role tool provbook capturing comparing provenance ml experiments reproducibility using jupyter notebooks "
172," framework fairness twosided marketplaces"," many interesting problems internet industry can framed twosided marketplace problem examples include search applications recommender systems showing people jobs movies products restaurants etc incorporating fairness building systems crucial can deep social economic impact applications include job recommendations recruiters searching candidates etc paper propose definition develop endtoend framework achieving fairness building machine learning systems scale extend prior work develop optimization framework can tackle fairness constraints source destination sides marketplace well dynamic aspects problem framework flexible enough adapt different definitions fairness can implemented largescale settings perform simulations show efficacy approach "
173," applicability ml fairness notions"," mlbased predictive systems increasingly used support decisions critical impact individuals lives college admission job hiring child custody criminal risk assessment etc result fairness emerged important requirement guarantee predictive systems discriminate specific individuals entire subpopulations particular minorities given inherent subjectivity viewing concept fairness several notions fairness introduced literature paper survey fairness notions unlike surveys literature addresses question notion fairness suited given realworld scenario attempt answer question consists identifying set fairnessrelated characteristics realworld scenario hand analyzing behavior fairness notion fitting two elements recommend suitable fairness notion every specific setup results summarized decision diagram can used practitioners policy makers navigate relatively large catalogue fairness notions "
174,"fairness machine learning false positive rate equality measure fairness"," machine learning informs increasingly consequential decisions different metrics proposed measuring algorithmic bias unfairness two popular fairness measures calibration equality false positive rate measure seems intuitively important notably usually impossible satisfy measures reason large literature machine learning speaks fairness tradeoff two measures framing assumes measures fact capturing something important date philosophers examined crucial assumption examined extent measure actually tracks normatively important property makes inevitable statistical conflict calibration false positive rate equality important topic ethics paper give ethical framework thinking measures argue contrary initial appearances false positive rate equality track anything fairness thus sets incoherent standard evaluating fairness algorithms "
175,"algorithmic fairness education"," datadriven predictive models increasingly used education support students instructors administrators however concerns fairness predictions uses algorithmic systems introduction algorithmic fairness education draw parallels prior literature educational access bias discrimination examine core components algorithmic systems measurement model learning action identify sources bias discrimination process developing deploying systems statistical similaritybased causal notions fairness reviewed contrasted way apply educational contexts recommendations policy makers developers educational technology offer guidance promote algorithmic fairness education "
176," causal linear model quantify edge unfairness unfair edge prioritization discrimination removal"," dataset can generated unfair mechanism numerous settings instance judicial system unfair rejects bail plea accused based race mitigate unfairness procedure generating dataset need identify sources unfairness quantify unfairness sources quantify sources affect overall unfairness prioritize sources addressing realworld issues underlying prior work zhang et al identifies removes discrimination data generated suggest methodology mitigate unfairness data generation phase use notion unfair edge chiappa et al source discrimination quantify unfairness along unfair edge also quantify overall unfairness particular decision towards subset sensitive attributes terms edge unfairness measure sensitivity former latter varied using formulation cumulative unfairness terms edge unfairness alter discrimination removal methodology discussed zhang et al formulating optimization problem helps getting rid constraints grow exponentially number sensitive attributes values taken finally discuss priority algorithm policymakers address realworld issues underlying edges result unfairness experimental section validates linear model assumption made quantify edge unfairness "
177," impossibility theorem machine fairness causal perspective"," increasing pervasive use machine learning social economic settings interest notion machine bias ai community models trained historic data reflect biases exist society propagated future decisions recent study conducted propublica revealed compas recidivism prediction tool biased africanamerican community three prominent metrics fairness used community statistically proved impossible satisfy time led ambiguity definition fairness report causal perspective impossibility theorem fairness presented along causal goal machine fairness "
178,"fair algorithms multiagent multiarmed bandits"," propose multiagent variant classical multiarmed bandit problem n agents k arms pulling arm generates possibly different stochastic reward agent unlike classical multiarmed bandit problem goal learn best arm agent may perceive different arm best instead seek learn fair distribution arms drawing long line research economics computer science use nash social welfare notion fairness design multiagent variants three classic multiarmed bandit algorithms show achieve sublinear regret now measured terms nash social welfare "
179,"algorithmic stability fair allocation indivisible goods among two agents"," propose notion algorithmic stability scenarios cardinal preferences elicited informally definition captures idea agent experience large change utility long make small innocuous mistakes reporting preferences study notion context fair efficient allocations indivisible goods among two agents show impossible achieve exact stability along even weak notion fairness even approximate efficiency result propose two relaxations stability namely approximatestability weakapproximatestability show existing algorithms fair division literature guarantee fair efficient outcomes perform poorly respect relaxations leads us explore possibility designing new algorithms stable towards end present general characterization result pairwise maximin share allocations turn use design algorithm approximatelystable guarantees pairwise maximin share pareto optimal allocation two agents finally present simple framework can used modify existing fair efficient algorithms order ensure also achieve weakapproximatestability "
180,"fairnessaware online personalization"," decision making crucial applications lending hiring college admissions witnessed increasing use algorithmic models techniques result confluence factors ubiquitous connectivity ability collect aggregate process large amounts finegrained data using cloud computing ease access applying sophisticated machine learning models quite often applications powered search recommendation systems turn make use personalized ranking algorithms time increasing awareness ethical legal challenges posed use datadriven systems researchers practitioners different disciplines recently highlighted potential systems discriminate certain population groups due biases datasets utilized learning underlying recommendation models present study fairness online personalization settings involving ranking individuals starting fair warmstart machinelearned model first demonstrate online personalization can cause model learn act unfair manner user biased hisher responses purpose construct stylized model generating training data potentially biased features well potentially biased labels quantify extent bias learned model user responds biased manner many realworld scenarios formulate problem learning personalized models fairness constraints present regularization based approach mitigating biases machine learning demonstrate efficacy approach extensive simulations different parameter settings code httpsgithubcomgroshanlalfairnessawareonlinepersonalization "
181,"predictability fairness social sensing"," many applications one may benefit collaborative collection data sensing physical phenomenon known social sensing show make social sensing predictable sense guaranteeing number queries per participant will independent initial state expectation even population participants varies time fair sense guaranteeing number queries per participant will equalised among participants expectation even population participants varies time use case consider large highdensity network participating parked vehicles awoken administrative centre network proceeds search moving missing entities interest using rfidbased techniques regulate number geographical distribution parked vehicles switched thus actively searching moving entity interest seek conserve vehicular energy consumption time maintaining good geographical coverage city moving entity interest likely located within acceptable time frame vehicle participants switched point time determined periodically use stochastic techniques illustrated example missing alzheimers patient melbourne australia "
182,"online task scheduling fog computing multiresource fairness"," fog computing systems one key challenge online task scheduling ie decide resource allocation tasks continuously generated end devices design challenging various uncertainties manifested fog computing systems eg tasks resource demands remain unknown actual arrivals recent works applied deep reinforcement learning drl techniques conduct online task scheduling improve various objectives however overlook multiresource fairness different tasks key achieving fair resource sharing among tasks general nontrivial achieve thusly still open problem design online task scheduling scheme multiresource fairness paper address challenges particularly leveraging drl techniques adopting idea dominant resource fairness drf propose fairts online task scheduling scheme learns directly experience effectively shorten average task slowdown ensuring multiresource fairness among tasks simulation results show fairts outperforms stateoftheart schemes ultralow task slowdown better resource fairness "
183,"machine learning fairness justice systems base rates false positives false negatives"," machine learning best practice statements proliferated lack consensus standards fairness standards particular little guidance fairness might achieved practice specifically fairness errors false negatives false positives can pose problem set weights make unavoidable tradeoffs judge models present different kinds errors across racial groups paper considers consequences higher rates false positives one racial group higher rates false negatives another racial group paper examines different errors justice settings can present problems machine learning applications limits computation resolving tradeoffs solutions might crafted courageous conversations leadership line workers stakeholders impacted communities "
184,"memory networks consumer protectionunfairness exposed"," recent work demonstrated datadriven ai methods can leverage consumer protection supporting automated analysis legal documents however shortcoming datadriven approaches poor explainability posit domain useful explanations classifier outcomes can provided resorting legal rationales thus consider several configurations memoryaugmented neural networks rationales given special role modeling context knowledge results show rationales contribute improve classification accuracy also able offer meaningful natural language explanations otherwise opaque classifier outcomes "
185,"lift scalable framework measuring fairness ml applications"," many internet applications powered machine learned models usually trained labeled datasets obtained either implicit explicit user feedback signals human judgments since societal biases may present generation datasets possible trained models biased thereby resulting potential discrimination harms disadvantaged groups motivated need understanding addressing algorithmic bias webscale ml systems limitations existing fairness toolkits present linkedin fairness toolkit lift framework scalable computation fairness metrics part large ml systems highlight key requirements deployed settings present design fairness measurement system discuss challenges encountered incorporating fairness tools practice lessons learned deployment linkedin finally provide open problems based practical experience "
186,"learning fair policies multiobjective deep reinforcement learning average discounted rewards"," operations autonomous systems generally affect simultaneously several users crucial designs account fairness considerations contrast standard deep reinforcement learning rl investigate problem learning policy treats users equitably paper formulate novel rl problem objective function encodes notion fairness formally define optimized problem provide theoretical discussion examine case discounted rewards average rewards analysis notably derive new result standard rl setting independent interest states novel bound approximation error respect optimal average reward policy optimal discounted reward since learning discounted rewards generally easier discussion justifies finding fair policy average reward learning fair policy discounted reward thus describe several classic deep rl algorithms can adapted fair optimization problem validate approach extensive experiments three different domains "
187,"improving fair predictions using variational inference causal models"," importance algorithmic fairness grows increasing impact machine learning peoples lives recent work fairness metrics shows need causal reasoning fairness constraints work practical method named fairtrade proposed creating flexible prediction models integrate fairness constraints sensitive causal paths method uses recent advances variational inference order account unobserved confounders method outline proposed uses causal mechanism estimates audit black box models experiments conducted simulated data real dataset context detecting unlawful social welfare research aims contribute machine learning techniques honour ethical legal boundaries "
188,"adversarial learning counterfactual fairness"," recent years fairness become important topic machine learning research community particular counterfactual fairness aims building prediction models ensure fairness individual level rather globally considering equity entire population idea imagine individual look like variation given attribute interest different gender race instance existing approaches rely variational autoencoding individuals using maximum mean discrepancy mmd penalization limit statistical dependence inferred representations corresponding sensitive attributes enables simulation counterfactual samples used training target fair model goal produce similar outcomes every alternate version individual work propose rely adversarial neural learning approach enables powerful inference mmd penalties particularly better fitted continuous setting values sensitive attributes exhaustively enumerated experiments show significant improvements term counterfactual fairness discrete continuous settings "
189,"fairxgboost fairnessaware classification xgboost"," highly regulated domains finance long favoured use machine learning algorithms scalable transparent robust yield better performance one prominent examples algorithm xgboost meanwhile also growing interest building fair unbiased models regulated domains numerous biasmitigation algorithms proposed end however biasmitigation methods restricted specific model families logistic regression support vector machine models thus leaving modelers difficult decision choosing fairness biasmitigation algorithms scalability transparency performance algorithms xgboost aim leverage best worlds proposing fair variant xgboost enjoys advantages xgboost also matching levels fairness stateoftheart biasmitigation algorithms furthermore proposed solution requires little terms changes original xgboost library thus making easy adoption provide empirical analysis proposed method standard benchmark datasets used fairness community "
190,"fairness eyes data certifying machinelearning models"," present framework allows certify fairness degree model based interactive privacypreserving test framework verifies trained model regardless training process architecture thus allows us evaluate deep learning model multiple fairness definitions empirically tackle two scenarios either test data privately available tester publicly known advance even model creator investigate soundness proposed approach using theoretical analysis present statistical guarantees interactive test finally provide cryptographic technique automate fairness testing certified inference blackbox access model hand hiding participants sensitive data "
191," general framework fairness multistakeholder recommendations"," contemporary recommender systems act intermediaries multisided platforms serving high utility recommendations sellers buyers systems attempt balance objectives multiple stakeholders including sellers buyers platform difficulty providing recommendations maximize utility buyer simultaneously representing sellers platform lead many interesting research problemstraditionally formulated integer linear programs compute recommendations buyers together emphoffline fashion incorporating coverage constraints individual sellers proportionally represented across recommended items approaches can lead unforeseen biases wherein certain buyers consistently receive low utility recommendations order meet global seller coverage constraints remedy situation propose general formulation incorporates seller coverage objectives alongside individual buyer objectives realtime personalized recommender system addition leverage highly scalable submodular optimization algorithms provide recommendations buyer provable theoretical quality bounds furthermore empirically evaluate efficacy approach using data online realestate marketplace "
192," winner dynamic lotteries multigroup fairnessaware recommendation"," recommender systems designed deployed increasing number sociallyconsequential applications become important consider properties fairness systems exhibit considerable research recommendation fairness however argue previous literature based simple uniform often unidimensional notions fairness assumptions recognize realworld complexities fairnessaware applications paper explicitly represent design decisions enter tradeoff accuracy fairness across multiplydefined intersecting protected groups supporting multiple fairness metrics framework also allows recommender adjust performance based historical view recommendations delivered time horizon dynamically rebalancing fairness concerns within framework formulate lotterybased mechanisms choosing fairness concerns demonstrate performance two recommendation domains "
193," identification fair auditors evaluate recommender systems based novel noncomparative fairness notion"," decisionsupport systems information systems offer support peoples decisions various applications judiciary realestate banking sectors lately support systems found discriminatory context many practical deployments attempt evaluate mitigate biases algorithmic fairness literature nurtured using notions comparative justice relies primarily comparing twomore individuals groups within society supported systems however fairness notion useful identification fair auditors hired evaluate latent biases within decisionsupport systems solution introduce paradigm shift algorithmic fairness via proposing new fairness notion based principle noncomparative justice assuming auditor makes fairness evaluations based potentially unknown desired properties decisionsupport system proposed fairness notion compares systems outcome auditors desired outcome show proposed fairness notion also provides guarantees terms comparative fairness notions proving system can deemed fair perspective comparative fairness eg individual fairness statistical parity noncomparatively fair respect auditor deemed fair respect fairness notions also show converse holds true context individual fairness brief discussion also presented regarding fairness notion can used identify fair reliable auditors can use quantify biases decisionsupport systems "
194,"addressing fairness classification modelagnostic multiobjective algorithm"," goal fairness classification learn classifier discriminate groups individuals based sensitive attributes race gender one approach designing fair algorithms use relaxations fairness notions regularization terms constrained optimization problem observe hyperbolic tangent function can approximate indicator function leverage property define differentiable relaxation approximates fairness notions provably better existing relaxations addition propose modelagnostic multiobjective architecture can simultaneously optimize multiple fairness notions multiple sensitive attributes supports statistical paritybased notions fairness use relaxation multiobjective architecture learn fair classifiers experiments public datasets show method suffers significantly lower loss accuracy current debiasing algorithms relative unconstrained model "
195," fairness fake data legal ai"," economics smaller budgets larger case numbers necessitates use ai legal proceedings examine concept disparate impact biases training data lead search fairer ai paper seeks begin discourse implementation actually look like criticism preprocessing methods legal context outline preprocessing used correct biased data examine legal implications effectively changing cases order achieve fairer outcome including black box problem slow encroachment legal precedent finally present recommendations avoid pitfalls preprocessed data methods either modify classifier correct output final step "
196,"fairness matters datadriven framework towards fair high performing facial recognition systems"," facial recognition technologies widely used governmental industrial applications together advancements deep learning dl humancentric tasks accurate age prediction based face images become feasible however issue fairness predicting age different ethnicity gender remains open problem policing systems use age estimate likelihood someone commit crime younger suspects tend likely involved unfair age prediction may lead unfair treatment humans crime prevention also marketing identity acquisition authentication therefore work follows two parts first empirical study conducted evaluating performance fairness stateoftheart systems age prediction including baseline recent works academia main industrial service providers amazon aws microsoft azure building findings present novel approach mitigate unfairness enhance performance using distributionaware dataset curation augmentation distributionawareness based outofdistribution detection utilized validate equal diverse dl system behavior towards eg ethnicity gender total train dnn models utilize one million data points assess performance fairness stateoftheart face recognition algorithms demonstrate improvement mean absolute age prediction error years fold increase fairness towards ethnicity compared related work utilizing presented methodology able outperform leading industry players amazon aws microsoft azure fairness age prediction accuracy provide necessary guidelines assess quality enhance face recognition systems based dl techniques "
197,"active fairness instead unawareness"," possible risk ai systems promote discrimination reproducing enforcing unwanted bias data broadly discussed research society many current legal standards demand remove sensitive attributes data order achieve fairness unawareness argue approach obsolete era big data large datasets highly correlated attributes common contrary propose active use sensitive attributes purpose observing controlling kind discrimination thus leading fair results "
198,"neither private fair impact data imbalance utility fairness differential privacy"," deployment deep learning different fields industries growing day day due performance relies availability data compute data often crowdsourced contains sensitive information contributors leaks models trained achieve rigorous privacy guarantees differentially private training mechanisms used however recently shown differential privacy can exacerbate existing biases data disparate impacts accuracy different subgroups data paper aim study effects within differentially private deep learning specifically aim study different levels imbalance data affect accuracy fairness decisions made model given different levels privacy demonstrate even small imbalances loose privacy guarantees can cause disparate impacts "
199,"justicia stochastic sat approach formally verify fairness"," technology ml oblivious societal good bad thus field fair machine learning stepped propose multiple mathematical definitions algorithms systems ensure different notions fairness ml applications given multitude propositions become imperative formally verify fairness metrics satisfied different algorithms different datasets paper propose textitstochastic satisfiability ssat framework justicia formally verifies different fairness measures supervised learning algorithms respect underlying data distribution instantiate justicia multiple classification bias mitigation algorithms datasets verify different fairness metrics disparate impact statistical parity equalized odds justicia scalable accurate operates nonboolean compound sensitive attributes unlike existing distributionbased verifiers fairsquare verifair distributionbased design justicia robust verifiers aif operate specific test samples also theoretically bound finitesample error verified fairness measure "
200,"group fairness probabilistic modeling latent fair decisions"," machine learning systems increasingly used make impactful decisions loan applications criminal justice risk assessments ensuring fairness systems critical often challenging labels data biased paper studies learning fair probability distributions biased data explicitly modeling latent variable represents hidden unbiased label particular aim achieve demographic parity enforcing certain independencies learned model also show group fairness guarantees meaningful distribution used provide guarantees indeed captures realworld data order closely model data distribution employ probabilistic circuits expressive tractable probabilistic model propose algorithm learn incomplete data evaluate approach synthetic dataset observed labels indeed come fair labels added bias demonstrate fair labels successfully retrieved moreover show realworld datasets approach better model existing methods data generated also achieves competitive accuracy "
201,"understanding fairness gender classification algorithms across genderrace groups"," automated gender classification important applications many domains demographic research law enforcement online advertising well humancomputer interaction recent research questioned fairness technology across gender race specifically majority studies raised concern higher error rates facebased gender classification system darkerskinned people like africanamerican women however date majority existing studies limited africanamerican caucasian aim paper investigate differential performance gender classification algorithms across genderrace groups aim investigate impact architectural differences deep learning algorithms b training set imbalance potential source bias causing differential performance across gender race experimental investigations conducted two latest largescale publicly available facial attribute datasets namely utkface fairface experimental results suggested algorithms architectural differences varied performance consistency towards specific genderrace groups instance algorithms used black females black race general always obtained least accuracy rates middle eastern males latino females obtained higher accuracy rates time training set imbalance widens gap unequal accuracy rates across genderrace groups investigations using facial landmarks suggested facial morphological differences due bone structure influenced genetic environmental factors cause least performance black females black race general "
202,"differentially private fair deep learning lagrangian dual approach"," critical concern datadriven decision making build models whose outcomes discriminate demographic groups including gender ethnicity age ensure nondiscrimination learning tasks knowledge sensitive attributes essential practice attributes may available due legal ethical requirements address challenge paper studies model protects privacy individuals sensitive information also allowing learn nondiscriminatory predictors method relies notion differential privacy use lagrangian duality design neural networks can accommodate fairness constraints guaranteeing privacy sensitive attributes paper analyses tension accuracy privacy fairness experimental evaluation illustrates benefits proposed model several prediction tasks "
203,"fair metalearning fewshot classification"," artificial intelligence nowadays plays increasingly prominent role life since decisions made humans now delegated automated systems machine learning algorithm trained based biased data however tends make unfair predictions developing classification algorithms fair respect protected attributes data thus becomes important problem motivated concerns surrounding fairness effects sharing fewshot machine learning tools model agnostic metalearning framework propose novel fair fastadapted fewshot metalearning approach efficiently mitigates biases metatrain ensuring controlling decision boundary covariance protected variable signed distance feature vectors decision boundary extensive experiments two realworld image benchmarks three stateoftheart metalearning algorithms empirically demonstrate proposed approach efficiently mitigates biases model output generalizes accuracy fairness unseen tasks limited amount training samples "
204,"towards measure individual fairness deep learning"," deep learning produced big advances artificial intelligence trained neural networks often reflect amplify bias training data thus produce unfair predictions propose novel measure individual fairness called prediction sensitivity approximates extent particular prediction dependent protected attribute show compute prediction sensitivity using standard automatic differentiation capabilities present modern deep learning frameworks present preliminary empirical results suggesting prediction sensitivity may effective measuring bias individual predictions "
205,"fairness diversity rankings twosided markets"," ranking items probability relevance long goal conventional ranking systems maximizes traditional criteria ranking performance growing understanding oversimplification online platforms serve diverse user population also producers items particular ranking algorithms expected fair serve groups users just majority group also need fair divide exposure among items fairness considerations can partially met adding diversity rankings done several recent works show paper user fairness item fairness diversity fundamentally different concepts particular find algorithms consider one three desiderata can fail satisfy even harm two overcome shortcoming present first ranking algorithm explicitly enforces three desiderata algorithm optimizes user item fairness convex optimization problem can solved optimally solution ranking policy can derived via new birkhoffvon neumann decomposition algorithm optimizes diversity beyond theoretical analysis provide comprehensive empirical evaluation new benchmark dataset show effectiveness proposed ranking algorithm controlling three desiderata interplay "
206,"astraea grammarbased fairness testing"," software often produces biased outputs particular machine learning ml based software known produce erroneous predictions processing discriminatory inputs unfair program behavior can caused societal bias last years amazon microsoft google provided software services produce unfair outputs mostly due societal bias eg gender race events developers saddled task conducting fairness testing fairness testing challenging developers tasked generating discriminatory inputs reveal explain biases propose grammarbased fairness testing approach called astraea leverages contextfree grammars generate discriminatory inputs reveal fairness violations software systems using probabilistic grammars astraea also provides fault diagnosis isolating cause observed software bias astraeas diagnoses facilitate improvement ml fairness astraea evaluated software systems provide three major natural language processing nlp services evaluation astraea generated fairness violations rate astraea generated k discriminatory test cases found k fairness violations furthermore astraea improves software fairness via modelretraining "
207,"fairmixrep selfsupervised robust representation learning heterogeneous data fairness constraints"," representation learning heterogeneous space mixed variables numerical categorical types interesting challenges due complex feature manifold moreover feature learning unsupervised setup without class labels suitable learning loss function adds problem complexity learned representation subsequent predictions reflect discriminatory behavior towards certain sensitive groups attributes proposed feature map preserve maximum variations present data needs fair respect sensitive variables propose first phase work efficient encoderdecoder framework capture mixeddomain information second phase work focuses debiasing mixed space representations adding relevant fairness constraints ensures minimal information loss representations fairnesspreserving projections information content fairness aspect final representation learned validated several metrics shows excellent performance work fairmixrep addresses problem mixed space fair representation learning unsupervised perspective learns universal representation timely unique novel research contribution "
208," banditbased algorithm fairnessaware hyperparameter optimization"," considerable research effort guided towards algorithmic fairness still major breakthrough practice exhaustive search possible techniques hyperparameters needed find optimal fairnessaccuracy tradeoffs hence coupled lack tools ml practitioners realworld adoption bias reduction methods still scarce address present fairband banditbased fairnessaware hyperparameter optimization ho algorithm fairband conceptually simple resourceefficient easy implement agnostic objective metrics model types hyperparameter space explored moreover introducing fairness notions ho enable seamless efficient integration fairness objectives realworld ml pipelines compare fairband popular ho methods four realworld decisionmaking datasets show fairband can efficiently navigate fairnessaccuracy tradeoff hyperparameter optimization furthermore without extra training cost consistently finds configurations attaining substantially improved fairness comparatively small decrease predictive accuracy "
209,"assessing fairness classifiers collider bias"," increasing maturity machine learning technologies applications decisions relate everyday decision making brought concerns fairness decisions however current fairness assessment systems often suffer collider bias leads spurious association protected attribute outcomes achieve fairness evaluation prediction models individual level paper develop causalitybased theorems support use direct causal effect estimation fairness assessment given classifier without access original training data based theorems unbiased situation test method presented assess individual fairness predictions classifier elimination impact collider bias classifier fairness assessment extensive experiments performed synthetic realworld data evaluate performance proposed method experimental results show proposed method reduces bias significantly "
210,"metrics methods systematic comparison fairnessaware machine learning algorithms"," understanding removing bias decisions made machine learning models essential avoid discrimination unprivileged groups despite recent progress algorithmic fairness still clear answer biasmitigation approaches effective evaluation strategies typically usecase specific rely data unclear bias employ fixed policy convert model outputs decision outcomes address problems performed systematic comparison number popular fairness algorithms applicable supervised classification study comprehensive kind utilizes three real four synthetic datasets two different ways converting model outputs decisions considers fairness predictiveperformance calibration quality speed different modelling pipelines corresponding fairnessunaware fairnessaware algorithms found fairnessunaware algorithms typically fail produce adequately fair models simplest algorithms necessarily fairest ones also found fairnessaware algorithms can induce fairness without material drops predictive power finally found dataset idiosyncracies eg degree intrinsic unfairness nature correlations affect performance fairnessaware approaches results allow practitioner narrow approaches like adopt without know advance fairness requirements "
211,"cryptocredit securely training fair models"," developing models regulated decision making sensitive features like age race gender used must obscured model developers prevent bias however remaining features still need tested correlation sensitive features can done knowledge features resolve dilemma using fully homomorphic encryption scheme allowing model developers train linear regression logistic regression models test possible bias without ever revealing sensitive features clear demonstrate can applied leaveoneout regression testing show using adult income data set method practical run "
212,"fairness perception networkcentric perspective"," algorithmic fairness major concern recent years influence machine learning algorithms becomes widespread paper investigate issue algorithmic fairness networkcentric perspective specifically introduce novel yet intuitive function known networkcentric fairness perception provide axiomatic approach analyze properties using peerreview network case study also examine utility terms assessing perception fairness paper acceptance decisions show function can extended group fairness metric known fairness visibility demonstrate relationship demographic parity also illustrate potential pitfall fairness visibility measure can exploited mislead individuals perceiving algorithmic decisions fair demonstrate problem can alleviated increasing local neighborhood size fairness perception function "
213,"fairn fair robust neural networks structured data"," fairness machine learning crucial individuals subject automated decisions made models highstake domains organizations employ models may also need satisfy regulations promote responsible ethical ai fairness metrics relying comparing model error rates across subpopulations widely investigated detection mitigation bias fairness terms equalized ability achieve recourse different protected attribute groups relatively unexplored present novel formulation training neural networks considers distance data points decision boundary new objective reduces average distance decision boundary two groups individuals subject negative outcome group ie network fair respect ability obtain recourse increases average distance data points boundary promote adversarial robustness demonstrate training loss yields fair robust neural networks similar accuracies models trained without moreover qualitatively motivate empirically show reducing recourse disparity across groups also improves fairness measures rely error rates best knowledge first time recourse capabilities across groups considered train fairer neural networks relation error rates based fairness recourse based fairness investigated "
214," fairness causal algorithmic recourse"," many recent works studied problem algorithmic fairness perspective predictions investigate fairness recourse actions recommended individuals recover unfavourable classification end propose two new fairness criteria group individual level whichunlike prior work equalising average distance decision boundary across protected groupsare based causal framework explicitly models relationships input features thereby allowing capture downstream effects recourse actions performed physical world explore criteria relate others counterfactual fairness show fairness recourse complementary fairness prediction investigate enforce fair recourse training classifier finally discuss whether fairness violations data generating process revealed criteria may better addressed societal interventions structural changes system opposed constraints classifier "
215,"equitable allocation healthcare resources fair cox models"," healthcare programs medicaid provide crucial services vulnerable populations due limited resources many individuals need services languish waiting lists survival models eg cox proportional hazards model can potentially improve situation predicting individuals levels need can used prioritize waiting lists providing care need can prevent institutionalization individuals improves quality life reduces overall costs benefits approach clear care must taken ensure prioritization process fair independent demographic informationbased harmful stereotypes work develop multiple fairness definitions survival models corresponding fair cox proportional hazards models ensure equitable allocation healthcare resources demonstrate utility methods terms fairness predictive accuracy two publicly available survival datasets "
216,"representativity fairness clustering"," incorporating fairness constructs machine learning algorithms topic much societal importance recent interest clustering fundamental task unsupervised learning manifests across number web data scenarios also subject attention within fair ml research paper develop novel notion fairness clustering called representativity fairness representativity fairness motivated need alleviate disparity across objects proximity assigned cluster representatives aid fairer decision making illustrate importance representativity fairness realworld decision making scenarios involving clustering provide ways quantifying objects representativity fairness develop new clustering formulation rfkm targets optimize representativity fairness along clustering quality inspired kmeans framework rfkm incorporates novel loss terms formulate objective function rfkm objective optimization approach guides towards clustering configurations yield higher representativity fairness empirical evaluation variety public datasets establish effectiveness method illustrate able significantly improve representativity fairness marginal impact clustering quality "
217,"exchanging lessons algorithmic fairness domain generalization"," standard learning approaches designed perform well average data distribution available training time developing learning approaches overly sensitive training distribution central research domain outofdistribution generalization robust optimization fairness work focus links research domain generalization algorithmic fairness performance distinct related test distributions studied show two fields can mutually beneficial domain generalization methods typically rely knowledge disjoint domains environments sensitive label information indicating demographic groups risk discrimination often used fairness literature drawing inspiration recent fairness approaches improve worstcase performance without knowledge sensitive groups propose novel domain generalization method handles realistic scenario environment partitions provided show theoretically empirically different partitioning schemes can lead increased decreased generalization performance enabling us outperform invariant risk minimization handcrafted environments multiple cases also show reinterpretation irmv allows us first time directly optimize common fairness criterion groupsufficiency thereby improve performance fair prediction task "
218,"explainability fair machine learning"," decisions made influenced machine learning models increasingly impact lives crucial detect understand mitigate unfairness even simply determining unfairness mean given context nontrivial many competing definitions choosing often requires deep understanding underlying task thus tempting use model explainability gain insights model fairness however existing explainability tools reliably indicate whether model indeed fair work present new approach explaining fairness machine learning based shapley value paradigm fairness explanations attribute models overall unfairness individual input features even cases model operate sensitive attributes directly moreover motivated linearity shapley explainability propose meta algorithm applying existing trainingtime fairness interventions wherein one trains perturbation original model rather new model entirely explaining original model perturbation faircorrected model gain insight accuracyfairness tradeoff made intervention show meta algorithm enjoys flexibility stability benefits loss performance "
219,"model reconstruction model explanations"," show theory experiment gradientbased explanations model quickly reveal model results speak tension desire keep proprietary model secret ability offer model explanations theoretical side give algorithm provably learns twolayer relu network setting algorithm may query gradient model respect chosen inputs number queries independent dimension nearly optimal dependence model size interest learningtheoretic perspective result highlights power gradients rather labels learning primitive complementing theory give effective heuristics reconstructing models gradient explanations orders magnitude queryefficient reconstruction attacks relying prediction interfaces "
220,"actionable recourse linear classification"," classification models often used make decisions affect humans whether approve loan application extend job offer provide insurance applications individuals ability change decision model person denied loan credit scoring model example able change input variables model way will guarantee approval otherwise person will denied loan long model deployed importantly will lack agency decision affects livelihood paper propose evaluate linear classification model terms recourse define ability person change decision model actionable input variables eg income vs age marital status present integer programming toolkit measure feasibility difficulty recourse target population ii generate list actionable changes person obtain desired outcome discuss tools can inform different stakeholders using audit recourse credit scoring models built realworld datasets results illustrate recourse can significantly affected common modeling practices motivate need evaluate recourse algorithmic decisionmaking "
221,"efficient search diverse coherent explanations"," paper proposes new search algorithms counterfactual explanations based upon mixed integer programming concerned complex data variables may take value contiguous range additional set discrete states propose novel set constraints refer mixed polytope show can used integer programming solver efficiently find coherent counterfactual explanations ie solutions guaranteed map back onto underlying data structure avoiding need bruteforce enumeration also look problem diverse explanations show can generated within framework "
222," human predictions explanations predictions machine learning models case study deception detection"," humans final decision makers critical tasks involve ethical legal concerns ranging recidivism prediction medical diagnosis fighting fake news although machine learning models can sometimes achieve impressive performance tasks tasks amenable full automation realize potential machine learning improving human decisions important understand assistance machine learning models affects human performance human agency paper use deception detection testbed investigate can harness explanations predictions machine learning models improve human performance retaining human agency propose spectrum full human agency full automation develop varying levels machine assistance along spectrum gradually increase influence machine predictions find without showing predicted labels explanations alone slightly improve human performance end task comparison human performance greatly improved showing predicted labels relative improvement can improved explicitly suggesting strong machine performance interestingly predicted labels shown explanations machine predictions induce similar level accuracy explicit statement strong machine performance results demonstrate tradeoff human performance human agency show explanations machine predictions can moderate tradeoff "
223,"problem formulation fairness"," formulating data science problems uncertain difficult process requires various forms discretionary work translate highlevel objectives strategic goals tractable problems necessitating among things identification appropriate target variables proxies choices rarely selfevident normative assessments data science projects often take granted even though different translations can raise profoundly different ethical concerns whether consider data science project fair often much formulation problem property resulting model building six months ethnographic fieldwork corporate data science teamand channeling ideas sociology history science critical data studies early writing knowledge discovery databaseswe describe complex set actors activities involved problem formulation research demonstrates specification operationalization problem always negotiated elastic rarely worked explicit normative considerations mind show careful accounts everyday data science work can help us better understand data science problems posed certain waysand specific formulations prevail practice even face might seem like normatively preferable alternatives conclude discussing implications findings arguing effective normative interventions will require attending practical work problem formulation "
224," years test unfairness lessons machine learning"," quantitative definitions unfair fair introduced multiple disciplines well years including education hiring machine learning trace notion fairness defined within testing communities education hiring past half century exploring cultural social context different fairness definitions emerged cases earlier definitions fairness similar identical definitions fairness current machine learning research foreshadow current formal work cases insights fairness means measure largely gone overlooked compare past current notions fairness along several dimensions including fairness criteria focus criteria eg test model use relationship fairness individuals groups subgroups mathematical method measuring fairness eg classification regression work points way towards future research measurement unfairness builds modern understanding fairness incorporating insights past "
225,"fairness abstraction sociotechnical systems"," key goal fairml community develop machinelearning based systems introduced social context can achieve social legal outcomes fairness justice due process bedrock concepts computer sciencesuch abstraction modular designare used define notions fairness discrimination produce fairnessaware learning algorithms intervene different stages decisionmaking pipeline produce fair outcomes paper however contend concepts render technical interventions ineffective inaccurate sometimes dangerously misguided enter societal context surrounds decisionmaking systems outline mismatch five traps fairml work can fall even attempts contextaware comparison traditional data science draw studies sociotechnical systems science technology studies explain traps occur avoid finally suggest ways technical designers can mitigate traps refocusing design terms process rather solutions drawing abstraction boundaries include social actors rather purely technical ones "
226,"clear sanctions vague rewards chinas social credit system currently defines good bad behavior"," chinas social credit system scs 社会信用体系 shehui xinyong tixi expected become first digitallyimplemented nationwide scoring system purpose rate behavior citizens companies entities thereby scs good behavior can result material rewards reputational gain bad behavior can lead exclusion material resources reputational loss crucially implementation scs society must able distinguish behaviors result reward lead sanction paper conduct first transparency analysis two central administrative information platforms scs understand scs currently defines good bad behavior analyze behavioral records reports citizens behaviors published official beijing scs website national scs platform credit china respectively applying mixedmethod approach demonstrate considerable asymmetry information provided socalled redlist information good behavior blacklist information bad behavior current stage scs implementation majority explanations blacklisted behaviors includes detailed description causal relation inadequate behavior sanction hand explanations redlisted behavior comprise positive norms fostering value internalization integration less transparent finally first scs transparency analysis suggests sociotechnical systems applying scoring mechanism might use different degrees transparency achieve particular behavioral engineering goals "
227," taxonomy ethical tensions inferring mental health states social media"," powered machine learning techniques social media provides unobtrusive lens individual behaviors emotions psychological states recent research successfully employed social media data predict mental health states individuals ranging presence severity mental disorders like depression risk suicide algorithmic inferences hold great potential supporting early detection treatment mental disorders design interventions time outcomes research can pose great risks individuals issues incorrect opaque algorithmic predictions involvement bad unaccountable actors potential biases intentional inadvertent misuse insights amplifying tensions also divergent sometimes inconsistent methodological gaps underexplored ethics privacy dimensions paper presents taxonomy concerns ethical challenges drawing existing literature poses questions resolved research gains traction identify three areas tension ethics committees gap social media research questions validity data machine learning implications research key stakeholders conclude calls action begin resolving interdisciplinary dilemmas "
228,"dissecting racial bias algorithm guides health decisions million people"," single algorithm drives important health care decision million people us health systems anticipate patient will especially complex intensive future health care needs enrolled care management program provides considerable additional resources greater attention trained providers help coordination care determine patients will complex future health care needs thus benefit program enrollment many systems rely algorithmically generated commercial risk score paper exploit rich dataset study racial bias commercial algorithm deployed nationwide today many uss prominent accountable care organizations acos document significant racial bias widely used algorithm using data primary care patients large hospital blacks whites algorithmic risk scores different realized health example highestrisk black patients threshold patients autoenrolled program significantly chronic illnesses white enrollees risk score use detailed physiological data show pervasiveness bias across range biomarkers hbac levels diabetics blood pressure control hypertensives find significant racial health gaps conditional risk score bias significant material consequences patients effectively means white patients health black patients far likely enrolled care management program benefit resources simulated world without gap predictions blacks autoenrolled program double current rate unusual aspect dataset observe just risk scores also input data objective function used construct provides unique window mechanisms bias arises algorithm given data frame yit label total medical expenditures costs year t xit features finegrained care utilization data year t eg visits cardiologists number xrays etc algorithms predicted risk developing complex health needs thus fact predicted costs metric one easily call algorithm unbiased costs similar black white patients risk scores far inconsistent algorithmic bias conditional risk score predictions favor whites blacks fundamental problem uncover thinking health care needs hospitals insurers focus costs use algorithm whose specific objective cost prediction perspective predictions accurate unbiased yet social perspective actual health just costs also matters problem arises costs health costs reasonable proxy health sick cost average imperfect one factors health can drive cost example race find blacks cost whites average gap can decomposed two countervailing effects first blacks bear different larger burden disease making costlier difference illness offset second factor blacks cost less holding constant exact chronic conditions force dramatically reduces overall cost gap perversely fact blacks cost less whites conditional health means algorithm predicts costs accurately across racial groups will necessarily also generate biased predictions health root cause bias procedure prediction underlying data algorithms objective function bias akin distinct mismeasured labels arises choice labels measurement turn consequence differing objective functions private actors health sector society private perspective variable focus cost appropriately optimized results hint algorithms may amplify fundamental problem health care whole externalities produced health care providers focus narrowly financial motives optimizing costs detriment health sense results suggest pervasive problem health care incentives induce health systems focus dollars rather health also consequences way algorithms built monitored "
229,"disparate interactions algorithmintheloop analysis fairness risk assessments"," despite vigorous debates technical characteristics risk assessments deployed us criminal justice system remarkably little research studied tools affect actual decisionmaking processes risk assessments make definitive decisionsthey inform judges final arbiters therefore essential considerations risk assessments informed rigorous studies judges actually interpret use paper takes first step toward research human interactions risk assessments controlled experimental study amazon mechanical turk found several behaviors call question supposed efficacy fairness risk assessments study participants underperformed risk assessment even presented predictions effectively evaluate accuracy risk assessments predictions exhibited behaviors fraught disparate interactions whereby use risk assessments led higher risk predictions black defendants lower risk predictions white defendants results suggest need new algorithmintheloop framework places machine learning decisionmaking aids sociotechnical context improving human decisions rather technical context generating best prediction abstract risk assessments used must grounded rigorous evaluations realworld impacts instead theoretical potential "
230," empirical study rich subgroup fairness machine learning"," kearns neel roth wu icml recently proposed notion rich subgroup fairness intended bridge gap statistical individual notions fairness rich subgroup fairness picks statistical fairness constraint say equalizing false positive rates across protected groups asks constraint hold exponentially infinitely large collection subgroups defined class functions bounded vc dimension give algorithm guaranteed learn subject constraint condition access oracles perfectly learning absent fairness constraint paper undertake extensive empirical evaluation algorithm kearns et al four real datasets fairness concern investigate basic convergence algorithm instantiated fast heuristics place learning oracles measure tradeoffs fairness accuracy compare approach recent algorithm agarwal beygelzeimer dudik langford wallach icml implements weaker traditional marginal fairness constraints defined individual protected attributes find general kearns et al algorithm converges quickly large gains fairness can obtained mild costs accuracy optimizing accuracy subject marginal fairness leads classifiers substantial subgroup unfairness also provide number analyses visualizations dynamics behavior kearns et al algorithm overall find algorithm effective real data rich subgroup fairness viable notion practice "
231," profiling potential computer vision challenge computational empiricism"," computer vision biometrics data science applications commenced new project profiling people rather using transaction generated information systems measure real world produce assessment world state case assessment individual trait instead using proxies scores evaluate people increasingly deploy logic revealing truth reality people within profiling knowledge claims sometimes tentative increasingly suggest computation can excesses reality captured understood article explores bases claims systems measurement representation classification deployed computer vision asks something new type knowledge claim sketches account new form computational empiricism operationalised questions kind human subject constructed technological systems practices finally article explores legal mechanisms contesting emergence computational empiricism dominant knowledge platform understanding world people within "
232,"bias bios case study semantic representation bias highstakes setting"," present largescale study gender bias occupation classification task use machine learning may lead negative outcomes peoples lives analyze potential allocation harms can result semantic representation bias study impact occupation classification including explicit gender indicatorssuch first names pronounsin different semantic representations online biographies additionally quantify bias remains indicators scrubbed describe proxy behavior occurs absence explicit gender indicators demonstrate differences true positive rates genders correlated existing gender imbalances occupations may compound imbalances "
233,"equality voice towards fair representation crowdsourced topk recommendations"," help users discover important items particular time major websites like twitter yelp tripadvisor nytimes provide topk recommendations eg trending topics top hotels paris viewed news stories rely crowdsourced popularity signals select items however different sections crowd may different preferences large silent majority explicitly express opinion also crowd often consists actors like bots spammers people running orchestrated campaigns recommendation algorithms today largely consider nuances hence vulnerable strategic manipulation small hyperactive user groups fairly aggregate preferences users recommending topk items borrow ideas prior research social choice theory identify voting mechanism called single transferable vote stv many fairness properties desire topk item selections develop innovative mechanism attribute preferences silent majority also make stv completely operational show generalizability approach implementing two different realworld datasets extensive experimentation comparison stateoftheart techniques show proposed approach provides maximum user satisfaction cuts drastically items disliked hyperactively promoted users "
234,"analyzing biases perception truth news stories implications fact checking"," recently social media sites like facebook twitter severely criticized policy makers media watchdog groups allowing fake news stories spread unchecked platforms response sites encouraging users report news story encounter site perceive fake stories reported fake large number users prioritized fact checking human experts fact checking organizations like snopes politifact thus social media sites today relying users perceptions truthfulness news stories select stories fact check however studies focused understanding users perceive truth news stories biases perceptions might affect current strategies detect label fake news stories end present indepth analysis users perceptions truth news stories specifically analyze users truth perception biases stories fact checked snopes based ground truth truth value perceived users can classify stories four categories c false stories perceived false users ii c true stories perceived false users iii c false stories perceived true users iv c true stories perceived true users stories likely reported flagged fact checking two classes c c lowest perceived truth levels argue little gained fact checking stories c whose truth value correctly perceived users although stories c reveal cynicality users true stories social media sites presently explicitly mark true resolve confusion contrary stories c false stories yet perceived true users arguably stories damaging c truth values story former situation incorrectly perceived truth values latter correctly perceived nevertheless stories c likely fact checked greater priority stories c fact todays social media sites higher gullibility users towards believing false story less likely reported fact checking summary make following contributions work methodological develop novel method assessing users truth perceptions news stories design test users rapidly assess ie rate seconds per story truthful untruthful claims news story conduct truth perception tests online gather truth perceptions usbased amazon mechanical turk workers story empirical exploratory analysis users truth perceptions reveal several interesting insights instance many stories collective wisdom crowd average truth rating differs significantly actual truth story ie wisdom crowds inaccurate ii across different stories find evidence false positive perception bias ie gullible user perceiving story true reality false negative perception bias ie cynical user perceiving story false reality iii users political ideologies influence truth perceptions controversial stories frequently result users political ideologies influencing truth perceptions practical based observations call prioritizing stories fact check order achieve following three important goals remove false news stories circulation ii correct misperception users iii decrease disagreement different users perceptions truth finally provide strategies utilize users truth perceptions predictive analysis biases achieve three goals stated prioritizing stories fact checking full paper available httpsbitlytrafo "
235," microtargeting socially divisive ads case study russialinked ad campaigns facebook"," targeted advertising meant improve efficiency matching advertisers customers however targeted advertising can also abused malicious advertisers efficiently reach people susceptible false stories stoke grievances incite social conflict since targeted ads seen nontargeted nonvulnerable people malicious ads likely go unreported effects undetected work examines specific case malicious advertising exploring extent political ads russian intelligence research agency ira run prior us elections exploited facebooks targeted advertising infrastructure efficiently target ads divisive polarizing topics eg immigration racebased policing vulnerable subpopulations particular following conduct us censusrepresentative surveys characterize users different political ideologies report approve perceive truth content ira ads surveys show many ads divisive elicit different reactions people belonging different socially salient groups b characterize divisive ads targeted subpopulations feel particularly aggrieved status quo findings support existing calls greater transparency content targeting political ads c particularly focus facebook ad api facilitates targeting show enormous amount personal data facebook aggregates users makes available advertisers enables malicious targeting "
236,"siren simulation framework understanding effects recommender systems online news environments"," growing volume digital data stimulates adoption recommender systems different socioeconomic domains including news industries news recommenders help consumers deal information overload increase engagement use also raises increasing number societal concerns matthew effects filter bubbles overall lack transparency argue focusing transparency contentproviders underexplored avenue designed simulation framework called siren simulating recommender effects online news environments allows content providers select parameterize different recommenders ii analyze visualize effects respect two diversity metrics taking us news media case study present analysis recommender effects respect longtail novelty unexpectedness using siren analysis offers number interesting findings similar potential certain algorithmically simple itembased knearest neighbour sophisticated strategies based bayesian personalized ranking increase diversity time overall argue simulating effects recommender systems can help content providers make informed decisions choosing algorithmic recommenders can help mitigate aforementioned societal concerns "
237,"controlling polarization personalization algorithmic framework"," personalization pervasive online space leads higher efficiency user higher revenue platform individualizing relevant content user however recent studies suggest personalization can learn propagate systemic biases polarize opinions led calls regulatory mechanisms algorithms constrained combat bias resulting echochamber effect propose versatile framework allows possibility reduce polarization personalized systems allowing user constrain distribution content selected present scalable algorithm provable guarantees satisfies given constraints types content can displayed user subject constraints will continue learn personalize content order maximize utility illustrate framework curated dataset online news articles conservative liberal show can control polarization examine tradeoff decreasing polarization resulting loss revenue exhibit flexibility scalability approach framing problem terms general diverse content selection problem test empirically news dataset movielens dataset "
238,"fair algorithms learning allocation problems"," settings lending policing can modeled centralized agent allocating scarce resource eg loans police officers amongst several groups order maximize objective eg loans given repaid criminals apprehended often problems fairness also concern one natural notion fairness based general principles equality opportunity asks conditional individual candidate resource question probability actually receiving approximately independent individuals group example lending mean equally creditworthy individuals different racial groups roughly equal chances receiving loan policing mean two individuals committing crime different districts roughly equal chances arrested paper formalize general notion fairness allocation problems investigate algorithmic consequences main technical results include efficient learning algorithm converges optimal fair allocation even allocator know frequency candidates ie creditworthy individuals criminals group algorithm operates censored feedback model number candidates received resource given allocation can observed rather true number candidates group models fact learn creditworthiness individuals give loans learn crimes committed police presence district low application framework algorithm consider predictive policing problem resource allocated group number police officers assigned district learning algorithm trained arrest data gathered deployments previous days resulting potential feedback loop algorithm provably overcomes case fairness constraint asks probability individual committed crime arrested independent district live investigate performance learning algorithm philadelphia crime incidents dataset "
239,"fair allocation competitive equilibrium generic incomes"," two food banks catering populations different sizes different needs must divide among donation food items constitutes fair allocation items among competitive equilibrium equal incomes ceei classic solution problem fair efficient allocation goods among equally entitled agents foley varian every agent foodbank receives equal endowment artificial currency purchase bundles goods food items prices goods set high enough agents can simultaneously get favorite withinbudget bundle low enough goods allocated waste ceei satisfies mathematical notions fairness like fairshare also builtin transparency prices can published agents can verify theyre treated equally however ceei guaranteed exist items indivisible study competitive equilibrium generic incomes cegi based idea slightly perturbed endowments enjoys similar fairness efficiency transparency properties ceei show two agents almost equal endowments additive preferences items cegi always exists consider agents priori nonequal like differentsized foodbanks formulate new notion fair allocation among nonequals satisfied cegi show existence cases interest like agents identical preferences experiments simulated spliddit data popular fair division website indicate general existence results open opportunities future research fairness generic endowments fair treatment nonequals "
240," moral framework understanding fair ml economic models equality opportunity"," map recently proposed notions algorithmic fairness economic models equality opportunity eopan extensively studied ideal fairness political philosophy formally show conceptual mapping many existing definition algorithmic fairness predictive value parity equality odds can interpreted special cases eop respect work serves unifying moral framework understanding existing notions algorithmic fairness importantly framework allows us explicitly spell moral assumptions underlying notion fairness interpret recent fairness impossibility results new light last least inspired luck egalitarian models eop propose new family measures algorithmic fairness illustrate proposal empirically show employing measure algorithmic unfairness underlying moral assumptions satisfied can devastating consequences disadvantaged groups welfare "
241,"beyond open vs closed balancing individual privacy public accountability data sharing"," data sensitive open analysis repurposing typically remains closed proprietary information dichotomy undermines efforts make algorithmic systems fair transparent accountable access proprietary data particular needed government agencies enforce policy researchers evaluate methods public hold agencies accountable needs must met preserving individual privacy firm competitiveness paper describe integrated legaltechnical approach provided thirdparty publicprivate data trust designed balance competing interests basic membership allows firms agencies enable lowrisk access data compliance reporting core methods research modular data sharing agreements support wide array projects use cases unless specifically stated otherwise agreement data access initially provided end users customized synthetic datasets offer strong privacy guarantees b removal signals expose competitive advantage c removal biases reinforce discriminatory policies maintaining fidelity original data find using synthetic data conjunction strong legal protections raw data strikes balance transparency proprietorship privacy research objectives legaltechnical framework can form basis data trusts variety contexts "
242,"whos guinea pig investigating online abn tests inthewild"," abn testing adopted many technology companies datadriven approach product design optimization tests often run websites without explicit consent users paper investigate online abn tests using optimizely lens first provide measurement results websites use optimizely drawn alexa topm analyze distributions audiences experiments use three case studies discuss potential ethical pitfalls experiments including involvement political content price discrimination advertising campaigns conclude suggestion greater awareness ethical concerns inherent human experimentation call increased transparency among abn test operators "
243,"fairnessaware programming"," increasingly programming tasks involve automating deploying sensitive decisionmaking processes may adverse impacts individuals groups people issue fairness automated decisionmaking thus become major problem attracting interdisciplinary attention work aim make fairness firstclass concern programming specifically propose fairnessaware programming programmers can state fairness expectations natively code runtime system monitor decisionmaking report violations fairness present rich general specification language allows programmer specify range fairness definitions literature well others decisionmaking program executes runtime maintains statistics decisions made incrementally checks whether fairness definitions violated reporting violations developer advantages approach two fold enabling declarative mathematical specifications fairness programming language simplifies process checking fairness programmer write ad hoc code maintaining statistics ii compared existing techniques checking ensuring fairness approach monitors decisionmaking program wild may running distribution unlike dataset classifier trained tested describe implementation proposed methodology library python programming language illustrate use case studies algorithmic fairness literature "
244,"model cards model reporting"," trained machine learning models increasingly used perform highimpact tasks areas law enforcement medicine education employment order clarify intended use cases machine learning models minimize usage contexts well suited recommend released models accompanied documentation detailing performance characteristics paper propose framework call model cards encourage transparent model reporting model cards short documents accompanying trained machine learning models provide benchmarked evaluation variety conditions across different cultural demographic phenotypic groups eg race geographic location sex fitzpatrick skin type intersectional groups eg age race sex fitzpatrick skin type relevant intended application domains model cards also disclose context models intended used details performance evaluation procedures relevant information focus primarily humancentered machine learning models application fields computer vision natural language processing framework can used document trained machine learning model solidify concept provide cards two supervised models one trained detect smiling faces images one trained detect toxic comments text propose model cards step towards responsible democratization machine learning related artificial intelligence technology increasing transparency well artificial intelligence technology works hope work encourages releasing trained machine learning models accompany model releases similar detailed evaluation numbers relevant documentation "
245," social cost strategic classification"," consequential decisionmaking typically incentivizes individuals behave strategically tailoring behavior specifics decision rule long line work therefore sought counteract strategic behavior designing conservative decision boundaries effort increase robustness effects strategic covariate shift show efforts benefit institutional decision maker expense individuals classified introducing notion social burden prove increase institutional utility necessarily leads corresponding increase social burden moreover show negative externalities strategic classification can disproportionately harm disadvantaged groups population results highlight strategyrobustness must weighed considerations social welfare fairness "
246,"downstream effects affirmative action"," study twostage model students admitted college basis entrance exam noisy signal qualifications type students admitted college can hired employer function college grades independently drawn noisy signal type students drawn one two populations might different type distributions assume employer end pipeline rational sense computes posterior distribution student type conditional information available college admissions grades group membership makes decision based posterior expectation study kinds fairness goals can achieved college setting admissions rule grading policy example college might goal guaranteeing equal opportunity across populations probability passing pipeline hired employer independent group membership conditioned type alternately college might goal incentivizing employer group blind hiring rule show goals can achieved college report grades hand show reasonable conditions goals impossible achieve even isolation college uses even minimally informative grading policy "
247,"access populationlevel signaling source inequality"," identify explore differential access populationlevel signaling also known information design source unequal access opportunity populationlevel signaler potentially noisy observations binary type member population based produces signal member decisionmaker infers types signals accepts individuals whose type high expectation assume signaler disadvantaged population reveals observations decisionmaker whereas signaler advantaged population forms signals strategically study expected utility populations measured fraction accepted members well false positive rates fpr false negative rates fnr first show intuitive results fixed environment advantaged population higher expected utility higher fpr lower fnr disadvantaged one despite identical population quality accurate observations improve expected utility advantaged population harming disadvantaged one next explore introduction publiclyobservable signal test score potential intervention main finding natural intervention intended reduce inequality populations utilities may actually exacerbate settings observations test scores noisy "
248," disparate effects strategic manipulation"," consequential decisions informed algorithmic input individuals may feel compelled alter behavior order gain systems approval models agent responsiveness termed strategic manipulation analyze interaction learner agents world agents equally able manipulate features attempt trick published classifier cases real world classification however agents ability adapt algorithm simply function personal interest receiving positive classification bound complex web social factors affect ability pursue certain action responses paper adapt models strategic manipulation capture dynamics may arise setting social inequality wherein candidate groups face different costs manipulation find whenever one groups costs higher others learners equilibrium strategy exhibits inequalityreinforcing phenomenon wherein learner erroneously admits members advantaged group erroneously excluding members disadvantaged group also consider effects interventions learner subsidizes members disadvantaged group lowering costs order improve classification performance encounter paradoxical result exist cases providing subsidy improves learners utility actually making candidate groups worseoffeven group receiving subsidy results reveal potentially adverse social ramifications deploying tools attempt evaluate individuals quality agents capacities adaptively respond differ "
249," account accounting algorithms systematic literature review algorithmic accountability"," research algorithms impact proliferates calls scrutinyaccountability algorithms systematic review work done field algorithmic accountability far lacking contribution puts forth systematic review following prisma statement english articles period including collected extracted web science scopus using recursive query design coupled computational methods articles prioritized ordered using affinity mapping resulting core articles presented contribution recursive search strategy made possible look beyond term algorithmic accountability query also included terms closely connected theme eg ethics ai regulation algorithms approach allows perspective just critical algorithm studies interdisciplinary overview drawing material data studies law computer science governance studies structure material bovenss widely accepted definition accountability serves focal point material analyzed five points bovens identified integral accountability arguments actor forum relationship two content criteria account finally consequences may result account review makes three contributions first integration accountability theory algorithmic accountability discussion second crosssectoral overview discussion viewed light accountability theory pays extra attention accountability risks algorithmic systems lastly provides definition algorithmic accountability based accountability theory algorithmic accountability literature "
250,"algorithmic realism expanding boundaries algorithmic thought"," although computer scientists eager help address social problems field faces growing awareness many wellintentioned applications algorithms social contexts led significant harm argue addressing gap fields desire good harmful impacts many interventions requires looking epistemic methodological underpinnings algorithms diagnose dominant mode algorithmic reasoning algorithmic formalism describe formalist orientations lead harmful algorithmic interventions addressing harms requires pursuing new mode algorithmic thinking attentive internal limits algorithms social concerns fall beyond bounds algorithmic formalism understand methodological evolution beyond formalism looks like may achieve turn twentieth century evolution american legal thought legal formalism legal realism drawing lessons legal realism propose new mode algorithmic thinkingalgorithmic realismthat provides tools computer scientists account realities social life algorithmic impacts realist approaches although foolproof will better equip computer scientists reduce algorithmic harms reason well good "
251,"algorithmic accountability public administration gdpr paradox"," eu general data protection regulation gdpr often represented larger life behemoth will fundamentally transform world big data abstracted constituent parts corresponding rights responsibilities exemptions operative scope gdpr can unduly aggrandized reality caters specific policy objectives legislators institutional stakeholders much uncertainty ahead precise implementation gdpr academic policy discussions debating adequacy protections automated decisionmaking gdpr articles right informed automated treatment right access data subject safeguards profiling unfortunately literature date disproportionately focuses impact ai private sector deflects extensive review automated enforcement tools public administration even though gdpr enacts significant safeguards automated decisions deliberate design balance interests data protection growing demand algorithms administrative state order facilitate interagency data flows sensitive data processing fuel predictive power algorithmic enforcement tools gdpr decisively surrenders procedural autonomy member states authorize practices yet due dearth research gdprs stance government deployed algorithms widely known public authorities can benefit broadly worded exemptions restrictions automated decisionmaking even circumvent remedies data subjects national legislation potential public authorities invoke derogations gdpr must contained fundamental guarantees due process judicial review equal treatment paper examines interplay principles within prospect algorithmic decisionmaking public authorities "
252,"closing ai accountability gap defining endtoend framework internal algorithmic auditing"," rising concern societal implications artificial intelligence systems inspired wave academic journalistic literature deployed systems audited harm investigators outside organizations deploying algorithms however remains challenging practitioners identify harmful repercussions systems prior deployment deployed emergent issues can become difficult impossible trace back source paper introduce framework algorithmic auditing supports artificial intelligence system development endtoend applied throughout internal organization development lifecycle stage audit yields set documents together form overall audit report drawing organizations values principles assess fit decisions made throughout process proposed auditing framework intended contribute closing accountability gap development deployment largescale artificial intelligence systems embedding robust process ensure audit integrity "
253,"toward situated interventions algorithmic equity lessons field"," research date aimed fairness accountability transparency algorithmic systems largely focused topics identifying failures current systems technical interventions intended reduce bias computational processes researchers given less attention methods account social political contexts specific situated technical systems points use codeveloping algorithmic accountability interventions communities supports outcomes likely address problems situated context recenter power disparately affected harms algorithmic systems paper report experiences using participatory codesign methods algorithmic accountability project called algorithmic equity toolkit main insights gleaned experiences many meaningful interventions toward equitable algorithmic systems nontechnical ii community organizations derive value localized materials opposed scalable beyond particular policy context iii framing harms around algorithmic bias suggests accurate data solution risk missing deeper questions whether technologies used broadly found communitybased methods important inroads addressing algorithmic harms situated contexts "
254,"explainability fact sheets framework systematic assessment explainable approaches"," explanations machine learning come many forms consensus regarding desired properties yet emerge paper introduce taxonomy set descriptors can used characterise systematically assess explainable systems along five key dimensions functional operational usability safety validation order design comprehensive representative taxonomy associated descriptors surveyed explainable artificial intelligence literature extracting criteria desiderata authors proposed implicitly used research survey includes papers introducing new explainability algorithms see criteria used guide development algorithms evaluated well papers proposing criteria computer science social science perspectives novel framework allows systematically compare contrast explainability approaches just better understand capabilities also identify discrepancies theoretical qualities properties implementations developed operationalisation framework form explainability fact sheets enable researchers practitioners alike quickly grasp capabilities limitations particular explainable method used work sheet taxonomy can guide development new explainability approaches aiding critical evaluation along five proposed dimensions "
255,"multilayered explanations algorithmic impact assessments gdpr"," impact assessments received particular attention sides atlantic tool implementing algorithmic accountability aim paper address data protection impact assessments dpias art european union eus general data protection regulation gdpr link gdprs two approaches algorithmic accountabilityindividual rights systemic governance potentially lead accountable explainable algorithms argue algorithmic explanation understood static statement circular multilayered transparency process based several layers general information algorithm groupbased explanations legal justification individual decisions taken argue impact assessment process plays crucial role connecting internal company heuristics risk mitigation outwardfacing rights forming substance several kinds explanations "
256," hidden assumptions behind counterfactual explanations principal reasons"," counterfactual explanations gaining prominence within technical legal business circles way explain decisions machine learning model explanations share trait longestablished principal reason explanations required us credit laws explain decision highlighting set features deemed relevantand withholding others featurehighlighting explanations several desirable properties place constraints model complexity require model disclosure detail needed different achieve different decision seem automate compliance law far complex subjective appear paper demonstrate utility featurehighlighting explanations relies number easily overlooked assumptions recommended change feature values clearly maps realworld actions features can made commensurate looking distribution training data features relevant decision hand underlying model stable time monotonic limited binary outcomes explore several consequences acknowledging attempting address assumptions including paradox way featurehighlighting explanations aim respect autonomy unchecked power featurehighlighting explanations grant decision makers tension making explanations useful need keep model hidden new research suggests several ways featurehighlighting explanations can work around problems identify disconnect features model actions real worldand subjective choices necessary compensate thismust understood techniques can usefully implemented "
257," model fail contrastive local explanations retail forecasting"," various business settings interest using complex machine learning techniques sales forecasting difficult convince analysts along superiors adopt techniques since models considered black boxes even perform better current models use examine impact contrastive explanations large errors users attitudes towards blackbox model propose algorithm monte carlo bounds reasonable predictions given large error mcbrp determines feature values result reasonable prediction general trends feature target based monte carlo simulations evaluate real dataset real users conducting user study participants determine explanations generated mcbrp help users understand prediction results large error promotes trust automaticallylearned model study shows users able answer objective questions models predictions overall accuracy provided contrastive explanations show users saw mcbrp explanations understand model makes large errors predictions significantly users control group also conduct indepth analysis difference attitudes practitioners researchers confirm results hold conditioning users background "
258," human body black box supporting clinical decisionmaking deep learning"," machine learning technologies increasingly developed use healthcare research communities focused creating stateoftheart models less focus real world implementation associated challenges fairness transparency accountability come actual situated use serious questions remain underexamined regarding ethically build models interpret explain model output recognize account biases minimize disruptions professional expertise work cultures address gap literature provide detailed case study covering development implementation evaluation sepsis watch machine learningdriven tool assists hospital clinicians early diagnosis treatment sepsis sepsis severe infection can lead organ failure death treated time leading cause inpatient deaths us hospitals team developed evaluated tool discuss conceptualization tool model deployed world instead sociotechnical system requiring integration existing social professional contexts rather focusing solely model interpretability ensure fair accountable machine learning point toward four key values practices considered developing machine learning support clinical decisionmaking rigorously define problem context build relationships stakeholders respect professional discretion create ongoing feedback loops stakeholders work significant implications future research regarding mechanisms institutional accountability considerations responsibly designing machine learning systems work underscores limits model interpretability solution ensure transparency accuracy accountability practice instead work demonstrates means goals achieve fatml values design practice "
259,"assessing algorithmic fairness unobserved protected class using data combination"," increasing impact algorithmic decisions peoples lives compels us scrutinize fairness particular disparate impacts ostensiblycolorblind algorithms can different groups examples include credit decisioning hiring advertising criminal justice personalized medicine targeted policymaking cases legislative regulatory frameworks fairness exist define specific protected classes paper study fundamental challenge assessing disparate impacts performance disparities general practice protected class membership often observed data particularly problem lending healthcare consider use auxiliary dataset us census includes protected class labels decisions outcomes show variety common disparity measures generally unidentifiable aside unrealistic cases providing new perspective documented biases popular proxybased methods provide exact characterizations sharpestpossible partial identification set disparities either assumptions incorporate mild smoothness constraints provide optimizationbased algorithms computing visualizing sets simultaneously achievable pairwise disparties assessing disparities arise multiple groups enables reliable robust assessments important tool disparity assessment can farreaching policy implications demonstrate two case studies real data mortgage lending personalized medicine dosing "
260,"fliptest fairness testing via optimal transport"," present fliptest blackbox technique uncovering discrimination classifiers fliptest motivated intuitive question individual different protected status model treated differently rather relying causal information answer question fliptest leverages optimal transport match individuals different protected groups creating similar pairs indistribution samples show use instances detect discrimination constructing flipset set individuals whose classifier output changes posttranslation corresponds set people may harmed group membership shed light model treats given subgroup differently fliptest produces transparency report ranking features associated models behavior flipset evaluating approach three case studies show provides computationally inexpensive way identify subgroups may harmed model discrimination including cases model satisfies group fairness criteria "
261,"implications ai unfairness higher education admissions effects perceived ai unfairness exit voice organizational reputation"," algorithmic decisionmaking adm becoming increasingly important areas social life higher education machinelearning systems manifold uses can efficiently process large amounts student data use data arrive effective decisions despite potential upsides adm systems fairness concerns gaining momentum academic public discourses criticism largely focuses disparate effects adm algorithms may serve objective fair decisionmakers rather reproduce biases existing within respective training data study adopted different approach focusing individual perceptions fairness specifically looked two different dimensions perceived fairness procedural fairness ii distributive fairness using crosssectional survey data n large german university tested whether students assessments fairness differ respect algorithmic vs human decisionmaking hdm within higher education context furthermore investigated whether fairness perceptions subsequent effects three different outcome variables hugely important universities exit voice organizational reputation results survey suggest participants evaluated adm higher hdm terms procedural distributive fairness concerning subsequent effects fairness perceptions find distributive fairness well procedural fairness perceptions negative impact intention protest adm system whereas procedural fairness perceptions negatively affect likelihood exiting finally distributive fairness procedural fairness perceptions positive effect organizational reputation universities aiming implement adm systems crucial therefore take possible fairness issues implications account "
262,"auditing radicalization pathways youtube"," nonprofits well media hypothesized existence radicalization pipeline youtube claiming users systematically progress towards extreme content platform yet date substantial quantitative evidence alleged pipeline close gap conduct largescale audit user radicalization youtube analyze videos posted channels broadly classified four types media altlite intellectual dark web idw altright according aforementioned radicalization hypothesis channels idw altlite serve gateways fringe farright ideology represented altright channels processing m comments show three channel types indeed increasingly share user base users consistently migrate milder extreme content large percentage users consume altright content now consumed altlite idw content past also probe youtubes recommendation algorithm looking m video channel recommendations mayjuly find altlite content easily reachable idw channels altright videos reachable channel recommendations overall paint comprehensive picture user radicalization youtube "
263,"case study predictive fairness reduce misdemeanor recidivism social service interventions"," criminal justice system currently illequipped improve outcomes individuals cycle system series misdemeanor offenses often due constraints caseload poor record linkage prior interactions individual may considered individual comes back system let alone proactive manner application diversion programs los angeles city attorneys office recently created new recidivism reduction drug diversion unit rd tasked reducing recidivism population describe collaboration new unit case study incorporation predictive equity machine learning based decision making resourceconstrained setting program seeks improve outcomes developing individuallytailored social service interventions ie diversions conditional plea agreements stayed sentencing favorable case disposition based appropriate social service linkage rather traditional sentencing methods individuals likely experience subsequent interactions criminal justice system time resourceintensive undertaking necessitates ability focus resources individuals likely involved future case seeking achieve efficiency predictive accuracy equity improving outcomes traditionally underserved communities working mitigate existing disparities criminal justice outcomes discuss equity outcomes seek achieve describe corresponding choice metric measuring predictive fairness context explore set options balancing equity efficiency building selecting machine learning models operational public policy setting "
264," concept fairness gdpr linguistic contextual interpretation"," growing attention notion fairness gdpr european legal literature however principle fairness data protection framework still ambiguous uncertain computer science literature interpretative guidelines reveal paper looks better understanding concept fairness data protection field two parallel methodological tools linguistic comparison contextual interpretation terms linguistic comparison paper analyses translations world fair gdpr eu official languages cjeu suggests cilfit case interpretation eu law analysis takes account also translation notion fairness contiguous fields eg article eu charter fundamental rights consumer field eg unfair terms directive unfair commercial practice directive general notion fairness translated several different nuances accordance discordance previous data protection directive article charter versions different words used interchangeably case french spanish portuguese texts versions seems specific rationale using different terms different parts gdpr case german greek version analysis reveals three mean semantic notions correctness italian swedish romanian loyalty french spanish portuguese german version treu und glaube equitability french spanish portuguese interestingly three notions common roots western legal history roman law notion bona fide taking account value bona fide current european legal contexts also contextual interpretation role fairness gdpr preliminary conclusions fairness refers substantial balancing interests among data controllers data subjects approach fairness effectbased relevant formal respect procedures terms transparency lawfulness accountability substantial mitigation unfair imbalances create situations vulnerability building reflections paper analyses notion fairness imbalance related idea vulnerability within beyond gdpr sum article suggests best interpretation fairness principles gdpr taking account notion procedural fairness fair balancing mitigation data subjects vulnerabilities specific safeguards measures "
265," concept fairness gdpr linguistic contextual interpretation"," growing attention notion fairness gdpr european legal literature however principle fairness data protection framework still ambiguous uncertain computer science literature interpretative guidelines reveal paper looks better understanding concept fairness data protection field two parallel methodological tools linguistic comparison contextual interpretation terms linguistic comparison paper analyses translations world fair gdpr eu official languages cjeu suggests cilfit case interpretation eu law analysis takes account also translation notion fairness contiguous fields eg article eu charter fundamental rights consumer field eg unfair terms directive unfair commercial practice directive general notion fairness translated several different nuances accordance discordance previous data protection directive article charter versions different words used interchangeably case french spanish portuguese texts versions seems specific rationale using different terms different parts gdpr case german greek version analysis reveals three mean semantic notions correctness italian swedish romanian loyalty french spanish portuguese german version treu und glaube equitability french spanish portuguese interestingly three notions common roots western legal history roman law notion bona fide taking account value bona fide current european legal contexts also contextual interpretation role fairness gdpr preliminary conclusions fairness refers substantial balancing interests among data controllers data subjects approach fairness effectbased relevant formal respect procedures terms transparency lawfulness accountability substantial mitigation unfair imbalances create situations vulnerability building reflections paper analyses notion fairness imbalance related idea vulnerability within beyond gdpr sum article suggests best interpretation fairness principles gdpr taking account notion procedural fairness fair balancing mitigation data subjects vulnerabilities specific safeguards measures "
266,"pots protective optimization technologies"," algorithmic fairness aims address economic moral social political impact digital systems populations solutions can applied service providers fairness frameworks part mapping problems narrow definition assuming service providers can trusted deploy countermeasures surprisingly decisions limit fairness frameworks ability capture variety harms caused systems characterize fairness limitations using concepts requirements engineering social sciences show focus algorithms inputs outputs misses harms arise systems interacting world focus bias discrimination omits broader harms populations environments relying service providers excludes scenarios cooperative intentionally adversarial propose protective optimization technologies pots pots provide means affected parties address negative impacts systems environment expanding avenues political contestation pots intervene outside system require service providers cooperate can serve correct shift expose harms systems impose populations environments illustrate potential limitations pots two case studies countering road congestion caused traffic beating applications recalibrating credit scoring loan applicants "
267,"fair decision making using privacyprotected data"," data collected individuals regularly used make decisions impact individuals consider settings sensitive personal data used decide will receive resources benefits well known tradeoff protecting privacy accuracy decisions initiate firstofitskind study impact formally private mechanisms based differential privacy fair equitable decisionmaking empirically investigate novel tradeoffs two realworld decisions made using us census data allocation federal funds assignment voting rights benefits well classic apportionment problem results show decisions made using ∈differentially private version data strict privacy constraints smaller ∈ noise added achieve privacy may disproportionately impact groups others propose novel measures fairness context randomized differentially private algorithms identify range causes outcome disparities also explore improved algorithms remedy unfairness observed "
268,"fairness warnings fairmaml learning fairly minimal data"," motivated concerns surrounding fairness effects sharing transferring fair machine learning tools propose two algorithms fairness warnings fairmaml first modelagnostic algorithm provides interpretable boundary conditions fairly trained model may behave fairly similar slightly different tasks within given domain second fair metalearning approach train models can quickly finetuned specific tasks number sample instances balancing fairness accuracy demonstrate experimentally individual utility model using relevant baselines provide first experiment knowledge kshot fairness ie training fair model new task k data points illustrate usefulness algorithms combined method training models data points new tasks using fairness warnings interpretable boundary conditions newly trained model may fair "
269,"fairness warnings fairmaml learning fairly minimal data"," motivated concerns surrounding fairness effects sharing transferring fair machine learning tools propose two algorithms fairness warnings fairmaml first modelagnostic algorithm provides interpretable boundary conditions fairly trained model may behave fairly similar slightly different tasks within given domain second fair metalearning approach train models can quickly finetuned specific tasks number sample instances balancing fairness accuracy demonstrate experimentally individual utility model using relevant baselines provide first experiment knowledge kshot fairness ie training fair model new task k data points illustrate usefulness algorithms combined method training models data points new tasks using fairness warnings interpretable boundary conditions newly trained model may fair "
270,"fairness warnings fairmaml learning fairly minimal data"," motivated concerns surrounding fairness effects sharing transferring fair machine learning tools propose two algorithms fairness warnings fairmaml first modelagnostic algorithm provides interpretable boundary conditions fairly trained model may behave fairly similar slightly different tasks within given domain second fair metalearning approach train models can quickly finetuned specific tasks number sample instances balancing fairness accuracy demonstrate experimentally individual utility model using relevant baselines provide first experiment knowledge kshot fairness ie training fair model new task k data points illustrate usefulness algorithms combined method training models data points new tasks using fairness warnings interpretable boundary conditions newly trained model may fair "
271,"whose side ethics codes power responsibility social good"," moral authority ethics codes stems assumption serve unified society yet ignores political aspects shared resource sociologist howard s becker challenged researchers clarify power responsibility classic essay whose side building beckers hierarchy credibility report critical discourse analysis data ethics codes emerging conceptualizations beneficence social good data technology analysis revealed ethics codes corporations professional associations conflated consumers society largely silent agency interviews community organizers social change digital era supplement analysis surfacing limits technical solutions concerns marginalized communities given evidence highlights gulf documents lived experiences argue ethics codes elevate consumers may simultaneously subordinate needs vulnerable populations understanding contested digital resources central emerging field public interest technology introduce concept digital differential vulnerability explain disproportionate exposures harm within data technology suggest recommendations future ethics codes "
272,"algorithmic targeting social policies fairness accuracy distributed governance"," targeted social policies main strategy poverty alleviation across developing world include targeted cash transfers cts well targeted subsidies health education housing energy childcare others due scale diversity widespread relevance targeted social policies like cts algorithmic rules decide eligible benefit themand notare among important algorithms operating world today report yearlong engagement towards improving social targeting systems couple developing countries demonstrate shift towards use ai methods povertybased targeting can substantially increase accuracy extending coverage poor nearly million people two countries without increasing expenditure however also show absent explicit parity constraints status quo aibased systems induce disparities across population subgroups moreover based qualitative interviews local social institutions find lack consensus normative standards prioritization fairness criteria hence close proposing decisionsupport platform distributed governance enables diversity institutions customize use aibased insights targeting decisions "
273,"roles computing social change"," recent normative turn computer science brought concerns fairness bias accountability core field yet recent scholarship warned much technical work treats problematic features status quo fixed fails address deeper patterns injustice inequality acknowledging critiques posit computational research valuable roles play addressing social problems roles whose value can recognized even perspective aspires toward fundamental social change paper articulate four roles analysis considers opportunities well significant risks inherent work computing research can serve diagnostic helping us understand measure social problems precision clarity formalizer computing shapes social problems explicitly defined changing problems possible responses understood computing serves rebuttal illuminates boundaries possible technical means computing acts synecdoche makes longstanding social problems newly salient public eye offer paths forward modalities leverage particular strengths computational work service social change without overclaiming computings capacity solve social problems "
274,"roles computing social change"," recent normative turn computer science brought concerns fairness bias accountability core field yet recent scholarship warned much technical work treats problematic features status quo fixed fails address deeper patterns injustice inequality acknowledging critiques posit computational research valuable roles play addressing social problems roles whose value can recognized even perspective aspires toward fundamental social change paper articulate four roles analysis considers opportunities well significant risks inherent work computing research can serve diagnostic helping us understand measure social problems precision clarity formalizer computing shapes social problems explicitly defined changing problems possible responses understood computing serves rebuttal illuminates boundaries possible technical means computing acts synecdoche makes longstanding social problems newly salient public eye offer paths forward modalities leverage particular strengths computational work service social change without overclaiming computings capacity solve social problems "
275," relationship trust ai trustworthy machine learning technologies"," design develop aibased systems users larger public can justifiably trust one needs understand machine learning technologies impact trust guide design implementation trusted aibased systems paper provides systematic approach relate considerations trust social sciences trustworthiness technologies proposed aibased services products start abi ability benevolence integrity predictability framework augmented recently proposed mapping abi qualities technologies support trust consider four categories trustworthiness technologies machine learning namely fairness explainability auditability safety feas discuss support required qualities moreover trust can impacted throughout life cycle aibased systems therefore introduce concept chain trust discuss trustworthiness technologies stages life cycle establish ways machine learning technologies support trusted aibased systems finally feas obvious relations known frameworks therefore relate feas variety international principled ai policy technology frameworks emerged recent years "
276," philosophical basis algorithmic recourse"," philosophers established certain ethically important values modally robust sense systematically deliver correlative benefits across range counterfactual scenarios paper contend recourse systematic process reversing unfavorable decisions algorithms bureaucracies across range counterfactual scenarios modally robust good particular argue two essential components good life temporally extended agency trust underwritten recourse critique existing approaches conceptualization operationalization implementation recourse based criticisms suggest revised approach recourse give examples might implemented especially least well off "
277," philosophical basis algorithmic recourse"," philosophers established certain ethically important values modally robust sense systematically deliver correlative benefits across range counterfactual scenarios paper contend recourse systematic process reversing unfavorable decisions algorithms bureaucracies across range counterfactual scenarios modally robust good particular argue two essential components good life temporally extended agency trust underwritten recourse critique existing approaches conceptualization operationalization implementation recourse based criticisms suggest revised approach recourse give examples might implemented especially least well off "
278,"effect confidence explanation accuracy trust calibration aiassisted decision making"," today ai increasingly used help human experts make decisions highstakes scenarios scenarios full automation often undesirable due significance outcome also human experts can draw domain knowledge complementary models ensure task success refer scenarios aiassisted decision making individual strengths human ai come together optimize joint decision outcome key success appropriately calibrate human trust ai casebycase basis knowing trust distrust ai allows human expert appropriately apply knowledge improving decision outcomes cases model likely perform poorly research conducts case study aiassisted decision making humans ai comparable performance alone explores whether features reveal casespecific model information can calibrate trust improve joint performance human ai specifically study effect showing confidence score local explanation particular prediction two human experiments show confidence score can help calibrate peoples trust ai model trust calibration alone sufficient improve aiassisted decision making may also depend whether human can bring enough unique knowledge complement ais errors also highlight problems using local explanation aiassisted decision making scenarios invite research community explore new approaches explainability calibrating human trust ai "
279,"leaveoneout unfairness"," introduce leaveoneout unfairness characterizes likely models prediction individual will change due inclusion removal single person models training data leaveoneout unfairness appeals idea fair decisions arbitrary based chance event one persons inclusion training data leaveoneout unfairness closely related algorithmic stability focuses consistency individual points prediction outcome unit changes training data rather error model aggregate beyond formalizing leaveoneout unfairness characterize extent deep models behave leaveoneout unfairly real data including cases generalization error small demonstrate adversarial training randomized smoothing techniques opposite effects leaveoneout fairness sheds light relationships robustness memorization individual fairness leaveoneout fairness deep models finally discuss salient practical applications may negatively affected leaveoneout unfairness "
280,"fairness welfare equity personalized pricing"," study interplay fairness welfare equity considerations personalized pricing based customer features sellers increasingly able conduct price personalization based predictive modeling demand conditional covariates setting customized interest rates targeted discounts consumer goods personalized subsidies scarce resources positive externalities like vaccines bed nets different application areas may lead different concerns around fairness welfare equity different objectives price burdens consumers price envy firm revenue access good equal access distributional consequences good question impacts downstream outcomes interest conduct comprehensive literature review order disentangle different normative considerations propose taxonomy different objectives mathematical definitions focus observational metrics assume access underlying valuation distribution either unobserved due binary feedback illdefined due overriding behavioral concerns regarding interpreting revealed preferences setting personalized pricing provision goods positive benefits discuss price optimization may provide unambiguous benefit achieving triple bottom line personalized pricing enables expanding access turn may lead gains welfare due heterogeneous utility improve revenue budget utilization empirically demonstrate potential benefits personalized pricing two settings pricing subsidies elective vaccine effects personalized interest rates downstream outcomes microcredit "
281,"reimagining algorithmic fairness india beyond"," conventional algorithmic fairness westcentric seen subgroups values methods paper decenter algorithmic fairness analyse ai power india based qualitative interviews discourse analysis algorithmic deployments india find several assumptions algorithmic fairness challenged find india data always reliable due socioeconomic factors ml makers appear follow double standards ai evokes unquestioning aspiration contend localising model fairness alone can window dressing india distance models oppressed communities large instead reimagine algorithmic fairness india provide roadmap recontextualise data models empower oppressed communities enable fairml ecosystems "
282,"narratives counternarratives data sharing africa"," machine learning data science applications grow ever prevalent increased focus data sharing open data initiatives particularly context african continent many argue data sharing can support research policy design alleviate poverty inequality derivative effects africa despite fact datasets question often extracted african communities conversations around challenges accessing sharing african data often driven nonafrican stakeholders perspectives frequently employ deficit narratives often focusing lack education training technological resources continent leading causes friction data ecosystem argue narratives obfuscate distort full complexity african data sharing landscape particular use storytelling via fictional personas built series interviews african data experts complicate dominant narratives provide counternarratives coupling personas research data practices within continent identify recurring barriers data sharing well inequities distribution data sharing benefits particular discuss issues arising power imbalances resulting legacies colonialism ethnocentrism slavery disinvestment building trust lack acknowledgement historical presentday extractive practices westerncentric policies illsuited african context outlining problems discuss avenues addressing sharing data generated continent "
283," whole thing smacks gender algorithmic exclusion bioimpedancebased body composition analysis"," smart weight scales offer bioimpedancebased body composition analysis supplement pure body weight measurement companies withings fitbit tout composition analysis providing selfknowledge ability make informed decisions however aspirational statements elide reality numbers product proprietary regression equations require binary sexgender input paper combines transgender studiesinfluenced personal narrative analysis scientific basis bioimpedance technology used part withings smart scale attempting include nonbinary people reveals bioelectrical impedance analysis always rested physiologically shaky ground white nonbinary people merely tip iceberg may find smart scale intelligent comes bodies using body composition analysis example explore problem trans nonbinary inclusion personal health tech goes beyond issues adding third gender box slapping rainbow flag packaging also provide recommendations approach creating inclusive technologies even still relying exclusionary data "
284,"algorithmic recourse counterfactual explanations interventions"," machine learning increasingly used inform consequential decisionmaking eg pretrial bail loan approval becomes important explain system arrived decision also suggest actions achieve favorable decision counterfactual explanations world different desirable outcome occur aim satisfy criteria existing works primarily focused designing algorithms obtain counterfactual explanations wide range settings however largely overlooked ultimately one main objectives allow people act rather just understand laymans terms counterfactual explanations inform individual need get get work rely causal reasoning caution use counterfactual explanations recommendable set actions recourse instead propose shift paradigm recourse via nearest counterfactual explanations recourse minimal interventions shifting focus explanations interventions "
285," semioticsbased epistemic tool reason ethical issues digital technology design development"," one important challenges regarding development morally responsible ethically qualified digital technologies support designers developers producing technologies especially conceptualizing vision technology will will benefit users avoid harm however traditional software design development life cycles explicitly support reflection upon either ethical moral issues paper look number ethical issues may dealt digital technology design development prevent damage improve technological fairness accountability transparency starting mature work semiotic theory methods humancomputer interaction propose extend core artifact used semiotic engineering humancentered technology design directly address moral responsibility ethical issues resulting extension epistemic tool instrument create elaborate specific kind knowledge paper describes tool illustrates used discusses promises limitations background related work also includes proposed empirical studies accompanied briefly described methodological challenges considerations deserve attention "
286,"measurement fairness"," propose measurement modeling quantitative social sciences framework understanding fairness computational systems computational systems often involve unobservable theoretical constructs socioeconomic status teacher effectiveness risk recidivism constructs measured directly must instead inferred measurements observable properties unobservable theoretical constructs thought related themie operationalized via measurement model process necessarily involves making assumptions introduces potential mismatches theoretical understanding construct purported measured operationalization argue many harms discussed literature fairness computational systems direct results mismatches show harms anticipated cases mitigated viewed lens measurement modeling contribute fairnessoriented conceptualizations construct reliability construct validity unite traditions political science education psychology provide set tools making explicit testing assumptions constructs operationalizations turn fairness essentially contested construct different theoretical understandings different contexts argue contestedness underlies recent debates fairness definitions although debates appear different operationalizations fact debates different theoretical understandings fairness show measurement modeling can provide framework getting core debates "
287,"fairness risk assessment instruments postprocessing achieve counterfactual equalized odds"," domains criminal justice medicine social welfare decision makers increasingly access algorithmic risk assessment instruments rais rais estimate risk adverse outcome recidivism child neglect potentially informing highstakes decisions whether release defendant bail initiate child welfare investigation important ensure rais fair benefits harms decisions equitably distributed widely used algorithmic fairness criteria formulated respect observable outcomes whether person actually recidivates criteria misleading applied rais since rais intended inform interventions can reduce risk prediction affects downstream outcome recent work argued fairness criteria rais instead utilize potential outcomes ie outcomes occur absence appropriate intervention however methods currently exist satisfy fairness criteria paper target one criterion counterfactual equalized odds develop postprocessed predictor estimated via doubly robust estimators extending adapting previous postprocessing approaches counterfactual setting also provide doubly robust estimators risk fairness properties arbitrary fixed postprocessed predictors predictor converges optimal fair predictor fast rates illustrate properties method show performs well simulated real data "
288,"high dimensional model explanations axiomatic approach"," complex blackbox machine learning models regularly used critical decisionmaking domains given rise several calls algorithmic explainability many explanation algorithms proposed literature assign importance feature individually however explanations fail capture joint effects sets features indeed works far formally analyze high dimensional model explanations paper propose novel high dimension model explanation method captures joint effect feature subsets propose new axiomatization generalization banzhaf index method can also thought approximation blackbox model higherorder polynomial words work justifies use generalized banzhaf index model explanation showing uniquely satisfies set natural desiderata optimal local approximation blackbox model empirical evaluation measure highlights manages capture desirable behavior whereas measures satisfy axioms behave unpredictable manner "
289," agentbased model evaluate interventions online dating platforms decrease racial homogamy"," perhaps controversial questions study online platforms today surround extent platforms can intervene reduce societal ills perpetrated debate whether exist effective lasting interventions platform can adopt address eg online bullying farreaching change necessary address problems empirical work critical addressing questions also challenging timeconsuming expensive sometimes limited questions companies willing ask help focus inform empirical work propose agentbased modeling abm approach application analyze impact set interventions simulated online dating platform lack longterm interracial relationships artificial society real world lack interracial relationships critical vehicle inequality maintained work shows many previously hypothesized interventions online dating platforms take increase number interracial relationships website limited effects effectiveness intervention subject assumptions sociocultural structure interventions effective increasing diversity longterm relationships odds platforms profitoriented goals general level present work shows value using abm approach help understand potential effects side effects different interventions platform take "
290,"designing accountable systems"," accountability often called property technical systems requirement algorithmic decision systems autonomous cyberphysical systems software systems general concept accountability goes back early history liberalism suggested tool limit use power long history also given us many often slightly differing definitions accountability problem software developers now face understand accountability means systems reflect systems design enable rigorous study accountability system need models suitable capturing varied concept paper present method express compare different definitions accountability using structural causal models show models can used evaluate systems design present small use case based autonomous car "
291,"socially fair kmeans clustering"," show popular kmeans clustering algorithm lloyds heuristic used variety scientific data can result outcomes unfavorable subgroups data eg demographic groups biased clusterings can deleterious implications humancentric applications resource allocation present fair kmeans objective algorithm choose cluster centers provide equitable costs different groups algorithm fairlloyd modification lloyds heuristic kmeans inheriting simplicity efficiency stability comparison standard lloyds find benchmark datasets fairlloyd exhibits unbiased performance ensuring groups equal costs output kclustering incurring negligible increase running time thus making viable fair option wherever kmeans currently used "
292,"towards crosslingual generalization translation gender bias"," crosslingual generalization issues less explored languages broadly tackled recent nlp studies study apply philosophy problem translation gender bias necessarily involves multilingualism sociocultural diversity beyond conventional evaluation criteria social bias aim put together various aspects linguistic viewpoints measuring process create template makes evaluation less tilted specific types language pairs manually constructed set content words template check accuracy gender inference fluency translation german korean portuguese tagalog inference accuracy disparate impact namely biasedness factors associated show failure bias mitigation threatens delicacy translation furthermore analyses system language indicate translation fluency inference accuracy necessarily correlated results implicitly suggest amount available language resources boost performance might amplify bias crosslinguistically "
293," pilot study surveying clinical judgments evaluate radiology report generation"," recent release many chest xray datasets prompted lot interest radiology report generation date framed image captioning task machine takes rgb image input generates sentence summary findings output quality reports canonically measured using metrics nlp community language generation machine translation summarization however evaluation metrics eg bleu cider inappropriate medical domain clinical correctness critical address team brought together machine learning experts radiologists pilot study codesigning better metric evaluating quality algorithmicallygenerated radiology report interdisciplinary collaborative process involved multiple interviews outreach preliminary annotation design larger scale study now underway build meaningful evaluation tool "
294,"fairness robustness investigating robustness disparity deep learning"," deep neural networks dnns increasingly used realworld applications eg facial recognition resulted concerns fairness decisions made models various notions measures fairness proposed ensure decisionmaking system disproportionately harm benefit particular subgroups population paper argue traditional notions fairness based models outputs sufficient model vulnerable adversarial attacks argue cases may easier attacker target particular subgroup resulting form robustness bias show measuring robustness bias challenging task dnns propose two methods measure form bias conduct empirical study stateoftheart neural networks commonly used realworld datasets cifar cifar adience utkface show almost cases subgroups cases based sensitive attributes like race gender etc less robust thus disadvantage argue kind bias arises due data distribution highly complex nature learned decision boundary case dnns thus making mitigation biases nontrivial task results show robustness bias important criterion consider auditing realworld systems rely dnns decision making code reproduce results can found httpsgithubcomnvedantfairnessthroughrobustness "
295,"operationalizing framing support multiperspective recommendations opinion pieces"," diversity personalized news recommender systems often defined dissimilarity operationalized based topic diversity eg corona versus farmers strike diversity news media however understood multiperspectivity eg different opinions corona measures arguably key responsibility press democratic society viewpoint diversity often considered synonymous source diversity communication science domain paper take computational view operationalize notion framing adopted communication science apply notion reranking topicrelevant recommended lists form basis novel viewpoint diversification method offline evaluation indicates proposed method capable enhancing viewpoint diversity recommendation lists according diversity metric literature online study blendle platform dutch news aggregator users found users willing consume viewpoint diverse news recommendations also found presentation characteristics significantly influence reading behaviour diverse recommendations results suggest future research presentation aspects recommendations can just important novel viewpoint diversification methods truly achieve multiperspectivity online news environments "
296,"bridging machine learning mechanism design towards algorithmic fairness"," decisionmaking systems increasingly orchestrate world intervene algorithmic components build fair equitable systems therefore question utmost importance one substantially complicated contextdependent nature fairness discrimination modern decisionmaking systems involve allocating resources information people eg school choice advertising incorporate machinelearned predictions pipelines raising concerns potential strategic behavior constrained allocation concerns usually tackled context mechanism design although machine learning mechanism design developed frameworks addressing issues fairness equity complex decisionmaking systems neither framework individually sufficient paper develop position building fair decisionmaking systems requires overcoming limitations argue inherent field ultimate objective build encompassing framework cohesively bridges individual frameworks mechanism design machine learning begin lay ground work towards goal comparing perspective discipline takes fair decisionmaking teasing lessons field taught can teach highlighting application domains require strong collaboration disciplines "
297,"fair clustering via equitable group representations"," mean clustering fair one popular approach seeks ensure cluster contains groups roughly proportion exist population normative principle play balance cluster might act representative data thus reflect diversity clustering also captures different form representativeness core principle clustering problems cluster center representative cluster represents close points associated can effectively replace points cluster centers without significant loss fidelity indeed common use case clustering clustering fair centers represent different groups equally well call clustering grouprepresentative clustering paper study structure computation grouprepresentative clusterings show notion naturally parallels development fairness notions classification direct analogs ideas like demographic parity equal opportunity demonstrate notions distinct captured balancebased notions fairness present approximation algorithms group representative kmedian clustering couple empirical evaluation various realworld data sets also extend idea facility location motivated current problem assigning polling locations voting "
298," cant sit us exclusionary pedagogy ai ethics education"," given growing concern lack ethical consideration artificial intelligence ai field many begun question dominant approaches disciplinary education computer science csand implications aihas led current ethics crisis however claim current ai ethics education space relies form exclusionary pedagogy ethics distilled computational approaches deeper epistemological engagement ways knowing benefit ethical thinking acknowledgement limitations univocal computational thinking results indifference devaluation lack mutual support cs humanistic social science hss elevating myth technologists ethical unicorns can though disciplinary tools ultimately limited analysis computer science education literature review collegelevel course syllabi ai ethics discuss limitations epistemological assumptions hierarchies knowledge dictate current attempts including ethics education cs training explore evidence practical mechanisms exclusion occurs propose shift towards substantively collaborative holistic ethically generative pedagogy ai education "
299,"fair classification groupdependent label noise"," work examines train fair classifiers settings training labels corrupted random noise error rates corruption depend label class membership function protected subgroup heterogeneous label noise models systematic biases towards particular groups generating annotations begin presenting analytical results show naively imposing parity constraints demographic disparity measures without accounting heterogeneous groupdependent error rates can decrease accuracy fairness resulting classifier experiments demonstrate issues arise practice well address problems performing empirical risk minimization carefully defined surrogate loss functions surrogate constraints help avoid pitfalls introduced heterogeneous label noise provide theoretical empirical justifications efficacy methods view results important example imposing fairness biased data sets without proper care can least much harm good "
300,"censorship online encyclopedias implications nlp models"," artificial intelligence provides backbone many tools people use around world recent work brought attention algorithms powering ai free politics stereotypes bias work area focused ways ai can exacerbate existing inequalities discrimination little work studied governments actively shape training data describe censorship affected development wikipedia corpuses text data regularly used pretrained inputs nlp algorithms show word embeddings trained baidu baike online chinese encyclopedia different associations adjectives range concepts democracy freedom collective action equality people historical events china regularly blocked uncensored counterpart chinese language wikipedia examine implications discrepancies studying use downstream ai applications paper shows government repression censorship selfcensorship may impact training data applications draw "
301,"impossible explanations beyond explainable ai gdpr covid use case scenario"," can achieve adequate level explanation complex machine learning models highrisk ai applications applying eu data protection framework article address question analysing multidisciplinary point view connection existing legal requirements explainability ai systems current state art field explainable ai present case study reallife scenario designed illustrate application aibased automated decision making process medical diagnosis covid patients scenario exemplifies trend usage increasingly complex machinelearning algorithms growing dimensionality data model parameters based setting analyse challenges providing human legible explanations practice discuss legal implications following general data protection regulation gdpr although might appear just one single form explanation gdpr conclude context decisionmaking system operates requires several forms explanation considered thus propose design explanations multiple forms depending moment disclosure explanation either ex ante ex post audience explanation explanation expert data controller explanation final data subject layer granularity general groupbased individual explanations level risks automated decision regarding fundamental rights freedoms consequently explanations embrace multifaceted environment furthermore highlight current inability complex deep learning based machine learning models make clear causal links input data final decisions represents limitation providing exact humanlegible reasons behind specific decisions makes provision satisfactorily fair transparent explanations serious challenge therefore cases quality possible explanations might assessed adequate safeguard automated decisionmaking processes article gdpr accordingly suggest research focus alternative tools gdpr algorithmic impact assessments article gdpr algorithmic lawfulness justifications might considered complement explanations automated decisionmaking "
302,"towards accountability machine learning datasets practices software engineering infrastructure"," datasets power machine learning often used shared reused little visibility processes deliberation led creation artificial intelligence systems increasingly used highstakes tasks system development deployment practices must adapted address real consequences model development data constructed used practice includes greater transparency data accountability decisions made developing paper introduce rigorous framework dataset development transparency supports decisionmaking accountability framework uses cyclical infrastructural engineering nature dataset development draw best practices software development lifecycle stage data development lifecycle yields documents facilitate improved communication decisionmaking well drawing attention value necessity careful data work proposed framework makes visible often overlooked work decisions go dataset creation critical step closing accountability gap artificial intelligence criticalnecessary resource aligned recent work auditing processes "
303,"fairness equality power algorithmic decisionmaking"," much debate impact algorithms concerned fairness defined absence discrimination individuals merit drawing theory justice argue leading notions fairness suffer three key limitations legitimize inequalities justified merit narrowly bracketed considering differences treatment within algorithm consider betweengroup withingroup differences contrast fairnessbased perspective two alternate perspectives first focuses inequality causal impact algorithms second distribution power formalize perspectives drawing techniques causal inference empirical economics characterize give divergent evaluations present theoretical results empirical examples demonstrate tension use insights present guide algorithmic auditing discuss importance inequality powercentered frameworks algorithmic decisionmaking "
304,"one label one billion faces usage consistency racial categories computer vision"," computer vision widely deployed highly visible societyaltering applications documented problems bias representation datasets critical benchmarking progress fair computer vision often employ broad racial categories population groups measuring group fairness similarly diversity often measured computer vision datasets ascribing counting categorical race labels however racial categories illdefined unstable temporally geographically problematic history scientific use although racial categories used across datasets superficially similar complexity human race perception suggests racial system encoded one dataset may substantially inconsistent another using insight classifier can learn racial system encoded dataset conduct empirical study computer vision datasets supplying categorical race labels face images determine crossdataset consistency generalization racial categories find dataset encodes substantially unique racial system despite nominally equivalent racial categories racial categories systemically less consistent others across datasets find evidence racial categories encode stereotypes exclude ethnic groups categories basis nonconformity stereotypes representing billion humans one racial category may obscure disparities create new ones encoding stereotypes racial systems difficulty adequately converting abstract concept race tool measuring fairness underscores need method flexible culturally aware racial categories "
305,"reviewable automated decisionmaking framework accountable algorithmic systems"," paper introduces reviewability framework improving accountability automated algorithmic decisionmaking adm involving machine learning draw understanding adm sociotechnical process involving human technical elements beginning decision made extending beyond decision explanations modelcentric mechanisms may assist accountability concerns often provide insufficient information broader adm processes regulatory oversight assessments legal compliance reviewability involves breaking adm process technical organisational elements provide systematic framework determining contextually appropriate recordkeeping mechanisms facilitate meaningful review individual decisions process whole argue reviewability framework drawing administrative laws approach reviewing human decisionmaking offers practical way forward towards holistic legallyrelevant form accountability adm "
306," dangers stochastic parrots can language models big 🦜"," past years work nlp characterized development deployment ever larger language models especially english bert variants gpt others recently switchc pushed boundaries possible architectural innovations sheer size using pretrained models methodology finetuning specific tasks researchers extended state art wide array tasks measured leaderboards specific benchmarks english paper take step back ask big big possible risks associated technology paths available mitigating risks provide recommendations including weighing environmental financial costs first investing resources curating carefully documenting datasets rather ingesting everything web carrying predevelopment exercises evaluating planned approach fits research development goals supports stakeholder values encouraging research directions beyond ever larger language models "
307,"formalizing trust artificial intelligence prerequisites causes goals human trust ai"," trust central component interaction people ai incorrect levels trust may cause misuse abuse disuse technology precisely nature trust ai prerequisites goals cognitive mechanism trust can promote assess whether satisfied given interaction work aims answer questions discuss model trust inspired identical interpersonal trust ie trust people defined sociologists model rests two key properties vulnerability user ability anticipate impact ai models decisions incorporate formalization contractual trust trust user ai model trust implicit explicit contract will hold formalization trustworthiness detaches notion trustworthiness sociology concepts warranted unwarranted trust present possible causes warranted trust intrinsic reasoning extrinsic behavior discuss design trustworthy ai evaluate whether trust manifested whether warranted finally elucidate connection trust xai using formalization "
308,"tilt gdpraligned transparency information language toolkit practical privacy engineering"," paper present tilt transparency information language toolkit explicitly designed represent process transparency information line requirements gdpr allowing automated adaptive use information established legalese data protection policies provide detailed analysis transparency obligations gdpr identify expressiveness required formal transparency language intended meet respective legal requirements addition identify set nonfunctional requirements need met foster practical adoption realworld web information systems engineering basis specify formal language present respective fully implemented toolkit around evaluate practical applicability language toolkit demonstrate additional prospects unlocks two different use cases interorganizational analysis personal datarelated practices allowing instance uncover data sharing networks based explicitly announced transparency information b presentation formally represented transparency information users novel comprehensible potentially adaptive user interfaces heightening data subjects actual informedness datarelated practices thus sovereignty altogether transparency information language toolkit allow differently previous work express transparency information line actual legal requirements practices modern web information systems engineering thereby pave way multitude novel possibilities heighten transparency user sovereignty practice "
