,id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed
342381,1205.338,Yefim Bakman,Yefim Bakman,Unfair items detection in educational measurement,"14 pages, 5 figures",,,,cs.AI physics.ed-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.
","[{'version': 'v1', 'created': 'Sat, 11 Feb 2012 16:12:05 GMT'}]",2020-10-06,"[['Bakman', 'Yefim', '']]"
414517,1303.286,"Moritz M\""uhlenthaler","Moritz M\""uhlenthaler, Rolf Wanka",Fairness in Academic Course Timetabling,"appeared in PATAT 2012, pp. 114-130",,10.1007/s10479-014-1553-2,,cs.AI cs.DS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of creating fair course timetables in the setting of
a university. Our motivation is to improve the overall satisfaction of
individuals concerned (students, teachers, etc.) by providing a fair timetable
to them. The central idea is that undesirable arrangements in the course
timetable, i.e., violations of soft constraints, should be distributed in a
fair way among the individuals. We propose two formulations for the fair course
timetabling problem that are based on max-min fairness and Jain's fairness
index, respectively. Furthermore, we present and experimentally evaluate an
optimization algorithm based on simulated annealing for solving max-min fair
course timetabling problems. The new contribution is concerned with measuring
the energy difference between two timetables, i.e., how much worse a timetable
is compared to another timetable with respect to max-min fairness. We introduce
three different energy difference measures and evaluate their impact on the
overall algorithm performance. The second proposed problem formulation focuses
on the tradeoff between fairness and the total amount of soft constraint
violations. Our experimental evaluation shows that the known best solutions to
the ITC2007 curriculum-based course timetabling instances are quite fair with
respect to Jain's fairness index. However, the experiments also show that the
fairness can be improved further for only a rather small increase in the total
amount of soft constraint violations.
","[{'version': 'v1', 'created': 'Tue, 12 Mar 2013 12:46:54 GMT'}]",2015-05-01,"[['Mühlenthaler', 'Moritz', ''], ['Wanka', 'Rolf', '']]"
439783,1306.4999,Lizi Zhang,Lizi Zhang,"Safeguarding E-Commerce against Advisor Cheating Behaviors: Towards More
  Robust Trust Models for Handling Unfair Ratings",,,,,cs.SI cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In electronic marketplaces, after each transaction buyers will rate the
products provided by the sellers. To decide the most trustworthy sellers to
transact with, buyers rely on trust models to leverage these ratings to
evaluate the reputation of sellers. Although the high effectiveness of
different trust models for handling unfair ratings have been claimed by their
designers, recently it is argued that these models are vulnerable to more
intelligent attacks, and there is an urgent demand that the robustness of the
existing trust models has to be evaluated in a more comprehensive way. In this
work, we classify the existing trust models into two broad categories and
propose an extendable e-marketplace testbed to evaluate their robustness
against different unfair rating attacks comprehensively. On top of highlighting
the robustness of the existing trust models for handling unfair ratings is far
from what they were claimed to be, we further propose and validate a novel
combination mechanism for the existing trust models, Discount-then-Filter, to
notably enhance their robustness against the investigated attacks.
","[{'version': 'v1', 'created': 'Thu, 20 Jun 2013 20:56:20 GMT'}]",2013-06-24,"[['Zhang', 'Lizi', '']]"
440385,1306.5601,"Moritz M\""uhlenthaler","Moritz M\""uhlenthaler and Rolf Wanka","A Decomposition of the Max-min Fair Curriculum-based Course Timetabling
  Problem","revised version (fixed problems in the notation and general
  improvements); original paper: 16 pages, accepted for publication at the
  Multidisciplinary International Scheduling Conference 2013 (MISTA 2013)",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a decomposition of the max-min fair curriculum-based course
timetabling (MMF-CB-CTT) problem. The decomposition models the room assignment
subproblem as a generalized lexicographic bottleneck optimization problem
(LBOP). We show that the generalized LBOP can be solved efficiently if the
corresponding sum optimization problem can be solved efficiently. As a
consequence, the room assignment subproblem of the MMF-CB-CTT problem can be
solved efficiently. We use this insight to improve a previously proposed
heuristic algorithm for the MMF-CB-CTT problem. Our experimental results
indicate that using the new decomposition improves the performance of the
algorithm on most of the 21 ITC2007 test instances with respect to the quality
of the best solution found. Furthermore, we introduce a measure of the quality
of a solution to a max-min fair optimization problem. This measure helps to
overcome some limitations imposed by the qualitative nature of max-min fairness
and aids the statistical evaluation of the performance of randomized algorithms
for such problems. We use this measure to show that using the new decomposition
the algorithm outperforms the original one on most instances with respect to
the average solution quality.
","[{'version': 'v1', 'created': 'Mon, 24 Jun 2013 12:54:50 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Aug 2013 13:33:24 GMT'}]",2013-08-27,"[['Mühlenthaler', 'Moritz', ''], ['Wanka', 'Rolf', '']]"
487899,1312.6546,Haris Aziz,Haris Aziz and Serge Gaspers and Simon Mackenzie and Toby Walsh,Fair assignment of indivisible objects under ordinal preferences,extended version of a paper presented at AAMAS 2014,,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the discrete assignment problem in which agents express ordinal
preferences over objects and these objects are allocated to the agents in a
fair manner. We use the stochastic dominance relation between fractional or
randomized allocations to systematically define varying notions of
proportionality and envy-freeness for discrete assignments. The computational
complexity of checking whether a fair assignment exists is studied for these
fairness notions. We also characterize the conditions under which a fair
assignment is guaranteed to exist. For a number of fairness concepts,
polynomial-time algorithms are presented to check whether a fair assignment
exists. Our algorithmic results also extend to the case of unequal entitlements
of agents. Our NP-hardness result, which holds for several variants of
envy-freeness, answers an open question posed by Bouveret, Endriss, and Lang
(ECAI 2010). We also propose fairness concepts that always suggest a non-empty
set of assignments with meaningful fairness properties. Among these concepts,
optimal proportionality and optimal weak proportionality appear to be desirable
fairness concepts.
","[{'version': 'v1', 'created': 'Mon, 23 Dec 2013 13:37:19 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Sep 2014 03:57:49 GMT'}, {'version': 'v3', 'created': 'Sat, 6 Dec 2014 04:30:06 GMT'}, {'version': 'v4', 'created': 'Wed, 17 Jun 2015 04:25:04 GMT'}]",2015-06-18,"[['Aziz', 'Haris', ''], ['Gaspers', 'Serge', ''], ['Mackenzie', 'Simon', ''], ['Walsh', 'Toby', '']]"
602231,1502.07571,Martin Aleksandrov D,"Martin Aleksandrov, Haris Aziz, Serge Gaspers, Toby Walsh",Online Fair Division: analysing a Food Bank problem,"7 pages, 2 figures, 1 table",,,,cs.GT cs.AI cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study an online model of fair division designed to capture features of a
real world charity problem. We consider two simple mechanisms for this model in
which agents simply declare what items they like. We analyse several axiomatic
properties of these mechanisms like strategy-proofness and envy-freeness.
Finally, we perform a competitive analysis and compute the price of anarchy.
","[{'version': 'v1', 'created': 'Thu, 26 Feb 2015 14:39:02 GMT'}, {'version': 'v2', 'created': 'Sat, 28 Feb 2015 08:05:32 GMT'}]",2015-03-03,"[['Aleksandrov', 'Martin', ''], ['Aziz', 'Haris', ''], ['Gaspers', 'Serge', ''], ['Walsh', 'Toby', '']]"
625995,1505.0572300000001,Indre Zliobaite,Indre Zliobaite,On the relation between accuracy and fairness in binary classification,"Accepted for presentation to the 2nd workshop on Fairness,
  Accountability, and Transparency in Machine Learning (http://www.fatml.org/)",,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our study revisits the problem of accuracy-fairness tradeoff in binary
classification. We argue that comparison of non-discriminatory classifiers
needs to account for different rates of positive predictions, otherwise
conclusions about performance may be misleading, because accuracy and
discrimination of naive baselines on the same dataset vary with different rates
of positive predictions. We provide methodological recommendations for sound
comparison of non-discriminatory classifiers, and present a brief theoretical
and empirical analysis of tradeoffs between accuracy and non-discrimination.
","[{'version': 'v1', 'created': 'Thu, 21 May 2015 13:20:06 GMT'}]",2015-05-22,"[['Zliobaite', 'Indre', '']]"
627706,1505.0743400000001,Qing Chuan Ye,"Qing Chuan Ye, Yingqian Zhang, Rommert Dekker",Fair task allocation in transportation,,"Ye QC, et al. Fair task allocation in transportation. Omega
  (2016), http://dx.doi.org/10.1016/j.omega.2016.05.005",10.1016/j.omega.2016.05.005,,cs.AI cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task allocation problems have traditionally focused on cost optimization.
However, more and more attention is being given to cases in which cost should
not always be the sole or major consideration. In this paper we study a fair
task allocation problem in transportation where an optimal allocation not only
has low cost but more importantly, it distributes tasks as even as possible
among heterogeneous participants who have different capacities and costs to
execute tasks. To tackle this fair minimum cost allocation problem we analyze
and solve it in two parts using two novel polynomial-time algorithms. We show
that despite the new fairness criterion, the proposed algorithms can solve the
fair minimum cost allocation problem optimally in polynomial time. In addition,
we conduct an extensive set of experiments to investigate the trade-off between
cost minimization and fairness. Our experimental results demonstrate the
benefit of factoring fairness into task allocation. Among the majority of test
instances, fairness comes with a very small price in terms of cost.
","[{'version': 'v1', 'created': 'Wed, 27 May 2015 19:03:07 GMT'}, {'version': 'v2', 'created': 'Wed, 18 May 2016 15:48:39 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Dec 2016 15:07:41 GMT'}]",2016-12-08,"[['Ye', 'Qing Chuan', ''], ['Zhang', 'Yingqian', ''], ['Dekker', 'Rommert', '']]"
720901,1604.01734,Michel Lemaitre,"Sylvain Bouveret, Michel Lema\^itre (Toulouse)","Efficiency and Sequenceability in Fair Division of Indivisible Goods
  with Additive Preferences","COMSOC-2016, Jun 2016, Toulouse, France",,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In fair division of indivisible goods, using sequences of sincere choices (or
picking sequences) is a natural way to allocate the objects. The idea is the
following: at each stage, a designated agent picks one object among those that
remain. This paper, restricted to the case where the agents have numerical
additive preferences over objects, revisits to some extent the seminal paper by
Brams and King [9] which was specific to ordinal and linear order preferences
over items. We point out similarities and differences with this latter context.
In particular, we show that any Pareto-optimal allocation (under additive
preferences) is sequenceable, but that the converse is not true anymore. This
asymmetry leads naturally to the definition of a ""scale of efficiency"" having
three steps: Pareto-optimality, sequenceability without Pareto-optimality, and
non-sequenceability. Finally, we investigate the links between these efficiency
properties and the ""scale of fairness"" we have described in an earlier work
[7]: we first show that an allocation can be envy-free and non-sequenceable,
but that every competitive equilibrium with equal incomes is sequenceable. Then
we experimentally explore the links between the scales of efficiency and
fairness.
","[{'version': 'v1', 'created': 'Wed, 6 Apr 2016 19:08:34 GMT'}]",2016-04-07,"[['Bouveret', 'Sylvain', '', 'Toulouse'], ['Lemaître', 'Michel', '', 'Toulouse']]"
781414,1610.0606699999998,Aws Albarghouthi,Aws Albarghouthi and Loris D'Antoni and Samuel Drews and Aditya Nori,Fairness as a Program Property,,,,,cs.PL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We explore the following question: Is a decision-making program fair, for
some useful definition of fairness? First, we describe how several algorithmic
fairness questions can be phrased as program verification problems. Second, we
discuss an automated verification technique for proving or disproving fairness
of decision-making programs with respect to a probabilistic model of the
population.
","[{'version': 'v1', 'created': 'Wed, 19 Oct 2016 15:31:34 GMT'}]",2016-10-20,"[['Albarghouthi', 'Aws', ''], [""D'Antoni"", 'Loris', ''], ['Drews', 'Samuel', ''], ['Nori', 'Aditya', '']]"
792036,1611.06589,Rediet Abebe,"Rediet Abebe, Jon Kleinberg, David Parkes",Fair Division via Social Comparison,"18 pages, 3 figures, Proceedings of the 16th Conference on Autonomous
  Agents and Multi-Agent Systems (AAMAS, 2017)",,,,cs.DS cs.AI cs.GT math.CO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the classical cake cutting problem, a resource must be divided among
agents with different utilities so that each agent believes they have received
a fair share of the resource relative to the other agents. We introduce a
variant of the problem in which we model an underlying social network on the
agents with a graph, and agents only evaluate their shares relative to their
neighbors' in the network. This formulation captures many situations in which
it is unrealistic to assume a global view, and also exposes interesting
phenomena in the original problem.
  Specifically, we say an allocation is locally envy-free if no agent envies a
neighbor's allocation and locally proportional if each agent values her own
allocation as much as the average value of her neighbor's allocations, with the
former implying the latter. While global envy-freeness implies local
envy-freeness, global proportionality does not imply local proportionality, or
vice versa. A general result is that for any two distinct graphs on the same
set of nodes and an allocation, there exists a set of valuation functions such
that the allocation is locally proportional on one but not the other.
  We fully characterize the set of graphs for which an oblivious single-cutter
protocol-- a protocol that uses a single agent to cut the cake into pieces
--admits a bounded protocol with $O(n^2)$ query complexity for locally
envy-free allocations in the Robertson-Webb model. We also consider the price
of envy-freeness, which compares the total utility of an optimal allocation to
the best utility of an allocation that is envy-free. We show that a lower bound
of $\Omega(\sqrt{n})$ on the price of envy-freeness for global allocations in
fact holds for local envy-freeness in any connected undirected graph. Thus,
sparse graphs surprisingly do not provide more flexibility with respect to the
quality of envy-free allocations.
","[{'version': 'v1', 'created': 'Sun, 20 Nov 2016 20:42:07 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Feb 2018 19:16:36 GMT'}]",2018-02-27,"[['Abebe', 'Rediet', ''], ['Kleinberg', 'Jon', ''], ['Parkes', 'David', '']]"
822880,1702.0828600000002,Duncan McElfresh,Duncan C. McElfresh and John P. Dickerson,"Balancing Lexicographic Fairness and a Utilitarian Objective with
  Application to Kidney Exchange",,,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Balancing fairness and efficiency in resource allocation is a classical
economic and computational problem. The price of fairness measures the
worst-case loss of economic efficiency when using an inefficient but fair
allocation rule; for indivisible goods in many settings, this price is
unacceptably high. One such setting is kidney exchange, where needy patients
swap willing but incompatible kidney donors. In this work, we close an open
problem regarding the theoretical price of fairness in modern kidney exchanges.
We then propose a general hybrid fairness rule that balances a strict
lexicographic preference ordering over classes of agents, and a utilitarian
objective that maximizes economic efficiency. We develop a utility function for
this rule that favors disadvantaged groups lexicographically; but if cost to
overall efficiency becomes too high, it switches to a utilitarian objective.
This rule has only one parameter which is proportional to a bound on the price
of fairness, and can be adjusted by policymakers. We apply this rule to real
data from a large kidney exchange and show that our hybrid rule produces more
reliable outcomes than other fairness rules.
","[{'version': 'v1', 'created': 'Mon, 27 Feb 2017 13:54:44 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Sep 2017 21:43:49 GMT'}]",2017-09-11,"[['McElfresh', 'Duncan C.', ''], ['Dickerson', 'John P.', '']]"
834049,1703.10545,Srijan Kumar,"Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos
  Faloutsos, V.S. Subrahamanian",FairJudge: Trustworthy User Prediction in Rating Platforms,,,,,cs.SI cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Rating platforms enable large-scale collection of user opinion about items
(products, other users, etc.). However, many untrustworthy users give
fraudulent ratings for excessive monetary gains. In the paper, we present
FairJudge, a system to identify such fraudulent users. We propose three
metrics: (i) the fairness of a user that quantifies how trustworthy the user is
in rating the products, (ii) the reliability of a rating that measures how
reliable the rating is, and (iii) the goodness of a product that measures the
quality of the product. Intuitively, a user is fair if it provides reliable
ratings that are close to the goodness of the product. We formulate a mutually
recursive definition of these metrics, and further address cold start problems
and incorporate behavioral properties of users and products in the formulation.
We propose an iterative algorithm, FairJudge, to predict the values of the
three metrics. We prove that FairJudge is guaranteed to converge in a bounded
number of iterations, with linear time complexity. By conducting five different
experiments on five rating platforms, we show that FairJudge significantly
outperforms nine existing algorithms in predicting fair and unfair users. We
reported the 100 most unfair users in the Flipkart network to their review
fraud investigators, and 80 users were correctly identified (80% accuracy). The
FairJudge algorithm is already being deployed at Flipkart.
","[{'version': 'v1', 'created': 'Thu, 30 Mar 2017 16:02:25 GMT'}]",2017-03-31,"[['Kumar', 'Srijan', ''], ['Hooi', 'Bryan', ''], ['Makhija', 'Disha', ''], ['Kumar', 'Mohit', ''], ['Faloutsos', 'Christos', ''], ['Subrahamanian', 'V. S.', '']]"
852345,1705.0880399999999,Sirui Yao,"Sirui Yao, Bert Huang",Beyond Parity: Fairness Objectives for Collaborative Filtering,,,,,cs.IR cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.
","[{'version': 'v1', 'created': 'Wed, 24 May 2017 14:52:06 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Nov 2017 21:11:25 GMT'}]",2017-12-04,"[['Yao', 'Sirui', ''], ['Huang', 'Bert', '']]"
864573,1706.0983800000001,Sirui Yao,"Sirui Yao, Bert Huang",New Fairness Metrics for Recommendation that Embrace Differences,"Presented as a poster at the 2017 Workshop on Fairness,
  Accountability, and Transparency in Machine Learning (FAT/ML 2017)",,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative filtering methods to make unfair predictions against
minority groups of users. We identify the insufficiency of existing fairness
metrics and propose four new metrics that address different forms of
unfairness. These fairness metrics can be optimized by adding fairness terms to
the learning objective. Experiments on synthetic and real data show that our
new metrics can better measure fairness than the baseline, and that the
fairness objectives effectively help reduce unfairness.
","[{'version': 'v1', 'created': 'Thu, 29 Jun 2017 16:32:12 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Dec 2017 19:00:37 GMT'}]",2017-12-15,"[['Yao', 'Sirui', ''], ['Huang', 'Bert', '']]"
866227,1707.01195,Thomas Miconi,Thomas Miconi,"The impossibility of ""fairness"": a generalized impossibility result for
  decisions",,,,,stat.AP cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Various measures can be used to estimate bias or unfairness in a predictor.
Previous work has already established that some of these measures are
incompatible with each other. Here we show that, when groups differ in
prevalence of the predicted event, several intuitive, reasonable measures of
fairness (probability of positive prediction given occurrence or
non-occurrence; probability of occurrence given prediction or non-prediction;
and ratio of predictions over occurrences for each group) are all mutually
exclusive: if one of them is equal among groups, the other two must differ. The
only exceptions are for perfect, or trivial (always-positive or
always-negative) predictors. As a consequence, any non-perfect, non-trivial
predictor must necessarily be ""unfair"" under two out of three reasonable sets
of criteria. This result readily generalizes to a wide range of well-known
statistical quantities (sensitivity, specificity, false positive rate,
precision, etc.), all of which can be divided into three mutually exclusive
groups. Importantly, The results applies to all predictors, whether algorithmic
or human. We conclude with possible ways to handle this effect when assessing
and designing prediction methods.
","[{'version': 'v1', 'created': 'Wed, 5 Jul 2017 02:02:42 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Aug 2017 16:31:06 GMT'}, {'version': 'v3', 'created': 'Mon, 11 Sep 2017 21:18:54 GMT'}]",2017-09-13,"[['Miconi', 'Thomas', '']]"
867065,1707.02033,Xiaohui Bei,"Xiaohui Bei, Youming Qiao, Shengyu Zhang",Networked Fairness in Cake Cutting,A preliminary version of this paper appears at IJCAI 2017,,,,cs.DS cs.AI cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a graphical framework for fair division in cake cutting, where
comparisons between agents are limited by an underlying network structure. We
generalize the classical fairness notions of envy-freeness and proportionality
to this graphical setting. Given a simple undirected graph G, an allocation is
envy-free on G if no agent envies any of her neighbor's share, and is
proportional on G if every agent values her own share no less than the average
among her neighbors, with respect to her own measure. These generalizations
open new research directions in developing simple and efficient algorithms that
can produce fair allocations under specific graph structures.
  On the algorithmic frontier, we first propose a moving-knife algorithm that
outputs an envy-free allocation on trees. The algorithm is significantly
simpler than the discrete and bounded envy-free algorithm recently designed by
Aziz and Mackenzie for complete graphs. Next, we give a discrete and bounded
algorithm for computing a proportional allocation on descendant graphs, a class
of graphs by taking a rooted tree and connecting all its ancestor-descendant
pairs.
","[{'version': 'v1', 'created': 'Fri, 7 Jul 2017 04:31:33 GMT'}]",2017-07-10,"[['Bei', 'Xiaohui', ''], ['Qiao', 'Youming', ''], ['Zhang', 'Shengyu', '']]"
875766,1708.0075399999998,Indre Zliobaite,Indre Zliobaite,Fairness-aware machine learning: a perspective,,,,,cs.AI cs.CY cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.
","[{'version': 'v1', 'created': 'Wed, 2 Aug 2017 14:14:49 GMT'}]",2017-08-03,"[['Zliobaite', 'Indre', '']]"
888087,1709.03221,Yuriy Brun,"Sainyam Galhotra, Yuriy Brun, Alexandra Meliou",Fairness Testing: Testing Software for Discrimination,"Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness
  Testing: Testing Software for Discrimination. In Proceedings of 2017 11th
  Joint Meeting of the European Software Engineering Conference and the ACM
  SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE),
  Paderborn, Germany, September 4-8, 2017 (ESEC/FSE'17).
  https://doi.org/10.1145/3106237.3106277, ESEC/FSE, 2017",,10.1145/3106237.3106277,,cs.SE cs.AI cs.CY cs.DB cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper defines software fairness and discrimination and develops a
testing-based method for measuring if and how much software discriminates,
focusing on causality in discriminatory behavior. Evidence of software
discrimination has been found in modern software systems that recommend
criminal sentences, grant access to financial products, and determine who is
allowed to participate in promotions. Our approach, Themis, generates efficient
test suites to measure discrimination. Given a schema describing valid system
inputs, Themis generates discrimination tests automatically and does not
require an oracle. We evaluate Themis on 20 software systems, 12 of which come
from prior work with explicit focus on avoiding discrimination. We find that
(1) Themis is effective at discovering software discrimination, (2)
state-of-the-art techniques for removing discrimination from algorithms fail in
many situations, at times discriminating against as much as 98% of an input
subdomain, (3) Themis optimizations are effective at producing efficient test
suites for measuring discrimination, and (4) Themis is more efficient on
systems that exhibit more discrimination. We thus demonstrate that fairness
testing is a critical aspect of the software development cycle in domains with
possible discrimination and provide initial tools for measuring software
discrimination.
","[{'version': 'v1', 'created': 'Mon, 11 Sep 2017 02:45:22 GMT'}]",2017-09-12,"[['Galhotra', 'Sainyam', ''], ['Brun', 'Yuriy', ''], ['Meliou', 'Alexandra', '']]"
898567,1710.0318399999999,Pratik Gajane,Pratik Gajane and Mykola Pechenizkiy,On Formalizing Fairness in Prediction with Machine Learning,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.
","[{'version': 'v1', 'created': 'Mon, 9 Oct 2017 16:39:31 GMT'}, {'version': 'v2', 'created': 'Fri, 25 May 2018 10:12:23 GMT'}, {'version': 'v3', 'created': 'Mon, 28 May 2018 08:22:01 GMT'}]",2018-05-29,"[['Gajane', 'Pratik', ''], ['Pechenizkiy', 'Mykola', '']]"
900307,1710.0492399999998,Junpei Komiyama,Junpei Komiyama and Hajime Shimao,Two-stage Algorithm for Fairness-aware Machine Learning,,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Algorithmic decision making process now affects many aspects of our lives.
Standard tools for machine learning, such as classification and regression, are
subject to the bias in data, and thus direct application of such off-the-shelf
tools could lead to a specific group being unfairly discriminated. Removing
sensitive attributes of data does not solve this problem because a
\textit{disparate impact} can arise when non-sensitive attributes and sensitive
attributes are correlated. Here, we study a fair machine learning algorithm
that avoids such a disparate impact when making a decision. Inspired by the
two-stage least squares method that is widely used in the field of economics,
we propose a two-stage algorithm that removes bias in the training data. The
proposed algorithm is conceptually simple. Unlike most of existing fair
algorithms that are designed for classification tasks, the proposed method is
able to (i) deal with regression tasks, (ii) combine explanatory attributes to
remove reverse discrimination, and (iii) deal with numerical sensitive
attributes. The performance and fairness of the proposed algorithm are
evaluated in simulations with synthetic and real-world datasets.
","[{'version': 'v1', 'created': 'Fri, 13 Oct 2017 13:58:42 GMT'}]",2017-10-16,"[['Komiyama', 'Junpei', ''], ['Shimao', 'Hajime', '']]"
905440,1710.1005699999998,Huang Lingxiao,"L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi",Multiwinner Voting with Fairness Constraints,The conference version of this paper appears in IJCAI-ECAI 2018,,,,cs.CY cs.AI cs.DS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multiwinner voting rules are used to select a small representative subset of
candidates or items from a larger set given the preferences of voters. However,
if candidates have sensitive attributes such as gender or ethnicity (when
selecting a committee), or specified types such as political leaning (when
selecting a subset of news items), an algorithm that chooses a subset by
optimizing a multiwinner voting rule may be unbalanced in its selection -- it
may under or over represent a particular gender or political orientation in the
examples above. We introduce an algorithmic framework for multiwinner voting
problems when there is an additional requirement that the selected subset
should be ""fair"" with respect to a given set of attributes. Our framework
provides the flexibility to (1) specify fairness with respect to multiple,
non-disjoint attributes (e.g., ethnicity and gender) and (2) specify a score
function. We study the computational complexity of this constrained multiwinner
voting problem for monotone and submodular score functions and present several
approximation algorithms and matching hardness of approximation results for
various attribute group structure and types of score functions. We also present
simulations that suggest that adding fairness constraints may not affect the
scores significantly when compared to the unconstrained case.
","[{'version': 'v1', 'created': 'Fri, 27 Oct 2017 10:13:31 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Jun 2018 19:19:15 GMT'}]",2018-06-20,"[['Celis', 'L. Elisa', ''], ['Huang', 'Lingxiao', ''], ['Vishnoi', 'Nisheeth K.', '']]"
914631,1711.0762100000002,Siddharth Barman,"Siddharth Barman, Arpita Biswas, Sanath Kumar Krishnamurthy, and Y.
  Narahari",Groupwise Maximin Fair Allocation of Indivisible Goods,19 pages,,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the problem of allocating indivisible goods among n agents in a fair
manner. For this problem, maximin share (MMS) is a well-studied solution
concept which provides a fairness threshold. Specifically, maximin share is
defined as the minimum utility that an agent can guarantee for herself when
asked to partition the set of goods into n bundles such that the remaining
(n-1) agents pick their bundles adversarially. An allocation is deemed to be
fair if every agent gets a bundle whose valuation is at least her maximin
share.
  Even though maximin shares provide a natural benchmark for fairness, it has
its own drawbacks and, in particular, it is not sufficient to rule out
unsatisfactory allocations. Motivated by these considerations, in this work we
define a stronger notion of fairness, called groupwise maximin share guarantee
(GMMS). In GMMS, we require that the maximin share guarantee is achieved not
just with respect to the grand bundle, but also among all the subgroups of
agents. Hence, this solution concept strengthens MMS and provides an ex-post
fairness guarantee. We show that in specific settings, GMMS allocations always
exist. We also establish the existence of approximate GMMS allocations under
additive valuations, and develop a polynomial-time algorithm to find such
allocations. Moreover, we establish a scale of fairness wherein we show that
GMMS implies approximate envy freeness.
  Finally, we empirically demonstrate the existence of GMMS allocations in a
large set of randomly generated instances. For the same set of instances, we
additionally show that our algorithm achieves an approximation factor better
than the established, worst-case bound.
","[{'version': 'v1', 'created': 'Tue, 21 Nov 2017 04:00:30 GMT'}]",2017-11-22,"[['Barman', 'Siddharth', ''], ['Biswas', 'Arpita', ''], ['Krishnamurthy', 'Sanath Kumar', ''], ['Narahari', 'Y.', '']]"
933309,1801.04378,AmirEmad Ghassami,"AmirEmad Ghassami, Sajad Khodadadian, Negar Kiyavash",Fairness in Supervised Learning: An Information Theoretic Approach,,,,,cs.LG cs.AI cs.IT math.IT stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated decision making systems are increasingly being used in real-world
applications. In these systems for the most part, the decision rules are
derived by minimizing the training error on the available historical data.
Therefore, if there is a bias related to a sensitive attribute such as gender,
race, religion, etc. in the data, say, due to cultural/historical
discriminatory practices against a certain demographic, the system could
continue discrimination in decisions by including the said bias in its decision
rule. We present an information theoretic framework for designing fair
predictors from data, which aim to prevent discrimination against a specified
sensitive attribute in a supervised learning setting. We use equalized odds as
the criterion for discrimination, which demands that the prediction should be
independent of the protected attribute conditioned on the actual label. To
ensure fairness and generalization simultaneously, we compress the data to an
auxiliary variable, which is used for the prediction task. This auxiliary
variable is chosen such that it is decontaminated from the discriminatory
attribute in the sense of equalized odds. The final predictor is obtained by
applying a Bayesian decision rule to the auxiliary variable.
","[{'version': 'v1', 'created': 'Sat, 13 Jan 2018 04:03:04 GMT'}, {'version': 'v2', 'created': 'Sun, 29 Jul 2018 21:49:01 GMT'}]",2018-07-31,"[['Ghassami', 'AmirEmad', ''], ['Khodadadian', 'Sajad', ''], ['Kiyavash', 'Negar', '']]"
952985,1803.0285199999998,Daniel Estrada,Daniel Estrada,"Value Alignment, Fair Play, and the Rights of Service Robots",,ACM/AIES 2018,10.1145/3278721.3278730,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ethics and safety research in artificial intelligence is increasingly framed
in terms of ""alignment"" with human values and interests. I argue that Turing's
call for ""fair play for machines"" is an early and often overlooked contribution
to the alignment literature. Turing's appeal to fair play suggests a need to
correct human behavior to accommodate our machines, a surprising inversion of
how value alignment is treated today. Reflections on ""fair play"" motivate a
novel interpretation of Turing's notorious ""imitation game"" as a condition not
of intelligence but instead of value alignment: a machine demonstrates a
minimal degree of alignment (with the norms of conversation, for instance) when
it can go undetected when interrogated by a human. I carefully distinguish this
interpretation from the Moral Turing Test, which is not motivated by a
principle of fair play, but instead depends on imitation of human moral
behavior. Finally, I consider how the framework of fair play can be used to
situate the debate over robot rights within the alignment literature. I argue
that extending rights to service robots operating in public spaces is ""fair"" in
precisely the sense that it encourages an alignment of interests between humans
and machines.
","[{'version': 'v1', 'created': 'Wed, 7 Mar 2018 19:33:08 GMT'}]",2018-12-07,"[['Estrada', 'Daniel', '']]"
960100,1803.0996699999998,Juan Duque Rodriguez,"Roberto Maestre, Juan Duque, Alberto Rubio, Juan Ar\'evalo",Reinforcement Learning for Fair Dynamic Pricing,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.
","[{'version': 'v1', 'created': 'Tue, 27 Mar 2018 09:00:48 GMT'}]",2018-03-28,"[['Maestre', 'Roberto', ''], ['Duque', 'Juan', ''], ['Rubio', 'Alberto', ''], ['Arévalo', 'Juan', '']]"
967253,1804.0556,Naman Goel,"Naman Goel, Boi Faltings",Deep Bayesian Trust : A Dominant and Fair Incentive Mechanism for Crowd,,,,,cs.GT cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An important class of game-theoretic incentive mechanisms for eliciting
effort from a crowd are the peer based mechanisms, in which workers are paid by
matching their answers with one another. The other classic mechanism is to have
the workers solve some gold standard tasks and pay them according to their
accuracy on gold tasks. This mechanism ensures stronger incentive compatibility
than the peer based mechanisms but assigning gold tasks to all workers becomes
inefficient at large scale. We propose a novel mechanism that assigns gold
tasks to only a few workers and exploits transitivity to derive accuracy of the
rest of the workers from their peers' accuracy. We show that the resulting
mechanism ensures a dominant notion of incentive compatibility and fairness.
","[{'version': 'v1', 'created': 'Mon, 16 Apr 2018 09:04:11 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Nov 2018 22:26:16 GMT'}]",2018-11-16,"[['Goel', 'Naman', ''], ['Faltings', 'Boi', '']]"
971214,1804.0952100000002,Siddharth Barman,Siddharth Barman and Arpita Biswas,Fair Division Under Cardinality Constraints,"19 pages. Accepted at the 27th International Joint Conference on
  Artificial Intelligence (IJCAI), 2018",,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of fairly allocating indivisible goods, among agents,
under cardinality constraints and additive valuations. In this setting, we are
given a partition of the entire set of goods---i.e., the goods are
categorized---and a limit is specified on the number of goods that can be
allocated from each category to any agent. The objective here is to find a fair
allocation in which the subset of goods assigned to any agent satisfies the
given cardinality constraints. This problem naturally captures a number of
resource-allocation applications, and is a generalization of the well-studied
(unconstrained) fair division problem.
  The two central notions of fairness, in the context of fair division of
indivisible goods, are envy freeness up to one good (EF1) and the (approximate)
maximin share guarantee (MMS). We show that the existence and algorithmic
guarantees established for these solution concepts in the unconstrained setting
can essentially be achieved under cardinality constraints. Specifically, we
develop efficient algorithms which compute EF1 and approximately MMS
allocations in the constrained setting.
  Furthermore, focusing on the case wherein all the agents have the same
additive valuation, we establish that EF1 allocations exist and can be computed
efficiently even under matroid constraints.
","[{'version': 'v1', 'created': 'Wed, 25 Apr 2018 12:38:06 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Jul 2018 15:21:43 GMT'}]",2018-07-24,"[['Barman', 'Siddharth', ''], ['Biswas', 'Arpita', '']]"
974262,1805.01217,Marco Lippi,"Marco Lippi, Przemyslaw Palka, Giuseppe Contissa, Francesca Lagioia,
  Hans-Wolfgang Micklitz, Giovanni Sartor, Paolo Torroni","CLAUDETTE: an Automated Detector of Potentially Unfair Clauses in Online
  Terms of Service",,Artif. Intell. Law 27 (2019) 117-139,10.1007/s10506-019-09243-2,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.
","[{'version': 'v1', 'created': 'Thu, 3 May 2018 10:53:54 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Feb 2019 14:17:02 GMT'}]",2020-01-29,"[['Lippi', 'Marco', ''], ['Palka', 'Przemyslaw', ''], ['Contissa', 'Giuseppe', ''], ['Lagioia', 'Francesca', ''], ['Micklitz', 'Hans-Wolfgang', ''], ['Sartor', 'Giovanni', ''], ['Torroni', 'Paolo', '']]"
978904,1805.0585899999999,Chris Russell,"Joshua R. Loftus, Chris Russell, Matt J. Kusner, and Ricardo Silva",Causal Reasoning for Algorithmic Fairness,,,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we argue for the importance of causal reasoning in creating
fair algorithms for decision making. We give a review of existing approaches to
fairness, describe work in causality necessary for the understanding of causal
approaches, argue why causality is necessary for any approach that wishes to be
fair, and give a detailed analysis of the many recent approaches to
causality-based fairness.
","[{'version': 'v1', 'created': 'Tue, 15 May 2018 15:42:35 GMT'}]",2018-05-16,"[['Loftus', 'Joshua R.', ''], ['Russell', 'Chris', ''], ['Kusner', 'Matt J.', ''], ['Silva', 'Ricardo', '']]"
982911,1805.09866,Fabio Massimo Zennaro,"Fabio Massimo Zennaro, Magdalena Ivanovska","Pooling of Causal Models under Counterfactual Fairness via Causal
  Judgement Aggregation","8 pages, 4 figures, workshop paper",,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we consider the problem of combining multiple probabilistic
causal models, provided by different experts, under the requirement that the
aggregated model satisfy the criterion of counterfactual fairness. We build
upon the work on causal models and fairness in machine learning, and we express
the problem of combining multiple models within the framework of opinion
pooling. We propose two simple algorithms, grounded in the theory of
counterfactual fairness and causal judgment aggregation, that are guaranteed to
generate aggregated probabilistic causal models respecting the criterion of
fairness, and we compare their behaviors on a toy case study.
","[{'version': 'v1', 'created': 'Thu, 24 May 2018 19:39:20 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Oct 2018 12:56:06 GMT'}]",2018-10-02,"[['Zennaro', 'Fabio Massimo', ''], ['Ivanovska', 'Magdalena', '']]"
988020,1806.0238,Matthew Kusner,"Matt J. Kusner, Chris Russell, Joshua R. Loftus, Ricardo Silva",Causal Interventions for Fairness,,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most approaches in algorithmic fairness constrain machine learning methods so
the resulting predictions satisfy one of several intuitive notions of fairness.
While this may help private companies comply with non-discrimination laws or
avoid negative publicity, we believe it is often too little, too late. By the
time the training data is collected, individuals in disadvantaged groups have
already suffered from discrimination and lost opportunities due to factors out
of their control. In the present work we focus instead on interventions such as
a new public policy, and in particular, how to maximize their positive effects
while improving the fairness of the overall system. We use causal methods to
model the effects of interventions, allowing for potential interference--each
individual's outcome may depend on who else receives the intervention. We
demonstrate this with an example of allocating a budget of teaching resources
using a dataset of schools in New York City.
","[{'version': 'v1', 'created': 'Wed, 6 Jun 2018 18:46:11 GMT'}]",2018-06-08,"[['Kusner', 'Matt J.', ''], ['Russell', 'Chris', ''], ['Loftus', 'Joshua R.', ''], ['Silva', 'Ricardo', '']]"
989874,1806.0423399999997,Ulle Endriss,Ulle Endriss,Lecture Notes on Fair Division,,,,,cs.AI cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fair division is the problem of dividing one or several goods amongst two or
more agents in a way that satisfies a suitable fairness criterion. These Notes
provide a succinct introduction to the field. We cover three main topics.
First, we need to define what is to be understood by a ""fair"" allocation of
goods to individuals. We present an overview of the most important fairness
criteria (as well as the closely related criteria for economic efficiency)
developed in the literature, together with a short discussion of their
axiomatic foundations. Second, we give an introduction to cake-cutting
procedures as an example of methods for fairly dividing a single divisible
resource amongst a group of individuals. Third, we discuss the combinatorial
optimisation problem of fairly allocating a set of indivisible goods to a group
of agents, covering both centralised algorithms (similar to auctions) and a
distributed approach based on negotiation.
  While the classical literature on fair division has largely developed within
Economics, these Notes are specifically written for readers with a background
in Computer Science or similar, and who may be (or may wish to be) engaged in
research in Artificial Intelligence, Multiagent Systems, or Computational
Social Choice. References for further reading, as well as a small number of
exercises, are included.
  Notes prepared for a tutorial at the 11th European Agent Systems Summer
School (EASSS-2009), Torino, Italy, 31 August and 1 September 2009. Updated for
a tutorial at the COST-ADT Doctoral School on Computational Social Choice,
Estoril, Portugal, 9--14 April 2010.
","[{'version': 'v1', 'created': 'Mon, 11 Jun 2018 20:41:23 GMT'}]",2018-06-13,"[['Endriss', 'Ulle', '']]"
990599,1806.0495899999999,Hoda Heidari,"Hoda Heidari, Claudio Ferrari, Krishna P. Gummadi, and Andreas Krause","Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated
  Decision Making","Conference: Thirty-second Conference on Neural Information Processing
  Systems (NIPS 2018)",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We draw attention to an important, yet largely overlooked aspect of
evaluating fairness for automated decision making systems---namely risk and
welfare considerations. Our proposed family of measures corresponds to the
long-established formulations of cardinal social welfare in economics, and is
justified by the Rawlsian conception of fairness behind a veil of ignorance.
The convex formulation of our welfare-based measures of fairness allows us to
integrate them as a constraint into any convex loss minimization pipeline. Our
empirical analysis reveals interesting trade-offs between our proposal and (a)
prediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of
individual fairness. Furthermore and perhaps most importantly, our work
provides both heuristic justification and empirical evidence suggesting that a
lower-bound on our measures often leads to bounded inequality in algorithmic
outcomes; hence presenting the first computationally feasible mechanism for
bounding individual-level inequality.
","[{'version': 'v1', 'created': 'Wed, 13 Jun 2018 11:36:05 GMT'}, {'version': 'v2', 'created': 'Mon, 29 Oct 2018 09:39:14 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Nov 2018 13:29:41 GMT'}, {'version': 'v4', 'created': 'Fri, 11 Jan 2019 11:36:36 GMT'}]",2019-01-14,"[['Heidari', 'Hoda', ''], ['Ferrari', 'Claudio', ''], ['Gummadi', 'Krishna P.', ''], ['Krause', 'Andreas', '']]"
990752,1806.0511199999999,Junpei Komiyama,Junpei Komiyama and Hajime Shimao,Comparing Fairness Criteria Based on Social Outcome,,,,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness in algorithmic decision-making processes is attracting increasing
concern. When an algorithm is applied to human-related decision-making an
estimator solely optimizing its predictive power can learn biases on the
existing data, which motivates us the notion of fairness in machine learning.
while several different notions are studied in the literature, little studies
are done on how these notions affect the individuals. We demonstrate such a
comparison between several policies induced by well-known fairness criteria,
including the color-blind (CB), the demographic parity (DP), and the equalized
odds (EO). We show that the EO is the only criterion among them that removes
group-level disparity. Empirical studies on the social welfare and disparity of
these policies are conducted.
","[{'version': 'v1', 'created': 'Wed, 13 Jun 2018 15:34:13 GMT'}]",2018-06-14,"[['Komiyama', 'Junpei', ''], ['Shimao', 'Hajime', '']]"
990890,1806.0525,Edward Raff,"Jared Sylvester, Edward Raff",What About Applied Fairness?,"Accepted at Machine Learning: The Debates (ML-D), at ICML Stockholm,
  Sweden, 2018. 5 pages","Machine Learning: The Debates (ML-D), at ICML Stockholm, Sweden,
  2018",,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning practitioners are often ambivalent about the ethical aspects
of their products. We believe anything that gets us from that current state to
one in which our systems are achieving some degree of fairness is an
improvement that should be welcomed. This is true even when that progress does
not get us 100% of the way to the goal of ""complete"" fairness or perfectly
align with our personal belief on which measure of fairness is used. Some
measure of fairness being built would still put us in a better position than
the status quo. Impediments to getting fairness and ethical concerns applied in
real applications, whether they are abstruse philosophical debates or technical
overhead such as the introduction of ever more hyper-parameters, should be
avoided. In this paper we further elaborate on our argument for this viewpoint
and its importance.
","[{'version': 'v1', 'created': 'Wed, 13 Jun 2018 20:15:28 GMT'}]",2018-06-15,"[['Sylvester', 'Jared', ''], ['Raff', 'Edward', '']]"
991695,1806.06055,Huang Lingxiao,"L. Elisa Celis, Lingxiao Huang, Vijay Keswani, Nisheeth K. Vishnoi","Classification with Fairness Constraints: A Meta-Algorithm with Provable
  Guarantees",,,,,cs.LG cs.AI cs.CY cs.DS stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Developing classification algorithms that are fair with respect to sensitive
attributes of the data has become an important problem due to the growing
deployment of classification algorithms in various social contexts. Several
recent works have focused on fairness with respect to a specific metric,
modeled the corresponding fair classification problem as a constrained
optimization problem, and developed tailored algorithms to solve them. Despite
this, there still remain important metrics for which we do not have fair
classifiers and many of the aforementioned algorithms do not come with
theoretical guarantees; perhaps because the resulting optimization problem is
non-convex. The main contribution of this paper is a new meta-algorithm for
classification that takes as input a large class of fairness constraints, with
respect to multiple non-disjoint sensitive attributes, and which comes with
provable guarantees. This is achieved by first developing a meta-algorithm for
a large family of classification problems with convex constraints, and then
showing that classification problems with general types of fairness constraints
can be reduced to those in this family. We present empirical results that show
that our algorithm can achieve near-perfect fairness with respect to various
fairness metrics, and that the loss in accuracy due to the imposed fairness
constraints is often small. Overall, this work unifies several prior works on
fair classification, presents a practical algorithm with theoretical
guarantees, and can handle fairness metrics that were previously not possible.
","[{'version': 'v1', 'created': 'Fri, 15 Jun 2018 17:45:58 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Aug 2018 17:53:43 GMT'}, {'version': 'v3', 'created': 'Wed, 15 Apr 2020 06:08:21 GMT'}]",2020-04-16,"[['Celis', 'L. Elisa', ''], ['Huang', 'Lingxiao', ''], ['Keswani', 'Vijay', ''], ['Vishnoi', 'Nisheeth K.', '']]"
997676,1807.00468,Sakshi Udeshi,"Sakshi Udeshi, Pryanshu Arora, Sudipta Chattopadhyay",Automated Directed Fairness Testing,"In Proceedings of the 2018 33rd ACM/IEEE International Conference on
  Automated Software Engineering (ASE 18), September 3-7, 2018, Montpellier,
  France","Automated Directed Fairness Testing. In Proceedings of the 2018
  33rd ACM/IEEE International Conference on Automated Software Engineering (ASE
  18), September 3-7, 2018, Montpellier, France",10.1145/3238147.3238165,,cs.LG cs.AI cs.SE stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Fairness is a critical trait in decision making. As machine-learning models
are increasingly being used in sensitive application domains (e.g. education
and employment) for decision making, it is crucial that the decisions computed
by such models are free of unintended bias. But how can we automatically
validate the fairness of arbitrary machine-learning models? For a given
machine-learning model and a set of sensitive input parameters, our AEQUITAS
approach automatically discovers discriminatory inputs that highlight fairness
violation. At the core of AEQUITAS are three novel strategies to employ
probabilistic search over the input space with the objective of uncovering
fairness violation. Our AEQUITAS approach leverages inherent robustness
property in common machine-learning models to design and implement scalable
test generation methodologies. An appealing feature of our generated test
inputs is that they can be systematically added to the training set of the
underlying model and improve its fairness. To this end, we design a fully
automated module that guarantees to improve the fairness of the underlying
model.
  We implemented AEQUITAS and we have evaluated it on six state-of-the-art
classifiers, including a classifier that was designed with fairness
constraints. We show that AEQUITAS effectively generates inputs to uncover
fairness violation in all the subject classifiers and systematically improves
the fairness of the respective models using the generated test inputs. In our
evaluation, AEQUITAS generates up to 70% discriminatory inputs (w.r.t. the
total number of inputs generated) and leverages these inputs to improve the
fairness up to 94%.
","[{'version': 'v1', 'created': 'Mon, 2 Jul 2018 05:29:57 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Jul 2018 12:08:59 GMT'}]",2018-08-02,"[['Udeshi', 'Sakshi', ''], ['Arora', 'Pryanshu', ''], ['Chattopadhyay', 'Sudipta', '']]"
1008573,1807.11367,Warut Suksompong,"Hoon Oh, Ariel D. Procaccia, Warut Suksompong",Fairly Allocating Many Goods with Few Queries,,,,,cs.GT cs.AI cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the query complexity of the fair allocation of indivisible
goods. For two agents with arbitrary monotonic valuations, we design an
algorithm that computes an allocation satisfying envy-freeness up to one good
(EF1), a relaxation of envy-freeness, using a logarithmic number of queries. We
show that the logarithmic query complexity bound also holds for three agents
with additive valuations. These results suggest that it is possible to fairly
allocate goods in practice even when the number of goods is extremely large. By
contrast, we prove that computing an allocation satisfying envy-freeness and
another of its relaxations, envy-freeness up to any good (EFX), requires a
linear number of queries even when there are only two agents with identical
additive valuations.
","[{'version': 'v1', 'created': 'Mon, 30 Jul 2018 14:27:52 GMT'}]",2018-07-31,"[['Oh', 'Hoon', ''], ['Procaccia', 'Ariel D.', ''], ['Suksompong', 'Warut', '']]"
1009125,1807.11919,Sylvain Bouveret,"Aur\'elie Beynier and Sylvain Bouveret and Michel Lema\^itre and
  Nicolas Maudet and Simon Rey","Efficiency, Sequenceability and Deal-Optimality in Fair Division of
  Indivisible Goods",arXiv admin note: substantial text overlap with arXiv:1604.01734,,,,cs.AI cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In fair division of indivisible goods, using sequences of sincere choices (or
picking sequences) is a natural way to allocate the objects. The idea is as
follows: at each stage, a designated agent picks one object among those that
remain. Another intuitive way to obtain an allocation is to give objects to
agents in the first place, and to let agents exchange them as long as such
""deals"" are beneficial. This paper investigates these notions, when agents have
additive preferences over objects, and unveils surprising connections between
them, and with other efficiency and fairness notions. In particular, we show
that an allocation is sequenceable iff it is optimal for a certain type of
deals, namely cycle deals involving a single object. Furthermore, any
Pareto-optimal allocation is sequenceable, but not the converse. Regarding
fairness, we show that an allocation can be envy-free and non-sequenceable, but
that every competitive equilibrium with equal incomes is sequenceable. To
complete the picture, we show how some domain restrictions may affect the
relations between these notions. Finally, we experimentally explore the links
between the scales of efficiency and fairness.
","[{'version': 'v1', 'created': 'Sat, 28 Jul 2018 12:13:31 GMT'}]",2018-08-01,"[['Beynier', 'Aurélie', ''], ['Bouveret', 'Sylvain', ''], ['Lemaître', 'Michel', ''], ['Maudet', 'Nicolas', ''], ['Rey', 'Simon', '']]"
1024214,1809.04198,Heinrich Jiang,"Andrew Cotter, Heinrich Jiang, Serena Wang, Taman Narayan, Maya Gupta,
  Seungil You, Karthik Sridharan","Optimization with Non-Differentiable Constraints with Applications to
  Fairness, Recall, Churn, and Other Goals",,,,,cs.LG cs.AI cs.GT math.OC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We show that many machine learning goals, such as improved fairness metrics,
can be expressed as constraints on the model's predictions, which we call rate
constraints. We study the problem of training non-convex models subject to
these rate constraints (or any non-convex and non-differentiable constraints).
In the non-convex setting, the standard approach of Lagrange multipliers may
fail. Furthermore, if the constraints are non-differentiable, then one cannot
optimize the Lagrangian with gradient-based methods. To solve these issues, we
introduce the proxy-Lagrangian formulation. This new formulation leads to an
algorithm that produces a stochastic classifier by playing a two-player
non-zero-sum game solving for what we call a semi-coarse correlated
equilibrium, which in turn corresponds to an approximately optimal and feasible
solution to the constrained optimization problem. We then give a procedure
which shrinks the randomized solution down to one that is a mixture of at most
$m+1$ deterministic solutions, given $m$ constraints. This culminates in
algorithms that can solve non-convex constrained optimization problems with
possibly non-differentiable and non-convex constraints with theoretical
guarantees. We provide extensive experimental results enforcing a wide range of
policy goals including different fairness metrics, and other goals on accuracy,
coverage, recall, and churn.
","[{'version': 'v1', 'created': 'Tue, 11 Sep 2018 23:41:47 GMT'}]",2018-09-13,"[['Cotter', 'Andrew', ''], ['Jiang', 'Heinrich', ''], ['Wang', 'Serena', ''], ['Narayan', 'Taman', ''], ['Gupta', 'Maya', ''], ['You', 'Seungil', ''], ['Sridharan', 'Karthik', '']]"
1024700,1809.0468399999997,Jiahao Chen,Jiahao Chen,Fair lending needs explainable models for responsible recommendation,"4 pages, position paper accepted for FATREC 2018 conference at ACM
  RecSys",,,,cs.LG cs.AI cs.CY stat.AP stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The financial services industry has unique explainability and fairness
challenges arising from compliance and ethical considerations in credit
decisioning. These challenges complicate the use of model machine learning and
artificial intelligence methods in business decision processes.
","[{'version': 'v1', 'created': 'Wed, 12 Sep 2018 21:29:20 GMT'}]",2018-09-14,"[['Chen', 'Jiahao', '']]"
1024753,1809.04737,Xintao Wu,Yongkai Wu and Lu Zhang and Xintao Wu,"Fairness-aware Classification: Criterion, Convexity, and Bounds",,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness-aware classification is receiving increasing attention in the
machine learning fields. Recently research proposes to formulate the
fairness-aware classification as constrained optimization problems. However,
several limitations exist in previous works due to the lack of a theoretical
framework for guiding the formulation. In this paper, we propose a general
framework for learning fair classifiers which addresses previous limitations.
The framework formulates various commonly-used fairness metrics as convex
constraints that can be directly incorporated into classic classification
models. Within the framework, we propose a constraint-free criterion on the
training data which ensures that any classifier learned from the data is fair.
We also derive the constraints which ensure that the real fairness metric is
satisfied when surrogate functions are used to achieve convexity. Our framework
can be used to for formulating fairness-aware classification with fairness
guarantee and computational efficiency. The experiments using real-world
datasets demonstrate our theoretical results and show the effectiveness of
proposed framework and methods.
","[{'version': 'v1', 'created': 'Thu, 13 Sep 2018 01:56:57 GMT'}]",2018-09-14,"[['Wu', 'Yongkai', ''], ['Zhang', 'Lu', ''], ['Wu', 'Xintao', '']]"
1031218,1810.0003100000001,Alejandro Noriega-Campero,"Alejandro Noriega-Campero, Michiel A. Bakker, Bernardo Garcia-Bulle,
  Alex Pentland",Active Fairness in Algorithmic Decision Making,,,,,cs.CY cs.AI cs.LG stat.AP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Society increasingly relies on machine learning models for automated decision
making. Yet, efficiency gains from automation have come paired with concern for
algorithmic discrimination that can systematize inequality. Recent work has
proposed optimal post-processing methods that randomize classification
decisions for a fraction of individuals, in order to achieve fairness measures
related to parity in errors and calibration. These methods, however, have
raised concern due to the information inefficiency, intra-group unfairness, and
Pareto sub-optimality they entail. The present work proposes an alternative
active framework for fair classification, where, in deployment, a
decision-maker adaptively acquires information according to the needs of
different groups or individuals, towards balancing disparities in
classification performance. We propose two such methods, where information
collection is adapted to group- and individual-level needs respectively. We
show on real-world datasets that these can achieve: 1) calibration and single
error parity (e.g., equal opportunity); and 2) parity in both false positive
and false negative rates (i.e., equal odds). Moreover, we show that by
leveraging their additional degree of freedom, active approaches can
substantially outperform randomization-based classifiers previously considered
optimal, while avoiding limitations such as intra-group unfairness.
","[{'version': 'v1', 'created': 'Fri, 28 Sep 2018 18:28:26 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Nov 2018 16:42:51 GMT'}]",2018-11-08,"[['Noriega-Campero', 'Alejandro', ''], ['Bakker', 'Michiel A.', ''], ['Garcia-Bulle', 'Bernardo', ''], ['Pentland', 'Alex', '']]"
1031881,1810.0069399999998,Fabio Massimo Zennaro,"Fabio Massimo Zennaro, Magdalena Ivanovska",Counterfactually Fair Prediction Using Multiple Causal Models,"18 pages, 5 figures, conference paper. arXiv admin note: text overlap
  with arXiv:1805.09866",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we study the problem of making predictions using multiple
structural casual models defined by different agents, under the constraint that
the prediction satisfies the criterion of counterfactual fairness. Relying on
the frameworks of causality, fairness and opinion pooling, we build upon and
extend previous work focusing on the qualitative aggregation of causal Bayesian
networks and causal models. In order to complement previous qualitative
results, we devise a method based on Monte Carlo simulations. This method
enables a decision-maker to aggregate the outputs of the causal models provided
by different experts while guaranteeing the counterfactual fairness of the
result. We demonstrate our approach on a simple, yet illustrative, toy case
study.
","[{'version': 'v1', 'created': 'Mon, 1 Oct 2018 13:11:27 GMT'}]",2018-10-02,"[['Zennaro', 'Fabio Massimo', ''], ['Ivanovska', 'Magdalena', '']]"
1032916,1810.0172899999998,Jean Michel Loubes,"Philippe Besse, Celine Castets-Renard, Aurelien Garivier, Jean-Michel
  Loubes",Can everyday AI be ethical. Fairness of Machine Learning Algorithms,"in French. L'IA du quotidien peut-elle \^etre \'ethique. Loyaut\'e
  des Algorithmes d'apprentissage automatique",,,,stat.OT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Combining big data and machine learning algorithms, the power of automatic
decision tools induces as much hope as fear. Many recently enacted European
legislation (GDPR) and French laws attempt to regulate the use of these tools.
Leaving aside the well-identified problems of data confidentiality and
impediments to competition, we focus on the risks of discrimination, the
problems of transparency and the quality of algorithmic decisions. The detailed
perspective of the legal texts, faced with the complexity and opacity of the
learning algorithms, reveals the need for important technological disruptions
for the detection or reduction of the discrimination risk, and for addressing
the right to obtain an explanation of the auto- matic decision. Since trust of
the developers and above all of the users (citizens, litigants, customers) is
essential, algorithms exploiting personal data must be deployed in a strict
ethical framework. In conclusion, to answer this need, we list some ways of
controls to be developed: institutional control, ethical charter, external
audit attached to the issue of a label.
","[{'version': 'v1', 'created': 'Wed, 3 Oct 2018 13:30:47 GMT'}]",2018-10-04,"[['Besse', 'Philippe', ''], ['Castets-Renard', 'Celine', ''], ['Garivier', 'Aurelien', ''], ['Loubes', 'Jean-Michel', '']]"
1033130,1810.01943,Michael Hind,"Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman,
  Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep
  Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy,
  John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R.
  Varshney, Yunfeng Zhang","AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and
  Mitigating Unwanted Algorithmic Bias",20 pages,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness is an increasingly important concern as machine learning models are
used to support decision making in high-stakes applications such as mortgage
lending, hiring, and prison sentencing. This paper introduces a new open source
Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released
under an Apache v2.0 license {https://github.com/ibm/aif360). The main
objectives of this toolkit are to help facilitate the transition of fairness
research algorithms to use in an industrial setting and to provide a common
framework for fairness researchers to share and evaluate algorithms.
  The package includes a comprehensive set of fairness metrics for datasets and
models, explanations for these metrics, and algorithms to mitigate bias in
datasets and models. It also includes an interactive Web experience
(https://aif360.mybluemix.net) that provides a gentle introduction to the
concepts and capabilities for line-of-business users, as well as extensive
documentation, usage guidance, and industry-specific tutorials to enable data
scientists and practitioners to incorporate the most appropriate tool for their
problem into their work products. The architecture of the package has been
engineered to conform to a standard paradigm used in data science, thereby
further improving usability for practitioners. Such architectural design and
abstractions enable researchers and developers to extend the toolkit with their
new algorithms and improvements, and to use it for performance benchmarking. A
built-in testing infrastructure maintains code quality.
","[{'version': 'v1', 'created': 'Wed, 3 Oct 2018 20:18:35 GMT'}]",2018-10-05,"[['Bellamy', 'Rachel K. E.', ''], ['Dey', 'Kuntal', ''], ['Hind', 'Michael', ''], ['Hoffman', 'Samuel C.', ''], ['Houde', 'Stephanie', ''], ['Kannan', 'Kalapriya', ''], ['Lohia', 'Pranay', ''], ['Martino', 'Jacquelyn', ''], ['Mehta', 'Sameep', ''], ['Mojsilovic', 'Aleksandra', ''], ['Nagar', 'Seema', ''], ['Ramamurthy', 'Karthikeyan Natesan', ''], ['Richards', 'John', ''], ['Saha', 'Diptikalyan', ''], ['Sattigeri', 'Prasanna', ''], ['Singh', 'Moninder', ''], ['Varshney', 'Kush R.', ''], ['Zhang', 'Yunfeng', '']]"
1036228,1810.05041,Jack Fitzsimons,"Jack Fitzsimons, AbdulRahman Al Ali, Michael Osborne and Stephen
  Roberts",A General Framework for Fair Regression,"8 pages, 4 figures, 2 pages references",,10.3390/e21080741,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness, through its many forms and definitions, has become an important
issue facing the machine learning community. In this work, we consider how to
incorporate group fairness constraints in kernel regression methods, applicable
to Gaussian processes, support vector machines, neural network regression and
decision tree regression. Further, we focus on examining the effect of
incorporating these constraints in decision tree regression, with direct
applications to random forests and boosted trees amongst other widespread
popular inference techniques. We show that the order of complexity of memory
and computation is preserved for such models and tightly bound the expected
perturbations to the model in terms of the number of leaves of the trees.
Importantly, the approach works on trained models and hence can be easily
applied to models in current use and group labels are only required on training
data.
","[{'version': 'v1', 'created': 'Wed, 10 Oct 2018 16:16:03 GMT'}, {'version': 'v2', 'created': 'Sat, 2 Feb 2019 08:09:20 GMT'}]",2019-09-04,"[['Fitzsimons', 'Jack', ''], ['Ali', 'AbdulRahman Al', ''], ['Osborne', 'Michael', ''], ['Roberts', 'Stephen', '']]"
1039727,1810.0854,Ansh Patel,Ansh Patel,"Fairness for Whom? Critically reframing fairness with Nash Welfare
  Product",Submitted to FAT* 2019,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies on disparate impact in machine learning applications have
sparked a debate around the concept of fairness along with attempts to
formalize its different criteria. Many of these approaches focus on reducing
prediction errors while maximizing sole utility of the institution. This work
seeks to reconceptualize and critically frame the existing discourse on
fairness by underlining the implicit biases embedded in common understandings
of fairness in the literature and how they contrast with its corresponding
economic and legal definitions. This paper expands the concept of utility and
fairness by bringing in concepts from established literature in welfare
economics and game theory. We then translate these concepts for the algorithmic
prediction domain by defining a formalization of Nash Welfare Product that
seeks to expand utility by collapsing that of the institution using the
prediction tool and the individual subject to the prediction into one function.
We then apply a modulating function that makes the fairness and welfare
trade-offs explicit based on designated policy goals and then apply it to a
temporal model to take into account the effects of decisions beyond the scope
of one-shot predictions. We apply this on a binary classification problem and
present results of a multi-epoch simulation based on the UCI Adult Income
dataset and a test case analysis of the ProPublica recidivism dataset that show
that expanding the concept of utility results in a fairer distribution
correcting for the embedded biases in the dataset without sacrificing the
classifier accuracy.
","[{'version': 'v1', 'created': 'Fri, 19 Oct 2018 15:12:56 GMT'}]",2018-10-22,"[['Patel', 'Ansh', '']]"
1044501,1810.13314,Naman Goel,Naman Goel and Boi Faltings,"Crowdsourcing with Fairness, Diversity and Budget Constraints",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have shown that the labels collected from crowdworkers can be
discriminatory with respect to sensitive attributes such as gender and race.
This raises questions about the suitability of using crowdsourced data for
further use, such as for training machine learning algorithms. In this work, we
address the problem of fair and diverse data collection from a crowd under
budget constraints. We propose a novel algorithm which maximizes the expected
accuracy of the collected data, while ensuring that the errors satisfy desired
notions of fairness. We provide guarantees on the performance of our algorithm
and show that the algorithm performs well in practice through experiments on a
real dataset.
","[{'version': 'v1', 'created': 'Wed, 31 Oct 2018 14:46:17 GMT'}, {'version': 'v2', 'created': 'Fri, 1 Mar 2019 13:29:53 GMT'}]",2019-03-04,"[['Goel', 'Naman', ''], ['Faltings', 'Boi', '']]"
1046113,1811.0148,Jixue Liu,"Jixue Liu, Jiuyong Li, Lin Liu, Thuc Duy Le, Feiyue Ye, Gefei Li",FairMod - Making Predictive Models Discrimination Aware,,,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Predictive models such as decision trees and neural networks may produce
discrimination in their predictions. This paper proposes a method to
post-process the predictions of a predictive model to make the processed
predictions non-discriminatory. The method considers multiple protected
variables together. Multiple protected variables make the problem more
challenging than a simple protected variable. The method uses a well-cited
discrimination metric and adapts it to allow the specification of explanatory
variables, such as position, profession, education, that describe the contexts
of the applications. It models the post-processing of predictions problem as a
nonlinear optimization problem to find best adjustments to the predictions so
that the discrimination constraints of all protected variables are all met at
the same time. The proposed method is independent of classification methods. It
can handle the cases that existing methods cannot handle: satisfying multiple
protected attributes at the same time, allowing multiple explanatory
attributes, and being independent of classification model types. An evaluation
using four real world data sets shows that the proposed method is as
effectively as existing methods, in addition to its extra power.
","[{'version': 'v1', 'created': 'Mon, 5 Nov 2018 02:07:03 GMT'}]",2018-11-06,"[['Liu', 'Jixue', ''], ['Li', 'Jiuyong', ''], ['Liu', 'Lin', ''], ['Le', 'Thuc Duy', ''], ['Ye', 'Feiyue', ''], ['Li', 'Gefei', '']]"
1048287,1811.0365399999998,Yang Liu,"Nripsuta Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David
  Parkes, Yang Liu","How Do Fairness Definitions Fare? Examining Public Attitudes Towards
  Algorithmic Definitions of Fairness",To appear at AI Ethics and Society (AIES) 2019,,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  What is the best way to define algorithmic fairness? While many definitions
of fairness have been proposed in the computer science literature, there is no
clear agreement over a particular definition. In this work, we investigate
ordinary people's perceptions of three of these fairness definitions. Across
two online experiments, we test which definitions people perceive to be the
fairest in the context of loan decisions, and whether fairness perceptions
change with the addition of sensitive information (i.e., race of the loan
applicants). Overall, one definition (calibrated fairness) tends to be more
preferred than the others, and the results also provide support for the
principle of affirmative action.
","[{'version': 'v1', 'created': 'Thu, 8 Nov 2018 19:21:14 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Jan 2019 19:56:07 GMT'}]",2019-01-29,"[['Saxena', 'Nripsuta', ''], ['Huang', 'Karen', ''], ['DeFilippis', 'Evan', ''], ['Radanovic', 'Goran', ''], ['Parkes', 'David', ''], ['Liu', 'Yang', '']]"
1050210,1811.05577,Pedro Saleiro,"Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby
  Stevens, Ari Anisfeld, Kit T. Rodolfa, Rayid Ghani",Aequitas: A Bias and Fairness Audit Toolkit,Aequitas website: http://dsapp.uchicago.edu/aequitas,,,,cs.LG cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has raised concerns on the risk of unintended bias in AI systems
being used nowadays that can affect individuals unfairly based on race, gender
or religion, among other possible characteristics. While a lot of bias metrics
and fairness definitions have been proposed in recent years, there is no
consensus on which metric/definition should be used and there are very few
available resources to operationalize them. Therefore, despite recent
awareness, auditing for bias and fairness when developing and deploying AI
systems is not yet a standard practice. We present Aequitas, an open source
bias and fairness audit toolkit that is an intuitive and easy to use addition
to the machine learning workflow, enabling users to seamlessly test models for
several bias and fairness metrics in relation to multiple population
sub-groups. Aequitas facilitates informed and equitable decisions around
developing and deploying algorithmic decision making systems for both data
scientists, machine learning researchers and policymakers.
","[{'version': 'v1', 'created': 'Wed, 14 Nov 2018 00:34:01 GMT'}, {'version': 'v2', 'created': 'Mon, 29 Apr 2019 16:28:23 GMT'}]",2019-04-30,"[['Saleiro', 'Pedro', ''], ['Kuester', 'Benedict', ''], ['Hinkson', 'Loren', ''], ['London', 'Jesse', ''], ['Stevens', 'Abby', ''], ['Anisfeld', 'Ari', ''], ['Rodolfa', 'Kit T.', ''], ['Ghani', 'Rayid', '']]"
1051888,1811.07255,James Foulds,"James Foulds, Rashidul Islam, Kamrun Keya, Shimei Pan",Bayesian Modeling of Intersectional Fairness: The Variance of Bias,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Intersectionality is a framework that analyzes how interlocking systems of
power and oppression affect individuals along overlapping dimensions including
race, gender, sexual orientation, class, and disability. Intersectionality
theory therefore implies it is important that fairness in artificial
intelligence systems be protected with regard to multi-dimensional protected
attributes. However, the measurement of fairness becomes statistically
challenging in the multi-dimensional setting due to data sparsity, which
increases rapidly in the number of dimensions, and in the values per dimension.
We present a Bayesian probabilistic modeling approach for the reliable,
data-efficient estimation of fairness with multi-dimensional protected
attributes, which we apply to two existing intersectional fairness metrics.
Experimental results on census data and the COMPAS criminal justice recidivism
dataset demonstrate the utility of our methodology, and show that Bayesian
methods are valuable for the modeling and measurement of fairness in an
intersectional context.
","[{'version': 'v1', 'created': 'Sun, 18 Nov 2018 01:54:24 GMT'}, {'version': 'v2', 'created': 'Tue, 10 Sep 2019 16:58:05 GMT'}]",2019-09-11,"[['Foulds', 'James', ''], ['Islam', 'Rashidul', ''], ['Keya', 'Kamrun', ''], ['Pan', 'Shimei', '']]"
1054593,1811.0996,Jack Fitzsimons,"Jack Fitzsimons, Michael Osborne and Stephen Roberts",Intersectionality: Multiple Group Fairness in Expectation Constraints,"NeurIPS (previously NIPS) 2018, Workshop on Ethical, Social and
  Governance Issues in AI",,,,stat.ML cs.AI cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Group fairness is an important concern for machine learning researchers,
developers, and regulators. However, the strictness to which models must be
constrained to be considered fair is still under debate. The focus of this work
is on constraining the expected outcome of subpopulations in kernel regression
and, in particular, decision tree regression, with application to random
forests, boosted trees and other ensemble models. While individual constraints
were previously addressed, this work addresses concerns about incorporating
multiple constraints simultaneously. The proposed solution does not affect the
order of computational or memory complexity of the decision trees and is easily
integrated into models post training.
","[{'version': 'v1', 'created': 'Sun, 25 Nov 2018 06:31:13 GMT'}]",2018-11-27,"[['Fitzsimons', 'Jack', ''], ['Osborne', 'Michael', ''], ['Roberts', 'Stephen', '']]"
1054737,1811.1010399999998,Ben Hutchinson,Ben Hutchinson and Margaret Mitchell,50 Years of Test (Un)fairness: Lessons for Machine Learning,"FAT* '19: Conference on Fairness, Accountability, and Transparency
  (FAT* '19), January 29--31, 2019, Atlanta, GA, USA",,10.1145/3287560.3287600,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Quantitative definitions of what is unfair and what is fair have been
introduced in multiple disciplines for well over 50 years, including in
education, hiring, and machine learning. We trace how the notion of fairness
has been defined within the testing communities of education and hiring over
the past half century, exploring the cultural and social context in which
different fairness definitions have emerged. In some cases, earlier definitions
of fairness are similar or identical to definitions of fairness in current
machine learning research, and foreshadow current formal work. In other cases,
insights into what fairness means and how to measure it have largely gone
overlooked. We compare past and current notions of fairness along several
dimensions, including the fairness criteria, the focus of the criteria (e.g., a
test, a model, or its use), the relationship of fairness to individuals,
groups, and subgroups, and the mathematical method for measuring fairness
(e.g., classification, regression). This work points the way towards future
research and measurement of (un)fairness that builds from our modern
understanding of fairness while incorporating insights from the past.
","[{'version': 'v1', 'created': 'Sun, 25 Nov 2018 21:48:19 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Dec 2018 23:18:49 GMT'}]",2018-12-05,"[['Hutchinson', 'Ben', ''], ['Mitchell', 'Margaret', '']]"
1055303,1811.1067,Shari Trewin,Shari Trewin,AI Fairness for People with Disabilities: Point of View,,,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider how fair treatment in society for people with disabilities might
be impacted by the rise in the use of artificial intelligence, and especially
machine learning methods. We argue that fairness for people with disabilities
is different to fairness for other protected attributes such as age, gender or
race. One major difference is the extreme diversity of ways disabilities
manifest, and people adapt. Secondly, disability information is highly
sensitive and not always shared, precisely because of the potential for
discrimination. Given these differences, we explore definitions of fairness and
how well they work in the disability space. Finally, we suggest ways of
approaching fairness for people with disabilities in AI applications.
","[{'version': 'v1', 'created': 'Mon, 26 Nov 2018 20:11:30 GMT'}]",2018-11-28,"[['Trewin', 'Shari', '']]"
1060147,1812.02573,Osbert Bastani,"Osbert Bastani, Xin Zhang, Armando Solar-Lezama",Probabilistic Verification of Fairness Properties via Concentration,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As machine learning systems are increasingly used to make real world legal
and financial decisions, it is of paramount importance that we develop
algorithms to verify that these systems do not discriminate against minorities.
We design a scalable algorithm for verifying fairness specifications. Our
algorithm obtains strong correctness guarantees based on adaptive concentration
inequalities; such inequalities enable our algorithm to adaptively take samples
until it has enough data to make a decision. We implement our algorithm in a
tool called VeriFair, and show that it scales to large machine learning models,
including a deep recurrent neural network that is more than five orders of
magnitude larger than the largest previously-verified neural network. While our
technique only gives probabilistic guarantees due to the use of random samples,
we show that we can choose the probability of error to be extremely small.
","[{'version': 'v1', 'created': 'Sun, 2 Dec 2018 19:54:38 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Dec 2019 17:07:59 GMT'}]",2020-01-01,"[['Bastani', 'Osbert', ''], ['Zhang', 'Xin', ''], ['Solar-Lezama', 'Armando', '']]"
1061792,1812.0421800000001,Jiaming Song,"Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, Stefano
  Ermon",Learning Controllable Fair Representations,"AISTATS 2019, fixed a typo",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning data representations that are transferable and are fair with respect
to certain protected attributes is crucial to reducing unfair decisions while
preserving the utility of the data. We propose an information-theoretically
motivated objective for learning maximally expressive representations subject
to fairness constraints. We demonstrate that a range of existing approaches
optimize approximations to the Lagrangian dual of our objective. In contrast to
these existing approaches, our objective allows the user to control the
fairness of the representations by specifying limits on unfairness. Exploiting
duality, we introduce a method that optimizes the model parameters as well as
the expressiveness-fairness trade-off. Empirical evidence suggests that our
proposed method can balance the trade-off between multiple notions of fairness
and achieves higher expressiveness at a lower computational cost.
","[{'version': 'v1', 'created': 'Tue, 11 Dec 2018 04:44:48 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Feb 2019 09:30:21 GMT'}, {'version': 'v3', 'created': 'Sat, 14 Mar 2020 10:12:56 GMT'}]",2020-03-17,"[['Song', 'Jiaming', ''], ['Kalluri', 'Pratyusha', ''], ['Grover', 'Aditya', ''], ['Zhao', 'Shengjia', ''], ['Ermon', 'Stefano', '']]"
1074109,1901.0456199999999,Alex Beutel,"Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Allison Woodruff,
  Christine Luu, Pierre Kreitmann, Jonathan Bischof, Ed H. Chi","Putting Fairness Principles into Practice: Challenges, Metrics, and
  Improvements",,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As more researchers have become aware of and passionate about algorithmic
fairness, there has been an explosion in papers laying out new metrics,
suggesting algorithms to address issues, and calling attention to issues in
existing applications of machine learning. This research has greatly expanded
our understanding of the concerns and challenges in deploying machine learning,
but there has been much less work in seeing how the rubber meets the road.
  In this paper we provide a case-study on the application of fairness in
machine learning research to a production classification system, and offer new
insights in how to measure and address algorithmic fairness issues. We discuss
open questions in implementing equality of opportunity and describe our
fairness metric, conditional equality, that takes into account distributional
differences. Further, we provide a new approach to improve on the fairness
metric during model training and demonstrate its efficacy in improving
performance for a real-world product
","[{'version': 'v1', 'created': 'Mon, 14 Jan 2019 21:02:29 GMT'}]",2019-01-16,"[['Beutel', 'Alex', ''], ['Chen', 'Jilin', ''], ['Doshi', 'Tulsee', ''], ['Qian', 'Hai', ''], ['Woodruff', 'Allison', ''], ['Luu', 'Christine', ''], ['Kreitmann', 'Pierre', ''], ['Bischof', 'Jonathan', ''], ['Chi', 'Ed H.', '']]"
1080384,1901.10837,Ziyuan Zhong,"Alexandre Louis Lamy, Ziyuan Zhong, Aditya Krishna Menon, Nakul Verma",Noise-tolerant fair classification,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness-aware learning involves designing algorithms that do not
discriminate with respect to some sensitive feature (e.g., race or gender).
Existing work on the problem operates under the assumption that the sensitive
feature available in one's training sample is perfectly reliable. This
assumption may be violated in many real-world cases: for example, respondents
to a survey may choose to conceal or obfuscate their group identity out of fear
of potential discrimination. This poses the question of whether one can still
learn fair classifiers given noisy sensitive features. In this paper, we answer
the question in the affirmative: we show that if one measures fairness using
the mean-difference score, and sensitive features are subject to noise from the
mutually contaminated learning model, then owing to a simple identity we only
need to change the desired fairness-tolerance. The requisite tolerance can be
estimated by leveraging existing noise-rate estimators from the label noise
literature. We finally show that our procedure is empirically effective on two
case-studies involving sensitive feature censoring.
","[{'version': 'v1', 'created': 'Wed, 30 Jan 2019 14:11:05 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Jun 2019 18:08:36 GMT'}, {'version': 'v3', 'created': 'Tue, 10 Dec 2019 07:07:43 GMT'}, {'version': 'v4', 'created': 'Thu, 9 Jan 2020 14:44:05 GMT'}]",2020-01-10,"[['Lamy', 'Alexandre Louis', ''], ['Zhong', 'Ziyuan', ''], ['Menon', 'Aditya Krishna', ''], ['Verma', 'Nakul', '']]"
1088907,1902.07823,Huang Lingxiao,Lingxiao Huang and Nisheeth K. Vishnoi,Stable and Fair Classification,,,,,cs.LG cs.AI cs.CY cs.DS stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fair classification has been a topic of intense study in machine learning,
and several algorithms have been proposed towards this important task. However,
in a recent study, Friedler et al. observed that fair classification algorithms
may not be stable with respect to variations in the training dataset -- a
crucial consideration in several real-world applications. Motivated by their
work, we study the problem of designing classification algorithms that are both
fair and stable. We propose an extended framework based on fair classification
algorithms that are formulated as optimization problems, by introducing a
stability-focused regularization term. Theoretically, we prove a stability
guarantee, that was lacking in fair classification algorithms, and also provide
an accuracy guarantee for our extended framework. Our accuracy guarantee can be
used to inform the selection of the regularization parameter in our framework.
To the best of our knowledge, this is the first work that combines stability
and fairness in automated decision-making tasks. We assess the benefits of our
approach empirically by extending several fair classification algorithms that
are shown to achieve the best balance between fairness and accuracy over the
Adult dataset. Our empirical results show that our framework indeed improves
the stability at only a slight sacrifice in accuracy.
","[{'version': 'v1', 'created': 'Thu, 21 Feb 2019 00:56:14 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Feb 2019 14:15:32 GMT'}, {'version': 'v3', 'created': 'Mon, 13 May 2019 16:22:18 GMT'}, {'version': 'v4', 'created': 'Wed, 9 Sep 2020 12:32:22 GMT'}]",2020-09-10,"[['Huang', 'Lingxiao', ''], ['Vishnoi', 'Nisheeth K.', '']]"
1089367,1902.08283,Babak Salimi,"Babak Salimi, Luke Rodriguez, Bill Howe, Dan Suciu",Capuchin: Causal Database Repair for Algorithmic Fairness,,"Proceedings of the 2019 International Conference on Management of
  Data. ACM, 2019",,,cs.DB cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness is increasingly recognized as a critical component of machine
learning systems. However, it is the underlying data on which these systems are
trained that often reflect discrimination, suggesting a database repair
problem. Existing treatments of fairness rely on statistical correlations that
can be fooled by statistical anomalies, such as Simpson's paradox. Proposals
for causality-based definitions of fairness can correctly model some of these
situations, but they require specification of the underlying causal models. In
this paper, we formalize the situation as a database repair problem, proving
sufficient conditions for fair classifiers in terms of admissible variables as
opposed to a complete causal model. We show that these conditions correctly
capture subtle fairness violations. We then use these conditions as the basis
for database repair algorithms that provide provable fairness guarantees about
classifiers trained on their training labels. We evaluate our algorithms on
real data, demonstrating improvement over the state of the art on multiple
fairness metrics proposed in the literature while retaining high utility.
","[{'version': 'v1', 'created': 'Thu, 21 Feb 2019 22:13:29 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Feb 2019 00:20:52 GMT'}, {'version': 'v3', 'created': 'Mon, 15 Jul 2019 23:34:33 GMT'}, {'version': 'v4', 'created': 'Mon, 22 Jul 2019 01:35:59 GMT'}, {'version': 'v5', 'created': 'Tue, 1 Oct 2019 19:37:23 GMT'}]",2019-10-04,"[['Salimi', 'Babak', ''], ['Rodriguez', 'Luke', ''], ['Howe', 'Bill', ''], ['Suciu', 'Dan', '']]"
1093166,1903.0078,Alex Beutel,"Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz
  Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi, Cristos Goodrow",Fairness in Recommendation Ranking through Pairwise Comparisons,,,,,cs.CY cs.AI cs.IR cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recommender systems are one of the most pervasive applications of machine
learning in industry, with many services using them to match users to products
or information. As such it is important to ask: what are the possible fairness
risks, how can we quantify them, and how should we address them? In this paper
we offer a set of novel metrics for evaluating algorithmic fairness concerns in
recommender systems. In particular we show how measuring fairness based on
pairwise comparisons from randomized experiments provides a tractable means to
reason about fairness in rankings from recommender systems. Building on this
metric, we offer a new regularizer to encourage improving this metric during
model training and thus improve fairness in the resulting rankings. We apply
this pairwise regularization to a large-scale, production recommender system
and show that we are able to significantly improve the system's pairwise
fairness.
","[{'version': 'v1', 'created': 'Sat, 2 Mar 2019 22:29:42 GMT'}]",2019-03-12,"[['Beutel', 'Alex', ''], ['Chen', 'Jilin', ''], ['Doshi', 'Tulsee', ''], ['Qian', 'Hai', ''], ['Wei', 'Li', ''], ['Wu', 'Yi', ''], ['Heldt', 'Lukasz', ''], ['Zhao', 'Zhe', ''], ['Hong', 'Lichan', ''], ['Chi', 'Ed H.', ''], ['Goodrow', 'Cristos', '']]"
1093595,1903.01209,Hoda Heidari,"Hoda Heidari, Vedant Nanda, and Krishna P. Gummadi","On the Long-term Impact of Algorithmic Decision Policies: Effort
  Unfairness and Feature Segregation through Social Learning",,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most existing notions of algorithmic fairness are one-shot: they ensure some
form of allocative equality at the time of decision making, but do not account
for the adverse impact of the algorithmic decisions today on the long-term
welfare and prosperity of certain segments of the population. We take a broader
perspective on algorithmic fairness. We propose an effort-based measure of
fairness and present a data-driven framework for characterizing the long-term
impact of algorithmic policies on reshaping the underlying population.
Motivated by the psychological literature on \emph{social learning} and the
economic literature on equality of opportunity, we propose a micro-scale model
of how individuals may respond to decision-making algorithms. We employ
existing measures of segregation from sociology and economics to quantify the
resulting macro-scale population-level change. Importantly, we observe that
different models may shift the group-conditional distribution of qualifications
in different directions. Our findings raise a number of important questions
regarding the formalization of fairness for decision-making models.
","[{'version': 'v1', 'created': 'Mon, 4 Mar 2019 12:38:00 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jun 2019 13:15:44 GMT'}]",2019-06-28,"[['Heidari', 'Hoda', ''], ['Nanda', 'Vedant', ''], ['Gummadi', 'Krishna P.', '']]"
1099394,1903.07008,Rodrigo Canaan,"Rodrigo Canaan, Christoph Salge, Julian Togelius, Andy Nealen","Leveling the Playing Field -- Fairness in AI Versus Human Game
  Benchmarks",7 pages,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  From the beginning if the history of AI, there has been interest in games as
a platform of research. As the field developed, human-level competence in
complex games became a target researchers worked to reach. Only relatively
recently has this target been finally met for traditional tabletop games such
as Backgammon, Chess and Go. Current research focus has shifted to electronic
games, which provide unique challenges. As is often the case with AI research,
these results are liable to be exaggerated or misrepresented by either authors
or third parties. The extent to which these games benchmark consist of fair
competition between human and AI is also a matter of debate. In this work, we
review the statements made by authors and third parties in the general media
and academic circle about these game benchmark results and discuss factors that
can impact the perception of fairness in the contest between humans and
machines
","[{'version': 'v1', 'created': 'Sun, 17 Mar 2019 00:42:26 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Mar 2019 17:52:49 GMT'}, {'version': 'v3', 'created': 'Sun, 14 Apr 2019 01:20:16 GMT'}, {'version': 'v4', 'created': 'Thu, 29 Aug 2019 16:52:14 GMT'}]",2019-08-30,"[['Canaan', 'Rodrigo', ''], ['Salge', 'Christoph', ''], ['Togelius', 'Julian', ''], ['Nealen', 'Andy', '']]"
1118583,1905.00147,Lily Hu,Lily Hu and Yiling Chen,Fair Classification and Social Welfare,"23 pages, 2 figures",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Now that machine learning algorithms lie at the center of many resource
allocation pipelines, computer scientists have been unwittingly cast as partial
social planners. Given this state of affairs, important questions follow. What
is the relationship between fairness as defined by computer scientists and
notions of social welfare? In this paper, we present a welfare-based analysis
of classification and fairness regimes. We translate a loss minimization
program into a social welfare maximization problem with a set of implied
welfare weights on individuals and groups--weights that can be analyzed from a
distribution justice lens. In the converse direction, we ask what the space of
possible labelings is for a given dataset and hypothesis class. We provide an
algorithm that answers this question with respect to linear hyperplanes in
$\mathbb{R}^d$ that runs in $O(n^dd)$. Our main findings on the relationship
between fairness criteria and welfare center on sensitivity analyses of
fairness-constrained empirical risk minimization programs. We characterize the
ranges of $\Delta \epsilon$ perturbations to a fairness parameter $\epsilon$
that yield better, worse, and neutral outcomes in utility for individuals and
by extension, groups. We show that applying more strict fairness criteria that
are codified as parity constraints, can worsen welfare outcomes for both
groups. More generally, always preferring ""more fair"" classifiers does not
abide by the Pareto Principle---a fundamental axiom of social choice theory and
welfare economics. Recent work in machine learning has rallied around these
notions of fairness as critical to ensuring that algorithmic systems do not
have disparate negative impact on disadvantaged social groups. By showing that
these constraints often fail to translate into improved outcomes for these
groups, we cast doubt on their effectiveness as a means to ensure justice.
","[{'version': 'v1', 'created': 'Wed, 1 May 2019 01:03:07 GMT'}]",2019-05-02,"[['Hu', 'Lily', ''], ['Chen', 'Yiling', '']]"
1125462,1905.0702600000002,Vaishak Belle,"Michael Varley, Vaishak Belle",Fairness in Machine Learning with Tractable Models,"In AAAI Workshop: Statistical Relational Artificial Intelligence
  (StarAI), 2020. (This is the extended version.)",,,,cs.LG cs.AI cs.SC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine Learning techniques have become pervasive across a range of different
applications, and are now widely used in areas as disparate as recidivism
prediction, consumer credit-risk analysis and insurance pricing. The prevalence
of machine learning techniques has raised concerns about the potential for
learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning
about probabilistic events is a challenging one, owing to the intractability of
inference.
  The focus of this paper is taking steps towards the application of tractable
models to fairness. Tractable probabilistic models have emerged that guarantee
that conditional marginal can be computed in time linear in the size of the
model. In particular, we show that sum product networks (SPNs) enable an
effective technique for determining the statistical relationships between
protected attributes and other training variables. If a subset of these
training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train
a classification model without concern that the resulting classifier will
result in disparate outcomes for different demographic groups.
  Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and
female credit applicants, with a small reduction in classification accuracy
compared to state of the art. We will also motivate the concept of ""fairness
through percentile equivalence"", a new definition predicated on the notion that
individuals at the same percentile of their respective distributions should be
treated equivalently, and this prevents unfair penalisation of those
individuals who lie at the extremities of their respective distributions.
","[{'version': 'v1', 'created': 'Thu, 16 May 2019 20:31:26 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jan 2020 13:25:33 GMT'}]",2020-01-14,"[['Varley', 'Michael', ''], ['Belle', 'Vaishak', '']]"
1129110,1905.10674,William L Hamilton,"Avishek Joey Bose, William L. Hamilton",Compositional Fairness Constraints for Graph Embeddings,"Proceedings of the 36th International Conference on Machine Learning,
  Long Beach, California, PMLR 97, 2019",,,,cs.LG cs.AI stat.ML,http://creativecommons.org/publicdomain/zero/1.0/,"  Learning high-quality node embeddings is a key building block for machine
learning models that operate on graph data, such as social networks and
recommender systems. However, existing graph embedding techniques are unable to
cope with fairness constraints, e.g., ensuring that the learned representations
do not correlate with certain attributes, such as age or gender. Here, we
introduce an adversarial framework to enforce fairness constraints on graph
embeddings. Our approach is compositional---meaning that it can flexibly
accommodate different combinations of fairness constraints during inference.
For instance, in the context of social recommendations, our framework would
allow one user to request that their recommendations are invariant to both
their age and gender, while also allowing another user to request invariance to
just their age. Experiments on standard knowledge graph and recommender system
benchmarks highlight the utility of our proposed framework.
","[{'version': 'v1', 'created': 'Sat, 25 May 2019 21:13:27 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Jun 2019 22:20:27 GMT'}, {'version': 'v3', 'created': 'Fri, 5 Jul 2019 16:42:35 GMT'}, {'version': 'v4', 'created': 'Tue, 16 Jul 2019 22:11:21 GMT'}]",2019-07-18,"[['Bose', 'Avishek Joey', ''], ['Hamilton', 'William L.', '']]"
1131164,1905.1272800000002,Fernando Mart\'inez Plumed,"Fernando Mart\'inez-Plumed, C\`esar Ferri, David Nieves, Jos\'e
  Hern\'andez-Orallo",Fairness and Missing Values,Preprint submitted to Decision Support Systems Journal,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The causes underlying unfair decision making are complex, being internalised
in different ways by decision makers, other actors dealing with data and
models, and ultimately by the individuals being affected by these decisions.
One frequent manifestation of all these latent causes arises in the form of
missing values: protected groups are more reluctant to give information that
could be used against them, delicate information for some groups can be erased
by human operators, or data acquisition may simply be less complete and
systematic for minority groups. As a result, missing values and bias in data
are two phenomena that are tightly coupled. However, most recent techniques,
libraries and experimental results dealing with fairness in machine learning
have simply ignored missing data. In this paper, we claim that fairness
research should not miss the opportunity to deal properly with missing data. To
support this claim, (1) we analyse the sources of missing data and bias, and we
map the common causes, (2) we find that rows containing missing values are
usually fairer than the rest, which should not be treated as the uncomfortable
ugly data that different techniques and libraries get rid of at the first
occasion, and (3) we study the trade-off between performance and fairness when
the rows with missing values are used (either because the technique deals with
them directly or by imputation methods). We end the paper with a series of
recommended procedures about what to do with missing data when aiming for fair
decision making.
","[{'version': 'v1', 'created': 'Wed, 29 May 2019 21:09:20 GMT'}]",2019-05-31,"[['Martínez-Plumed', 'Fernando', ''], ['Ferri', 'Cèsar', ''], ['Nieves', 'David', ''], ['Hernández-Orallo', 'José', '']]"
1132314,1906.0012800000002,Boli Fang,"Boli Fang, Miao Jiang, Jerry Shen","Achieving Fairness in Determining Medicaid Eligibility through Fairgroup
  Construction",,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Effective complements to human judgment, artificial intelligence techniques
have started to aid human decisions in complicated social problems across the
world. In the context of United States for instance, automated ML/DL
classification models offer complements to human decisions in determining
Medicaid eligibility. However, given the limitations in ML/DL model design,
these algorithms may fail to leverage various factors for decision making,
resulting in improper decisions that allocate resources to individuals who may
not be in the most need. In view of such an issue, we propose in this paper the
method of \textit{fairgroup construction}, based on the legal doctrine of
\textit{disparate impact}, to improve the fairness of regressive classifiers.
Experiments on American Community Survey dataset demonstrate that our method
could be easily adapted to a variety of regressive classification models to
boost their fairness in deciding Medicaid Eligibility, while maintaining high
levels of classification accuracy.
","[{'version': 'v1', 'created': 'Sat, 1 Jun 2019 01:54:19 GMT'}]",2019-06-04,"[['Fang', 'Boli', ''], ['Jiang', 'Miao', ''], ['Shen', 'Jerry', '']]"
1133353,1906.0116699999999,Lingjuan Lyu,"Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma,
  Jiong Jin, Han Yu, and Kee Siong Ng",Towards Fair and Privacy-Preserving Federated Deep Models,Accepted for publication in TPDS,,,,cs.CR cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The current standalone deep learning framework tends to result in overfitting
and low utility. This problem can be addressed by either a centralized
framework that deploys a central server to train a global model on the joint
data from all parties, or a distributed framework that leverages a parameter
server to aggregate local model updates. Server-based solutions are prone to
the problem of a single-point-of-failure. In this respect, collaborative
learning frameworks, such as federated learning (FL), are more robust. Existing
federated learning frameworks overlook an important aspect of participation:
fairness. All parties are given the same final model without regard to their
contributions. To address these issues, we propose a decentralized Fair and
Privacy-Preserving Deep Learning (FPPDL) framework to incorporate fairness into
federated deep learning models. In particular, we design a local credibility
mutual evaluation mechanism to guarantee fairness, and a three-layer
onion-style encryption scheme to guarantee both accuracy and privacy. Different
from existing FL paradigm, under FPPDL, each participant receives a different
version of the FL model with performance commensurate with his contributions.
Experiments on benchmark datasets demonstrate that FPPDL balances fairness,
privacy and accuracy. It enables federated learning ecosystems to detect and
isolate low-contribution parties, thereby promoting responsible participation.
","[{'version': 'v1', 'created': 'Tue, 4 Jun 2019 02:43:42 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Oct 2019 01:09:35 GMT'}, {'version': 'v3', 'created': 'Tue, 19 May 2020 10:43:55 GMT'}]",2020-05-20,"[['Lyu', 'Lingjuan', ''], ['Yu', 'Jiangshan', ''], ['Nandakumar', 'Karthik', ''], ['Li', 'Yitong', ''], ['Ma', 'Xingjun', ''], ['Jin', 'Jiong', ''], ['Yu', 'Han', ''], ['Ng', 'Kee Siong', '']]"
1134775,1906.02589,Elliot Creager,"Elliot Creager, David Madras, J\""orn-Henrik Jacobsen, Marissa A. Weis,
  Kevin Swersky, Toniann Pitassi, Richard Zemel",Flexibly Fair Representation Learning by Disentanglement,,"Proceedings of the International Conference on Machine Learning
  (ICML), 2019",,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of learning representations that achieve group and
subgroup fairness with respect to multiple sensitive attributes. Taking
inspiration from the disentangled representation learning literature, we
propose an algorithm for learning compact representations of datasets that are
useful for reconstruction and prediction, but are also \emph{flexibly fair},
meaning they can be easily modified at test time to achieve subgroup
demographic parity with respect to multiple sensitive attributes and their
conjunctions. We show empirically that the resulting encoder---which does not
require the sensitive attributes for inference---enables the adaptation of a
single representation to a variety of fair classification tasks with new target
labels and subgroup definitions.
","[{'version': 'v1', 'created': 'Thu, 6 Jun 2019 13:56:24 GMT'}]",2019-06-07,"[['Creager', 'Elliot', ''], ['Madras', 'David', ''], ['Jacobsen', 'Jörn-Henrik', ''], ['Weis', 'Marissa A.', ''], ['Swersky', 'Kevin', ''], ['Pitassi', 'Toniann', ''], ['Zemel', 'Richard', '']]"
1134961,1906.02775,Alexander Peysakhovich,"Alexander Peysakhovich, Christian Kroer",Fair Division Without Disparate Impact,,,,,cs.GT cs.AI cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of dividing items between individuals in a way that
is fair both in the sense of distributional fairness and in the sense of not
having disparate impact across protected classes. An important existing
mechanism for distributionally fair division is competitive equilibrium from
equal incomes (CEEI). Unfortunately, CEEI will not, in general, respect
disparate impact constraints. We consider two types of disparate impact
measures: requiring that allocations be similar across protected classes and
requiring that average utility levels be similar across protected classes. We
modify the standard CEEI algorithm in two ways: equitable equilibrium from
equal incomes, which removes disparate impact in allocations, and competitive
equilibrium from equitable incomes which removes disparate impact in attained
utility levels. We show analytically that removing disparate impact in outcomes
breaks several of CEEI's desirable properties such as envy, regret, Pareto
optimality, and incentive compatibility. By contrast, we can remove disparate
impact in attained utility levels without affecting these properties. Finally,
we experimentally evaluate the tradeoffs between efficiency, equity, and
disparate impact in a recommender-system based market.
","[{'version': 'v1', 'created': 'Thu, 6 Jun 2019 18:56:17 GMT'}]",2019-06-10,"[['Peysakhovich', 'Alexander', ''], ['Kroer', 'Christian', '']]"
1136029,1906.03843,YooJung Choi,"YooJung Choi, Golnoosh Farnadi, Behrouz Babaki, Guy Van den Broeck","Learning Fair Naive Bayes Classifiers by Discovering and Eliminating
  Discrimination Patterns",,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As machine learning is increasingly used to make real-world decisions, recent
research efforts aim to define and ensure fairness in algorithmic decision
making. Existing methods often assume a fixed set of observable features to
define individuals, but lack a discussion of certain features not being
observed at test time. In this paper, we study fairness of naive Bayes
classifiers, which allow partial observations. In particular, we introduce the
notion of a discrimination pattern, which refers to an individual receiving
different classifications depending on whether some sensitive attributes were
observed. Then a model is considered fair if it has no such pattern. We propose
an algorithm to discover and mine for discrimination patterns in a naive Bayes
classifier, and show how to learn maximum likelihood parameters subject to
these fairness constraints. Our approach iteratively discovers and eliminates
discrimination patterns until a fair model is learned. An empirical evaluation
on three real-world datasets demonstrates that we can remove exponentially many
discrimination patterns by only adding a small fraction of them as constraints.
","[{'version': 'v1', 'created': 'Mon, 10 Jun 2019 08:48:19 GMT'}, {'version': 'v2', 'created': 'Thu, 7 May 2020 22:46:36 GMT'}]",2020-05-11,"[['Choi', 'YooJung', ''], ['Farnadi', 'Golnoosh', ''], ['Babaki', 'Behrouz', ''], ['Broeck', 'Guy Van den', '']]"
1136149,1906.03963,Moin Hussain Moti,"Moin Hussain Moti, Dimitris Chatzopoulos, Pan Hui, Sujit Gujar","FaRM: Fair Reward Mechanism for Information Aggregation in Spontaneous
  Localized Settings (Extended Version)","13 pages, IJCAI Main Track Extended version of ""FaRM: Fair Reward
  Mechanism for Information Aggregation in Spontaneous Localized Settings""",,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although peer prediction markets are widely used in crowdsourcing to
aggregate information from agents, they often fail to reward the participating
agents equitably. Honest agents can be wrongly penalized if randomly paired
with dishonest ones. In this work, we introduce \emph{selective} and
\emph{cumulative} fairness. We characterize a mechanism as fair if it satisfies
both notions and present FaRM, a representative mechanism we designed. FaRM is
a Nash incentive mechanism that focuses on information aggregation for
spontaneous local activities which are accessible to a limited number of agents
without assuming any prior knowledge of the event. All the agents in the
vicinity observe the same information. FaRM uses \textit{(i)} a \emph{report
strength score} to remove the risk of random pairing with dishonest reporters,
\textit{(ii)} a \emph{consistency score} to measure an agent's history of
accurate reports and distinguish valuable reports, \textit{(iii)} a
\emph{reliability score} to estimate the probability of an agent to collude
with nearby agents and prevents agents from getting swayed, and \textit{(iv)} a
\emph{location robustness score} to filter agents who try to participate
without being present in the considered setting. Together, report strength,
consistency, and reliability represent a fair reward given to agents based on
their reports.
","[{'version': 'v1', 'created': 'Mon, 10 Jun 2019 13:21:10 GMT'}]",2019-06-11,"[['Moti', 'Moin Hussain', ''], ['Chatzopoulos', 'Dimitris', ''], ['Hui', 'Pan', ''], ['Gujar', 'Sujit', '']]"
1140572,1906.0838600000002,Han Zhao,"Han Zhao, Geoffrey J. Gordon",Inherent Tradeoffs in Learning Fair Representations,A tighter bound on accuracy parity in Theorem 3.3,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the prevalence of machine learning in high-stakes applications,
especially the ones regulated by anti-discrimination laws or societal norms, it
is crucial to ensure that the predictive models do not propagate any existing
bias or discrimination. Due to the ability of deep neural nets to learn rich
representations, recent advances in algorithmic fairness have focused on
learning fair representations with adversarial techniques to reduce bias in
data while preserving utility simultaneously. In this paper, through the lens
of information theory, we provide the first result that quantitatively
characterizes the tradeoff between demographic parity and the joint utility
across different population groups. Specifically, when the base rates differ
between groups, we show that any method aiming to learn fair representations
admits an information-theoretic lower bound on the joint error across these
groups. To complement our negative results, we also prove that if the optimal
decision functions across different groups are close, then learning fair
representations leads to an alternative notion of fairness, known as the
accuracy parity, which states that the error rates are close between groups.
Finally, our theoretical findings are also confirmed empirically on real-world
datasets.
","[{'version': 'v1', 'created': 'Wed, 19 Jun 2019 22:44:14 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Oct 2019 18:49:45 GMT'}, {'version': 'v3', 'created': 'Sun, 7 Jun 2020 18:54:23 GMT'}]",2020-06-09,"[['Zhao', 'Han', ''], ['Gordon', 'Geoffrey J.', '']]"
1143519,1906.1133300000001,Daniel Gilbert,"Benjamin R. Baer, Daniel E. Gilbert and Martin T. Wells",Fairness criteria through the lens of directed acyclic graphical models,,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A substantial portion of the literature on fairness in algorithms proposes,
analyzes, and operationalizes simple formulaic criteria for assessing fairness.
Two of these criteria, Equalized Odds and Calibration by Group, have gained
significant attention for their simplicity and intuitive appeal, but also for
their incompatibility. This chapter provides a perspective on the meaning and
consequences of these and other fairness criteria using graphical models which
reveals Equalized Odds and related criteria to be ultimately misleading. An
assessment of various graphical models suggests that fairness criteria should
ultimately be case-specific and sensitive to the nature of the information the
algorithm processes.
","[{'version': 'v1', 'created': 'Wed, 26 Jun 2019 20:21:39 GMT'}]",2019-06-28,"[['Baer', 'Benjamin R.', ''], ['Gilbert', 'Daniel E.', ''], ['Wells', 'Martin T.', '']]"
1144849,1907.00313,Stefanos Nikolaidis,"Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, Stefanos
  Nikolaidis","Reinforcement Learning with Fairness Constraints for Resource
  Distribution in Human-Robot Teams",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Much work in robotics and operations research has focused on optimal resource
distribution, where an agent dynamically decides how to sequentially distribute
resources among different candidates. However, most work ignores the notion of
fairness in candidate selection. In the case where a robot distributes
resources to human team members, disproportionately favoring the highest
performing teammate can have negative effects in team dynamics and system
acceptance. We introduce a multi-armed bandit algorithm with fairness
constraints, where a robot distributes resources to human teammates of
different skill levels. In this problem, the robot does not know the skill
level of each human teammate, but learns it by observing their performance over
time. We define fairness as a constraint on the minimum rate that each human
teammate is selected throughout the task. We provide theoretical guarantees on
performance and perform a large-scale user study, where we adjust the level of
fairness in our algorithm. Results show that fairness in resource distribution
has a significant effect on users' trust in the system.
","[{'version': 'v1', 'created': 'Sun, 30 Jun 2019 03:41:05 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Jul 2019 02:06:54 GMT'}]",2019-07-09,"[['Claure', 'Houston', ''], ['Chen', 'Yifang', ''], ['Modi', 'Jignesh', ''], ['Jung', 'Malte', ''], ['Nikolaidis', 'Stefanos', '']]"
1146381,1907.01845,Xiangxiang Chu,Xiangxiang Chu and Bo Zhang and Ruijun Xu and Jixiang Li,"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural
  Architecture Search","Talk about the fairness of WS NAS and analyze the mechanisms of
  single path training",,,,cs.LG cs.AI cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One of the most critical problems in two-stage weight-sharing neural
architecture search is the evaluation of candidate models. A faithful ranking
certainly leads to accurate searching results. However, current methods are
prone to making misjudgments. In this paper, we prove that they inevitably give
biased evaluations due to inherent unfairness in the supernet training. In view
of this, we propose two levels of constraints: expectation fairness and strict
fairness. Particularly, strict fairness ensures equal optimization
opportunities for all choice blocks throughout the training, which neither
overestimates nor underestimates their capacity. We demonstrate this is crucial
to improving confidence in models' ranking. Incorporating our supernet trained
under fairness constraints with a multi-objective evolutionary search
algorithm, we obtain various state-of-the-art models on ImageNet. Especially,
FairNAS-A attains 77.5% top-1 accuracy. The models and their evaluation codes
are made publicly available online http://github.com/fairnas/FairNAS .
","[{'version': 'v1', 'created': 'Wed, 3 Jul 2019 10:50:38 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Aug 2019 06:16:45 GMT'}, {'version': 'v3', 'created': 'Wed, 20 Nov 2019 06:41:45 GMT'}, {'version': 'v4', 'created': 'Tue, 10 Mar 2020 11:53:02 GMT'}]",2020-03-11,"[['Chu', 'Xiangxiang', ''], ['Zhang', 'Bo', ''], ['Xu', 'Ruijun', ''], ['Li', 'Jixiang', '']]"
1146763,1907.02227,Anhong Guo,"Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna Wallach,
  Meredith Ringel Morris",Toward Fairness in AI for People with Disabilities: A Research Roadmap,ACM ASSETS 2019 Workshop on AI Fairness for People with Disabilities,,,,cs.CY cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  AI technologies have the potential to dramatically impact the lives of people
with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for
many state-of-the-art AI systems, such as automated speech recognition tools
that can caption videos for people who are deaf and hard of hearing, or
language prediction algorithms that can augment communication for people with
speech or cognitive disabilities. However, widely deployed AI systems may not
work properly for PWD, or worse, may actively discriminate against them. These
considerations regarding fairness in AI for PWD have thus far received little
attention. In this position paper, we identify potential areas of concern
regarding how several AI technology categories may impact particular disability
constituencies if care is not taken in their design, development, and testing.
We intend for this risk assessment of how various classes of AI might interact
with various classes of disability to provide a roadmap for future research
that is needed to gather data, test these hypotheses, and build more inclusive
algorithms.
","[{'version': 'v1', 'created': 'Thu, 4 Jul 2019 05:29:49 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Aug 2019 18:39:41 GMT'}]",2019-08-06,"[['Guo', 'Anhong', ''], ['Kamar', 'Ece', ''], ['Vaughan', 'Jennifer Wortman', ''], ['Wallach', 'Hanna', ''], ['Morris', 'Meredith Ringel', '']]"
1148363,1907.03827,An Yan,"An Yan, Bill Howe","FairST: Equitable Spatial and Temporal Demand Prediction for New
  Mobility Systems",,,,,cs.CY cs.AI cs.LG stat.AP,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Emerging transportation modes, including car-sharing, bike-sharing, and
ride-hailing, are transforming urban mobility but have been shown to reinforce
socioeconomic inequities. Spatiotemporal demand prediction models for these new
mobility regimes must therefore consider fairness as a first-class design
requirement. We present FairST, a fairness-aware model for predicting demand
for new mobility systems. Our approach utilizes 1D, 2D and 3D convolutions to
integrate various urban features and learn the spatial-temporal dynamics of a
mobility system, but we include fairness metrics as a form of regularization to
make the predictions more equitable across demographic groups. We propose two
novel spatiotemporal fairness metrics, a region-based fairness gap (RFG) and an
individual-based fairness gap (IFG). Both quantify equity in a spatiotemporal
context, but vary by whether demographics are labeled at the region level (RFG)
or whether population distribution information is available (IFG). Experimental
results on real bike share and ride share datasets demonstrate the
effectiveness of the proposed model: FairST not only reduces the fairness gap
by more than 80%, but can surprisingly achieve better accuracy than
state-of-the-art yet fairness-oblivious methods including LSTMs, ConvLSTMs, and
3D CNN.
","[{'version': 'v1', 'created': 'Fri, 21 Jun 2019 19:33:16 GMT'}]",2019-07-10,"[['Yan', 'An', ''], ['Howe', 'Bill', '']]"
1151759,1907.07223,Vasileios Iosifidis,"Vasileios Iosifidis, Thi Ngoc Han Tran, Eirini Ntoutsi",Fairness-enhancing interventions in stream classification,"15 pages, 7 figures. To appear in the proceedings of 30th
  International Conference on Database and Expert Systems Applications, Linz,
  Austria August 26 - 29, 2019",,10.1007/978-3-030-27615-7_20,,cs.LG cs.AI stat.ML,http://creativecommons.org/licenses/by/4.0/,"  The wide spread usage of automated data-driven decision support systems has
raised a lot of concerns regarding accountability and fairness of the employed
models in the absence of human supervision. Existing fairness-aware approaches
tackle fairness as a batch learning problem and aim at learning a fair model
which can then be applied to future instances of the problem. In many
applications, however, the data comes sequentially and its characteristics
might evolve with time. In such a setting, it is counter-intuitive to ""fix"" a
(fair) model over the data stream as changes in the data might incur changes in
the underlying model therefore, affecting its fairness. In this work, we
propose fairness-enhancing interventions that modify the input data so that the
outcome of any stream classifier applied to that data will be fair. Experiments
on real and synthetic data show that our approach achieves good predictive
performance and low discrimination scores over the course of the stream.
","[{'version': 'v1', 'created': 'Tue, 16 Jul 2019 19:27:19 GMT'}]",2020-01-24,"[['Iosifidis', 'Vasileios', ''], ['Tran', 'Thi Ngoc Han', ''], ['Ntoutsi', 'Eirini', '']]"
1151773,1907.0723699999999,Wenbin Zhang,Wenbin Zhang and Eirini Ntoutsi,FAHT: An Adaptive Fairness-aware Decision Tree Classifier,Accepted to IJCAI 2019,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated data-driven decision-making systems are ubiquitous across a wide
spread of online as well as offline services. These systems, depend on
sophisticated learning algorithms and available data, to optimize the service
function for decision support assistance. However, there is a growing concern
about the accountability and fairness of the employed models by the fact that
often the available historic data is intrinsically discriminatory, i.e., the
proportion of members sharing one or more sensitive attributes is higher than
the proportion in the population as a whole when receiving positive
classification, which leads to a lack of fairness in decision support system. A
number of fairness-aware learning methods have been proposed to handle this
concern. However, these methods tackle fairness as a static problem and do not
take the evolution of the underlying stream population into consideration. In
this paper, we introduce a learning mechanism to design a fair classifier for
online stream based decision-making. Our learning model, FAHT (Fairness-Aware
Hoeffding Tree), is an extension of the well-known Hoeffding Tree algorithm for
decision tree induction over streams, that also accounts for fairness. Our
experiments show that our algorithm is able to deal with discrimination in
streaming environments, while maintaining a moderate predictive performance
over the stream.
","[{'version': 'v1', 'created': 'Tue, 16 Jul 2019 20:00:41 GMT'}]",2019-07-18,"[['Zhang', 'Wenbin', ''], ['Ntoutsi', 'Eirini', '']]"
1154859,1907.1032300000002,Paul Weng,Paul Weng,Fairness in Reinforcement Learning,Presented at the AI for Social Good Workshop at IJCAI 2019,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decision support systems (e.g., for ecological conservation) and autonomous
systems (e.g., adaptive controllers in smart cities) start to be deployed in
real applications. Although their operations often impact many users or
stakeholders, no fairness consideration is generally taken into account in
their design, which could lead to completely unfair outcomes for some users or
stakeholders. To tackle this issue, we advocate for the use of social welfare
functions that encode fairness and present this general novel problem in the
context of (deep) reinforcement learning, although it could possibly be
extended to other machine learning tasks.
","[{'version': 'v1', 'created': 'Wed, 24 Jul 2019 09:27:11 GMT'}]",2019-07-25,"[['Weng', 'Paul', '']]"
1159194,1908.0102399999998,Os Keyes,"Cynthia L. Bennett, Os Keyes","What is the Point of Fairness? Disability, AI and The Complexity of
  Justice",,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Work integrating conversations around AI and Disability is vital and valued,
particularly when done through a lens of fairness. Yet at the same time,
analyzing the ethical implications of AI for disabled people solely through the
lens of a singular idea of ""fairness"" risks reinforcing existing power
dynamics, either through reinforcing the position of existing medical
gatekeepers, or promoting tools and techniques that benefit
otherwise-privileged disabled people while harming those who are rendered
outliers in multiple ways. In this paper we present two case studies from
within computer vision - a subdiscipline of AI focused on training algorithms
that can ""see"" - of technologies putatively intended to help disabled people
but, through failures to consider structural injustices in their design, are
likely to result in harms not addressed by a ""fairness"" framing of ethics.
Drawing on disability studies and critical data science, we call on researchers
into AI ethics and disability to move beyond simplistic notions of fairness,
and towards notions of justice.
","[{'version': 'v1', 'created': 'Fri, 2 Aug 2019 19:25:57 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Aug 2019 21:54:56 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Aug 2019 23:17:52 GMT'}]",2019-08-13,"[['Bennett', 'Cynthia L.', ''], ['Keyes', 'Os', '']]"
1167013,1908.08843,Mengnan Du,"Mengnan Du, Fan Yang, Na Zou, Xia Hu",Fairness in Deep Learning: A Computational Perspective,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning is increasingly being used in high-stake decision making
applications that affect individual lives. However, deep learning models might
exhibit algorithmic discrimination behaviors with respect to protected groups,
potentially posing negative impacts on individuals and society. Therefore,
fairness in deep learning has attracted tremendous attention recently. We
provide a review covering recent progresses to tackle algorithmic fairness
problems of deep learning from the computational perspective. Specifically, we
show that interpretability can serve as a useful ingredient to diagnose the
reasons that lead to algorithmic discrimination. We also discuss fairness
mitigation approaches categorized according to three stages of deep learning
life-cycle, aiming to push forward the area of fairness in deep learning and
build genuinely fair and reliable deep learning systems.
","[{'version': 'v1', 'created': 'Fri, 23 Aug 2019 14:38:07 GMT'}, {'version': 'v2', 'created': 'Thu, 19 Mar 2020 02:33:17 GMT'}]",2020-03-20,"[['Du', 'Mengnan', ''], ['Yang', 'Fan', ''], ['Zou', 'Na', ''], ['Hu', 'Xia', '']]"
1169621,1908.11451,Mahnaz Sadat Qafari,Mahnaz Sadat Qafari and Wil van der Aalst,Fairness-Aware Process Mining,,,,,cs.CR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Process mining is a multi-purpose tool enabling organizations to improve
their processes. One of the primary purposes of process mining is finding the
root causes of performance or compliance problems in processes. The usual way
of doing so is by gathering data from the process event log and other sources
and then applying some data mining and machine learning techniques. However,
the results of applying such techniques are not always acceptable. In many
situations, this approach is prone to making obvious or unfair diagnoses and
applying them may result in conclusions that are unsurprising or even
discriminating (e.g., blaming overloaded employees for delays). In this paper,
we present a solution to this problem by creating a fair classifier for such
situations. The undesired effects are removed at the expense of reduction on
the accuracy of the resulting classifier. We have implemented this method as a
plug-in in ProM. Using the implemented plug-in on two real event logs, we
decreased the discrimination caused by the classifier, while losing a small
fraction of its accuracy.
","[{'version': 'v1', 'created': 'Wed, 28 Aug 2019 15:08:34 GMT'}]",2019-09-02,"[['Qafari', 'Mahnaz Sadat', ''], ['van der Aalst', 'Wil', '']]"
1171020,1909.00982,Arpita Biswas,"Arpita Biswas, Siddharth Barman, Amit Deshpande, Amit Sharma",Quantifying Infra-Marginality and Its Trade-off with Group Fairness,,,,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In critical decision-making scenarios, optimizing accuracy can lead to a
biased classifier, hence past work recommends enforcing group-based fairness
metrics in addition to maximizing accuracy. However, doing so exposes the
classifier to another kind of bias called infra-marginality. This refers to
individual-level bias where some individuals/subgroups can be worse off than
under simply optimizing for accuracy. For instance, a classifier implementing
race-based parity may significantly disadvantage women of the advantaged race.
To quantify this bias, we propose a general notion of $\eta$-infra-marginality
that can be used to evaluate the extent of this bias. We prove theoretically
that, unlike other fairness metrics, infra-marginality does not have a
trade-off with accuracy: high accuracy directly leads to low infra-marginality.
This observation is confirmed through empirical analysis on multiple simulated
and real-world datasets. Further, we find that maximizing group fairness often
increases infra-marginality, suggesting the consideration of both group-level
fairness and individual-level infra-marginality. However, measuring
infra-marginality requires knowledge of the true distribution of
individual-level outcomes correctly and explicitly. We propose a practical
method to measure infra-marginality, and a simple algorithm to maximize
group-wise accuracy and avoid infra-marginality.
","[{'version': 'v1', 'created': 'Tue, 3 Sep 2019 07:24:35 GMT'}]",2019-09-04,"[['Biswas', 'Arpita', ''], ['Barman', 'Siddharth', ''], ['Deshpande', 'Amit', ''], ['Sharma', 'Amit', '']]"
1171289,1909.0125100000002,Guy W Cole,Guy W. Cole and Sinead A. Williamson,Avoiding Resentment Via Monotonic Fairness,,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classifiers that achieve demographic balance by explicitly using protected
attributes such as race or gender are often politically or culturally
controversial due to their lack of individual fairness, i.e. individuals with
similar qualifications will receive different outcomes. Individually and group
fair decision criteria can produce counter-intuitive results, e.g. that the
optimal constrained boundary may reject intuitively better candidates due to
demographic imbalance in similar candidates. Both approaches can be seen as
introducing individual resentment, where some individuals would have received a
better outcome if they either belonged to a different demographic class and had
the same qualifications, or if they remained in the same class but had
objectively worse qualifications (e.g. lower test scores). We show that both
forms of resentment can be avoided by using monotonically constrained machine
learning models to create individually fair, demographically balanced
classifiers.
","[{'version': 'v1', 'created': 'Tue, 3 Sep 2019 15:28:16 GMT'}]",2019-09-15,"[['Cole', 'Guy W.', ''], ['Williamson', 'Sinead A.', '']]"
1175205,1909.0516699999998,Kacper Sokol,"Kacper Sokol, Raul Santos-Rodriguez, Peter Flach","FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability
  and Transparency",,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning algorithms can take important decisions, sometimes legally
binding, about our everyday life. In most cases, however, these systems and
decisions are neither regulated nor certified. Given the potential harm that
these algorithms can cause, qualities such as fairness, accountability and
transparency of predictive systems are of paramount importance. Recent
literature suggested voluntary self-reporting on these aspects of predictive
systems -- e.g., data sheets for data sets -- but their scope is often limited
to a single component of a machine learning pipeline, and producing them
requires manual labour. To resolve this impasse and ensure high-quality, fair,
transparent and reliable machine learning systems, we developed an open source
toolbox that can inspect selected fairness, accountability and transparency
aspects of these systems to automatically and objectively report them back to
their engineers and users. We describe design, scope and usage examples of this
Python toolbox in this paper. The toolbox provides functionality for inspecting
fairness, accountability and transparency of all aspects of the machine
learning process: data (and their features), models and predictions. It is
available to the public under the BSD 3-Clause open source licence.
","[{'version': 'v1', 'created': 'Wed, 11 Sep 2019 16:11:44 GMT'}]",2019-09-12,"[['Sokol', 'Kacper', ''], ['Santos-Rodriguez', 'Raul', ''], ['Flach', 'Peter', '']]"
1179179,1909.0914100000002,Elliot Creager,"Elliot Creager, David Madras, Toniann Pitassi, Richard Zemel",Causal Modeling for Fairness in Dynamical Systems,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In many application areas---lending, education, and online recommenders, for
example---fairness and equity concerns emerge when a machine learning system
interacts with a dynamically changing environment to produce both immediate and
long-term effects for individuals and demographic groups. We discuss causal
directed acyclic graphs (DAGs) as a unifying framework for the recent
literature on fairness in such dynamical systems. We show that this formulation
affords several new directions of inquiry to the modeler, where causal
assumptions can be expressed and manipulated. We emphasize the importance of
computing interventional quantities in the dynamical fairness setting, and show
how causal assumptions enable simulation (when environment dynamics are known)
and off-policy estimation (when dynamics are unknown) of intervention on short-
and long-term outcomes, at both the group and individual levels.
","[{'version': 'v1', 'created': 'Wed, 18 Sep 2019 20:21:56 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jul 2020 17:43:02 GMT'}]",2020-07-07,"[['Creager', 'Elliot', ''], ['Madras', 'David', ''], ['Pitassi', 'Toniann', ''], ['Zemel', 'Richard', '']]"
1186016,1910.02097,Heinrich Jiang,"Ofir Nachum, Heinrich Jiang",Group-based Fair Learning Leads to Counter-intuitive Predictions,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A number of machine learning (ML) methods have been proposed recently to
maximize model predictive accuracy while enforcing notions of group parity or
fairness across sub-populations. We propose a desirable property for these
procedures, slack-consistency: For any individual, the predictions of the model
should be monotonic with respect to allowed slack (i.e., maximum allowed
group-parity violation). Such monotonicity can be useful for individuals to
understand the impact of enforcing fairness on their predictions. Surprisingly,
we find that standard ML methods for enforcing fairness violate this basic
property. Moreover, this undesirable behavior arises in situations agnostic to
the complexity of the underlying model or approximate optimizations, suggesting
that the simple act of incorporating a constraint can lead to drastically
unintended behavior in ML. We present a simple theoretical method for enforcing
slack-consistency, while encouraging further discussions on the unintended
behaviors potentially induced when enforcing group-based parity.
","[{'version': 'v1', 'created': 'Fri, 4 Oct 2019 18:20:50 GMT'}]",2019-10-08,"[['Nachum', 'Ofir', ''], ['Jiang', 'Heinrich', '']]"
1186240,1910.02321,Nuno Louren\c{c}o,"In\^es Valentim, Nuno Louren\c{c}o, Nuno Antunes",The Impact of Data Preparation on the Fairness of Software Systems,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning models are widely adopted in scenarios that directly affect
people. The development of software systems based on these models raises
societal and legal concerns, as their decisions may lead to the unfair
treatment of individuals based on attributes like race or gender. Data
preparation is key in any machine learning pipeline, but its effect on fairness
is yet to be studied in detail. In this paper, we evaluate how the fairness and
effectiveness of the learned models are affected by the removal of the
sensitive attribute, the encoding of the categorical attributes, and instance
selection methods (including cross-validators and random undersampling). We
used the Adult Income and the German Credit Data datasets, which are widely
studied and known to have fairness concerns. We applied each data preparation
technique individually to analyse the difference in predictive performance and
fairness, using statistical parity difference, disparate impact, and the
normalised prejudice index. The results show that fairness is affected by
transformations made to the training data, particularly in imbalanced datasets.
Removing the sensitive attribute is insufficient to eliminate all the
unfairness in the predictions, as expected, but it is key to achieve fairer
models. Additionally, the standard random undersampling with respect to the
true labels is sometimes more prejudicial than performing no random
undersampling.
","[{'version': 'v1', 'created': 'Sat, 5 Oct 2019 19:50:16 GMT'}]",2019-10-08,"[['Valentim', 'Inês', ''], ['Lourenço', 'Nuno', ''], ['Antunes', 'Nuno', '']]"
1189032,1910.05113,Deepak P,"Savitha Sam Abraham, Deepak P, Sowmya S Sundaram",Fairness in Clustering with Multiple Sensitive Attributes,"Proceedings of the 23rd International Conference on Extending
  Database Technology (EDBT 2020), 30th March-2nd April, 2020",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A clustering may be considered as fair on pre-specified sensitive attributes
if the proportions of sensitive attribute groups in each cluster reflect that
in the dataset. In this paper, we consider the task of fair clustering for
scenarios involving multiple multi-valued or numeric sensitive attributes. We
propose a fair clustering method, \textit{FairKM} (Fair K-Means), that is
inspired by the popular K-Means clustering formulation. We outline a
computational notion of fairness which is used along with a cluster coherence
objective, to yield the FairKM clustering method. We empirically evaluate our
approach, wherein we quantify both the quality and fairness of clusters, over
real-world datasets. Our experimental evaluation illustrates that the clusters
generated by FairKM fare significantly better on both clustering quality and
fair representation of sensitive attribute groups compared to the clusters from
a state-of-the-art baseline fair clustering method.
","[{'version': 'v1', 'created': 'Fri, 11 Oct 2019 12:28:52 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Jan 2020 16:52:25 GMT'}]",2020-01-27,"[['Abraham', 'Savitha Sam', ''], ['P', 'Deepak', ''], ['Sundaram', 'Sowmya S', '']]"
1191081,1910.07162,Han Zhao,"Han Zhao, Amanda Coston, Tameem Adel, Geoffrey J. Gordon",Conditional Learning of Fair Representations,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a novel algorithm for learning fair representations that can
simultaneously mitigate two notions of disparity among different demographic
subgroups in the classification setting. Two key components underpinning the
design of our algorithm are balanced error rate and conditional alignment of
representations. We show how these two components contribute to ensuring
accuracy parity and equalized false-positive and false-negative rates across
groups without impacting demographic parity. Furthermore, we also demonstrate
both in theory and on two real-world experiments that the proposed algorithm
leads to a better utility-fairness trade-off on balanced datasets compared with
existing algorithms on learning fair representations for classification.
","[{'version': 'v1', 'created': 'Wed, 16 Oct 2019 04:12:50 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Feb 2020 18:10:34 GMT'}, {'version': 'v3', 'created': 'Sat, 15 Feb 2020 00:10:35 GMT'}]",2020-02-18,"[['Zhao', 'Han', ''], ['Coston', 'Amanda', ''], ['Adel', 'Tameem', ''], ['Gordon', 'Geoffrey J.', '']]"
1194174,1910.10255,Hanchen Wang,"Hanchen Wang, Nina Grgic-Hlaca, Preethi Lahoti, Krishna P. Gummadi,
  Adrian Weller","An Empirical Study on Learning Fairness Metrics for COMPAS Data with
  Human Supervision",Accepted at NeurIPS 2019 HCML Workshop,,,,cs.CY cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The notion of individual fairness requires that similar people receive
similar treatment. However, this is hard to achieve in practice since it is
difficult to specify the appropriate similarity metric. In this work, we
attempt to learn such similarity metric from human annotated data. We gather a
new dataset of human judgments on a criminal recidivism prediction (COMPAS)
task. By assuming the human supervision obeys the principle of individual
fairness, we leverage prior work on metric learning, evaluate the performance
of several metric learning methods on our dataset, and show that the learned
metrics outperform the Euclidean and Precision metric under various criteria.
We do not provide a way to directly learn a similarity metric satisfying the
individual fairness, but to provide an empirical study on how to derive the
similarity metric from human supervisors, then future work can use this as a
tool to understand human supervision.
","[{'version': 'v1', 'created': 'Tue, 22 Oct 2019 22:22:59 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Oct 2019 14:47:27 GMT'}]",2019-11-01,"[['Wang', 'Hanchen', ''], ['Grgic-Hlaca', 'Nina', ''], ['Lahoti', 'Preethi', ''], ['Gummadi', 'Krishna P.', ''], ['Weller', 'Adrian', '']]"
1194405,1910.1048600000001,Haochen Liu,"Haochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zitao Liu and Jiliang
  Tang",Does Gender Matter? Towards Fairness in Dialogue Systems,10 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently there are increasing concerns about the fairness of Artificial
Intelligence (AI) in real-world applications such as computer vision and
recommendations. For example, recognition algorithms in computer vision are
unfair to black people such as poorly detecting their faces and inappropriately
identifying them as ""gorillas"". As one crucial application of AI, dialogue
systems have been extensively applied in our society. They are usually built
with real human conversational data; thus they could inherit some fairness
issues which are held in the real world. However, the fairness of dialogue
systems has not been investigated. In this paper, we perform the initial study
about the fairness issues in dialogue systems. In particular, we construct the
first dataset and propose quantitative measures to understand fairness in
dialogue models. Our studies demonstrate that popular dialogue models show
significant prejudice towards different genders and races. Besides, to mitigate
the bias exhibited in dialogue systems, we propose two effective debiasing
methods. Experiments show that our methods can reduce the biases in dialogue
systems significantly. We will release the dataset and the measurement code
later to foster the fairness research in dialogue systems.
","[{'version': 'v1', 'created': 'Wed, 16 Oct 2019 22:17:02 GMT'}, {'version': 'v2', 'created': 'Sat, 18 Apr 2020 15:12:39 GMT'}]",2020-04-21,"[['Liu', 'Haochen', ''], ['Dacon', 'Jamell', ''], ['Fan', 'Wenqi', ''], ['Liu', 'Hui', ''], ['Liu', 'Zitao', ''], ['Tang', 'Jiliang', '']]"
1194790,1910.1087100000002,Andrew Tomkins,Benjamin Spector and Ravi Kumar and Andrew Tomkins,"Preventing Adversarial Use of Datasets through Fair Core-Set
  Construction","6 pages, 2 figures, NeurIPS 2019 Privacy In Machine Learning Workshop
  (PriML 2019)",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose improving the privacy properties of a dataset by publishing only a
strategically chosen ""core-set"" of the data containing a subset of the
instances. The core-set allows strong performance on primary tasks, but forces
poor performance on unwanted tasks. We give methods for both linear models and
neural networks and demonstrate their efficacy on data.
","[{'version': 'v1', 'created': 'Thu, 24 Oct 2019 01:28:58 GMT'}]",2019-10-25,"[['Spector', 'Benjamin', ''], ['Kumar', 'Ravi', ''], ['Tomkins', 'Andrew', '']]"
1196505,1910.12586,Yongkai Wu,"Yongkai Wu, Lu Zhang, Xintao Wu, Hanghang Tong",PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,Accepted as a poster to NeurIPS 2019,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A recent trend of fair machine learning is to define fairness as
causality-based notions which concern the causal connection between protected
attributes and decisions. However, one common challenge of all causality-based
fairness notions is identifiability, i.e., whether they can be uniquely
measured from observational data, which is a critical barrier to applying these
notions to real-world situations. In this paper, we develop a framework for
measuring different causality-based fairness. We propose a unified definition
that covers most of previous causality-based fairness notions, namely the
path-specific counterfactual fairness (PC fairness). Based on that, we propose
a general method in the form of a constrained optimization problem for bounding
the path-specific counterfactual fairness under all unidentifiable situations.
Experiments on synthetic and real-world datasets show the correctness and
effectiveness of our method.
","[{'version': 'v1', 'created': 'Sun, 20 Oct 2019 23:00:53 GMT'}]",2019-10-29,"[['Wu', 'Yongkai', ''], ['Zhang', 'Lu', ''], ['Wu', 'Xintao', ''], ['Tong', 'Hanghang', '']]"
1198391,1910.14472,Zongqing Lu,Jiechuan Jiang and Zongqing Lu,Learning Fairness in Multi-Agent Systems,NeurIPS'19,,,,cs.LG cs.AI cs.MA stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness is essential for human society, contributing to stability and
productivity. Similarly, fairness is also the key for many multi-agent systems.
Taking fairness into multi-agent learning could help multi-agent systems become
both efficient and stable. However, learning efficiency and fairness
simultaneously is a complex, multi-objective, joint-policy optimization. To
tackle these difficulties, we propose FEN, a novel hierarchical reinforcement
learning model. We first decompose fairness for each agent and propose
fair-efficient reward that each agent learns its own policy to optimize. To
avoid multi-objective conflict, we design a hierarchy consisting of a
controller and several sub-policies, where the controller maximizes the
fair-efficient reward by switching among the sub-policies that provides diverse
behaviors to interact with the environment. FEN can be trained in a fully
decentralized way, making it easy to be deployed in real-world applications.
Empirically, we show that FEN easily learns both fairness and efficiency and
significantly outperforms baselines in a variety of multi-agent scenarios.
","[{'version': 'v1', 'created': 'Thu, 31 Oct 2019 13:59:37 GMT'}]",2019-11-01,"[['Jiang', 'Jiechuan', ''], ['Lu', 'Zongqing', '']]"
1200062,1911.01468,Viktoriia Oliinyk,"Giulio Morina, Viktoriia Oliinyk, Julian Waton, Ines Marusic and
  Konstantinos Georgatzis","Auditing and Achieving Intersectional Fairness in Classification
  Problems",,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning algorithms are extensively used to make increasingly more
consequential decisions about people, so achieving optimal predictive
performance can no longer be the only focus. A particularly important
consideration is fairness with respect to race, gender, or any other sensitive
attribute. This paper studies intersectional fairness, where intersections of
multiple sensitive attributes are considered. Prior research has mainly focused
on fairness with respect to a single sensitive attribute, with intersectional
fairness being comparatively less studied despite its critical importance for
the safety of modern machine learning systems. We present a comprehensive
framework for auditing and achieving intersectional fairness in classification
problems: we define a suite of metrics to assess intersectional fairness in the
data or model outputs by extending known single-attribute fairness metrics, and
propose methods for robustly estimating them even when some intersectional
subgroups are underrepresented. Furthermore, we develop post-processing
techniques to mitigate any detected intersectional bias in a classification
model. Our techniques do not rely on any assumptions regarding the underlying
model and preserve predictive performance at a guaranteed level of fairness.
Finally, we give guidance on a practical implementation, showing how the
proposed methods perform on a real-world dataset.
","[{'version': 'v1', 'created': 'Mon, 4 Nov 2019 19:55:23 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Jun 2020 16:41:23 GMT'}]",2020-06-09,"[['Morina', 'Giulio', ''], ['Oliinyk', 'Viktoriia', ''], ['Waton', 'Julian', ''], ['Marusic', 'Ines', ''], ['Georgatzis', 'Konstantinos', '']]"
1201614,1911.0302,Hoda Heidari,"Mohammad Yaghini, Hoda Heidari, and Andreas Krause","A Human-in-the-loop Framework to Construct Context-dependent
  Mathematical Formulations of Fairness",,,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the recent surge of interest in designing and guaranteeing
mathematical formulations of fairness, virtually all existing notions of
algorithmic fairness fail to be adaptable to the intricacies and nuances of the
decision-making context at hand. We argue that capturing such factors is an
inherently human task, as it requires knowledge of the social background in
which machine learning tools impact real people's outcomes and a deep
understanding of the ramifications of automated decisions for decision subjects
and society. In this work, we present a framework to construct a
context-dependent mathematical formulation of fairness utilizing people's
judgment of fairness. We utilize the theoretical model of Heidari et al.
(2019)---which shows that most existing formulations of algorithmic fairness
are special cases of economic models of Equality of Opportunity (EOP)---and
present a practical human-in-the-loop approach to pinpoint the fairness notion
in the EOP family that best captures people's perception of fairness in the
given context. To illustrate our framework, we run human-subject experiments
designed to learn the parameters of Heidari et al.'s EOP model (including
circumstance, desert, and utility) in a hypothetical recidivism decision-making
scenario. Our work takes an initial step toward democratizing the formulation
of fairness and utilizing human-judgment to tackle a fundamental shortcoming of
automated decision-making systems: that the machine on its own is incapable of
understanding and processing the human aspects and social context of its
decisions.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 03:41:03 GMT'}]",2019-11-11,"[['Yaghini', 'Mohammad', ''], ['Heidari', 'Hoda', ''], ['Krause', 'Andreas', '']]"
1203523,1911.04929,Boris Ruf,"Vincent Grari, Boris Ruf, Sylvain Lamprier, Marcin Detyniecki",Fairness-Aware Neural R\'eyni Minimization for Continuous Features,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The past few years have seen a dramatic rise of academic and societal
interest in fair machine learning. While plenty of fair algorithms have been
proposed recently to tackle this challenge for discrete variables, only a few
ideas exist for continuous ones. The objective in this paper is to ensure some
independence level between the outputs of regression models and any given
continuous sensitive variables. For this purpose, we use the
Hirschfeld-Gebelein-R\'enyi (HGR) maximal correlation coefficient as a fairness
metric. We propose two approaches to minimize the HGR coefficient. First, by
reducing an upper bound of the HGR with a neural network estimation of the
$\chi^{2}$ divergence. Second, by minimizing the HGR directly with an
adversarial neural network architecture. The idea is to predict the output Y
while minimizing the ability of an adversarial neural network to find the
estimated transformations which are required to predict the HGR coefficient. We
empirically assess and compare our approaches and demonstrate significant
improvements on previously presented work in the field.
","[{'version': 'v1', 'created': 'Tue, 12 Nov 2019 15:20:29 GMT'}]",2019-11-13,"[['Grari', 'Vincent', ''], ['Ruf', 'Boris', ''], ['Lamprier', 'Sylvain', ''], ['Detyniecki', 'Marcin', '']]"
1203963,1911.05369,Boris Ruf,"Vincent Grari, Boris Ruf, Sylvain Lamprier, Marcin Detyniecki",Fair Adversarial Gradient Tree Boosting,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fair classification has become an important topic in machine learning
research. While most bias mitigation strategies focus on neural networks, we
noticed a lack of work on fair classifiers based on decision trees even though
they have proven very efficient. In an up-to-date comparison of
state-of-the-art classification algorithms in tabular data, tree boosting
outperforms deep learning. For this reason, we have developed a novel approach
of adversarial gradient tree boosting. The objective of the algorithm is to
predict the output $Y$ with gradient tree boosting while minimizing the ability
of an adversarial neural network to predict the sensitive attribute $S$. The
approach incorporates at each iteration the gradient of the neural network
directly in the gradient tree boosting. We empirically assess our approach on 4
popular data sets and compare against state-of-the-art algorithms. The results
show that our algorithm achieves a higher accuracy while obtaining the same
level of fairness, as measured using a set of different common fairness
definitions.
","[{'version': 'v1', 'created': 'Wed, 13 Nov 2019 09:43:55 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Nov 2019 10:28:37 GMT'}]",2019-11-19,"[['Grari', 'Vincent', ''], ['Ruf', 'Boris', ''], ['Lamprier', 'Sylvain', ''], ['Detyniecki', 'Marcin', '']]"
1205279,1911.06685,Drago Plecko,"Drago Ple\v{c}ko, Nicolai Meinshausen",Fair Data Adaptation with Quantile Preservation,,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness of classification and regression has received much attention
recently and various, partially non-compatible, criteria have been proposed.
The fairness criteria can be enforced for a given classifier or, alternatively,
the data can be adapated to ensure that every classifier trained on the data
will adhere to desired fairness criteria. We present a practical data adaption
method based on quantile preservation in causal structural equation models. The
data adaptation is based on a presumed counterfactual model for the data. While
the counterfactual model itself cannot be verified experimentally, we show that
certain population notions of fairness are still guaranteed even if the
counterfactual model is misspecified. The precise nature of the fulfilled
non-causal fairness notion (such as demographic parity, separation or
sufficiency) depends on the structure of the underlying causal model and the
choice of resolving variables. We describe an implementation of the proposed
data adaptation procedure based on Random Forests and demonstrate its practical
use on simulated and real-world data.
","[{'version': 'v1', 'created': 'Fri, 15 Nov 2019 15:16:24 GMT'}]",2019-11-18,"[['Plečko', 'Drago', ''], ['Meinshausen', 'Nicolai', '']]"
1206886,1911.0829199999998,Xintao Wu,"Wen Huang, Yongkai Wu, Lu Zhang, Xintao Wu",Fairness through Equality of Effort,,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fair machine learning is receiving an increasing attention in machine
learning fields. Researchers in fair learning have developed correlation or
association-based measures such as demographic disparity, mistreatment
disparity, calibration, causal-based measures such as total effect, direct and
indirect discrimination, and counterfactual fairness, and fairness notions such
as equality of opportunity and equal odds that consider both decisions in the
training data and decisions made by predictive models. In this paper, we
develop a new causal-based fairness notation, called equality of effort.
Different from existing fairness notions which mainly focus on discovering the
disparity of decisions between two groups of individuals, the proposed equality
of effort notation helps answer questions like to what extend a legitimate
variable should change to make a particular individual achieve a certain
outcome level and addresses the concerns whether the efforts made to achieve
the same outcome level for individuals from the protected group and that from
the unprotected group are different. We develop algorithms for determining
whether an individual or a group of individuals is discriminated in terms of
equality of effort. We also develop an optimization-based method for removing
discriminatory effects from the data if discrimination is detected. We conduct
empirical evaluations to compare the equality of effort and existing fairness
notion and show the effectiveness of our proposed algorithms.
","[{'version': 'v1', 'created': 'Mon, 11 Nov 2019 18:49:45 GMT'}]",2019-11-20,"[['Huang', 'Wen', ''], ['Wu', 'Yongkai', ''], ['Zhang', 'Lu', ''], ['Wu', 'Xintao', '']]"
1208082,1911.09488,Toby Walsh,Martin Aleksandrov and Toby Walsh,Online Fair Division: A Survey,"Accepted by the 34th AAAI Conference on Artificial Intelligence (AAAI
  2020)",,,,cs.AI cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We survey a burgeoning and promising new research area that considers the
online nature of many practical fair division problems. We identify wide
variety of such online fair division problems, as well as discuss new
mechanisms and normative properties that apply to this online setting. The
online nature of such fair division problems provides both opportunities and
challenges such as the possibility to develop new online mechanisms as well as
the difficulty of dealing with an uncertain future.
","[{'version': 'v1', 'created': 'Thu, 21 Nov 2019 14:30:34 GMT'}]",2019-11-22,"[['Aleksandrov', 'Martin', ''], ['Walsh', 'Toby', '']]"
1208125,1911.0953100000002,Joao Moreira,"Remzi Celebi, Joao Rebelo Moreira, Ahmed A. Hassan, Sandeep Ayyar,
  Lars Ridder, Tobias Kuhn, and Michel Dumontier",Towards FAIR protocols and workflows: The OpenPREDICT case study,"Preprint. Submitted to PeerJ on 13th November 2019. 3 appendixes as
  PDF files",,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  It is essential for the advancement of science that scientists and
researchers share, reuse and reproduce workflows and protocols used by others.
The FAIR principles are a set of guidelines that aim to maximize the value and
usefulness of research data, and emphasize a number of important points
regarding the means by which digital objects are found and reused by others.
The question of how to apply these principles not just to the static input and
output data but also to the dynamic workflows and protocols that consume and
produce them is still under debate and poses a number of challenges. In this
paper we describe our inclusive and overarching approach to apply the FAIR
principles to workflows and protocols and demonstrate its benefits. We apply
and evaluate our approach on a case study that consists of making the PREDICT
workflow, a highly cited drug repurposing workflow, open and FAIR. This
includes FAIRification of the involved datasets, as well as applying semantic
technologies to represent and store data about the detailed versions of the
general protocol, of the concrete workflow instructions, and of their execution
traces. A semantic model was proposed to better address these specific
requirements and were evaluated by answering competency questions. This
semantic model consists of classes and relations from a number of existing
ontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then
to formulate and answer new kinds of competency questions. Our evaluation shows
the high degree to which our FAIRified OpenPREDICT workflow now adheres to the
FAIR principles and the practicality and usefulness of being able to answer our
new competency questions.
","[{'version': 'v1', 'created': 'Wed, 20 Nov 2019 08:53:57 GMT'}]",2019-11-22,"[['Celebi', 'Remzi', ''], ['Moreira', 'Joao Rebelo', ''], ['Hassan', 'Ahmed A.', ''], ['Ayyar', 'Sandeep', ''], ['Ridder', 'Lars', ''], ['Kuhn', 'Tobias', ''], ['Dumontier', 'Michel', '']]"
1209599,1911.11005,Martin Aleksandrov D,Martin Aleksandrov and Toby Walsh,Greedy Algorithms for Fair Division of Mixed Manna,21 pages,,,,cs.AI cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider a multi-agent model for fair division of mixed manna (i.e. items
for which agents can have positive, zero or negative utilities), in which
agents have additive utilities for bundles of items. For this model, we give
several general impossibility results and special possibility results for three
common fairness concepts (i.e. EF1, EFX, EFX3) and one popular efficiency
concept (i.e. PO). We also study how these interact with common welfare
objectives such as the Nash, disutility Nash and egalitarian welfares. For
example, we show that maximizing the Nash welfare with mixed manna (or
minimizing the disutility Nash welfare) does not ensure an EF1 allocation
whereas with goods and the Nash welfare it does. We also prove that an EFX3
allocation may not exist even with identical utilities. By comparison, with
tertiary utilities, EFX and PO allocations, or EFX3 and PO allocations always
exist. Also, with identical utilities, EFX and PO allocations always exist. For
these cases, we give polynomial-time algorithms, returning such allocations and
approximating further the Nash, disutility Nash and egalitarian welfares in
special cases.
","[{'version': 'v1', 'created': 'Mon, 25 Nov 2019 15:52:28 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2019 16:53:34 GMT'}]",2019-12-18,"[['Aleksandrov', 'Martin', ''], ['Walsh', 'Toby', '']]"
1209647,1911.1105300000002,Nicolas Maudet,"Parham Shams and Aur\'elie Beynier and Sylvain Bouveret and Nicolas
  Maudet",Fair in the Eyes of Others,,,,,cs.AI cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Envy-freeness is a widely studied notion in resource allocation, capturing
some aspects of fairness. The notion of envy being inherently subjective
though, it might be the case that an agent envies another agent, but that she
objectively has no reason to do so. The difficulty here is to define the notion
of objectivity, since no ground-truth can properly serve as a basis of this
definition. A natural approach is to consider the judgement of the other agents
as a proxy for objectivity. Building on previous work by Parijs (who introduced
""unanimous envy"") we propose the notion of approval envy: an agent $a_i$
experiences approval envy towards $a_j$ if she is envious of $a_j$, and
sufficiently many agents agree that this should be the case, from their own
perspectives. Some interesting properties of this notion are put forward.
Computing the minimal threshold guaranteeing approval envy clearly inherits
well-known intractable results from envy-freeness, but (i) we identify some
tractable cases such as house allocation; and (ii) we provide a general method
based on a mixed integer programming encoding of the problem, which proves to
be efficient in practice. This allows us in particular to show experimentally
that existence of such allocations, with a rather small threshold, is very
often observed.
","[{'version': 'v1', 'created': 'Mon, 25 Nov 2019 17:05:27 GMT'}]",2019-11-26,"[['Shams', 'Parham', ''], ['Beynier', 'Aurélie', ''], ['Bouveret', 'Sylvain', ''], ['Maudet', 'Nicolas', '']]"
1210720,1911.1212600000001,Xiangxiang Chu,Xiangxiang Chu and Tianbao Zhou and Bo Zhang and Jixiang Li,"Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture
  Search","Accepted to ECCV 2020, camera ready version",,,,cs.LG cs.AI cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Differentiable Architecture Search (DARTS) is now a widely disseminated
weight-sharing neural architecture search method. However, it suffers from
well-known performance collapse due to an inevitable aggregation of skip
connections. In this paper, we first disclose that its root cause lies in an
unfair advantage in exclusive competition. Through experiments, we show that if
either of two conditions is broken, the collapse disappears. Thereby, we
present a novel approach called Fair DARTS where the exclusive competition is
relaxed to be collaborative. Specifically, we let each operation's
architectural weight be independent of others. Yet there is still an important
issue of discretization discrepancy. We then propose a zero-one loss to push
architectural weights towards zero or one, which approximates an expected
multi-hot solution. Our experiments are performed on two mainstream search
spaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet.
Our code is available on https://github.com/xiaomi-automl/fairdarts .
","[{'version': 'v1', 'created': 'Wed, 27 Nov 2019 13:10:25 GMT'}, {'version': 'v2', 'created': 'Tue, 10 Mar 2020 11:31:52 GMT'}, {'version': 'v3', 'created': 'Wed, 15 Jul 2020 02:37:59 GMT'}, {'version': 'v4', 'created': 'Thu, 16 Jul 2020 01:16:52 GMT'}]",2020-07-17,"[['Chu', 'Xiangxiang', ''], ['Zhou', 'Tianbao', ''], ['Zhang', 'Bo', ''], ['Li', 'Jixiang', '']]"
1212657,1912.00761,Alice Xiang,Alice Xiang and Inioluwa Deborah Raji,On the Legal Compatibility of Fairness Definitions,"6 pages, Workshop on Human-Centric Machine Learning at the 33rd
  Conference on Neural Information Processing Systems (NeurIPS 2019)",,,,cs.CY cs.AI cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Past literature has been effective in demonstrating ideological gaps in
machine learning (ML) fairness definitions when considering their use in
complex socio-technical systems. However, we go further to demonstrate that
these definitions often misunderstand the legal concepts from which they
purport to be inspired, and consequently inappropriately co-opt legal language.
In this paper, we demonstrate examples of this misalignment and discuss the
differences in ML terminology and their legal counterparts, as well as what
both the legal and ML fairness communities can learn from these tensions. We
focus this paper on U.S. anti-discrimination law since the ML fairness research
community regularly references terms from this body of law.
","[{'version': 'v1', 'created': 'Mon, 25 Nov 2019 21:28:46 GMT'}]",2019-12-03,"[['Xiang', 'Alice', ''], ['Raji', 'Inioluwa Deborah', '']]"
1212990,1912.01094,Kevin Matthew Stangl,"Avrim Blum, Kevin Stangl",Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multiple fairness constraints have been proposed in the literature, motivated
by a range of concerns about how demographic groups might be treated unfairly
by machine learning classifiers. In this work we consider a different
motivation; learning from biased training data. We posit several ways in which
training data may be biased, including having a more noisy or negatively biased
labeling process on members of a disadvantaged group, or a decreased prevalence
of positive or negative examples from the disadvantaged group, or both.
  Given such biased training data, Empirical Risk Minimization (ERM) may
produce a classifier that not only is biased but also has suboptimal accuracy
on the true data distribution. We examine the ability of fairness-constrained
ERM to correct this problem. In particular, we find that the Equal Opportunity
fairness constraint (Hardt, Price, and Srebro 2016) combined with ERM will
provably recover the Bayes Optimal Classifier under a range of bias models. We
also consider other recovery methods including reweighting the training data,
Equalized Odds, and Demographic Parity. These theoretical results provide
additional motivation for considering fairness interventions even if an actor
cares primarily about accuracy.
","[{'version': 'v1', 'created': 'Mon, 2 Dec 2019 22:00:14 GMT'}]",2019-12-04,"[['Blum', 'Avrim', ''], ['Stangl', 'Kevin', '']]"
1215698,1912.03802,Candice Schumann,"Candice Schumann, Zhi Lang, Nicholas Mattei, John P. Dickerson",Group Fairness in Bandit Arm Selection,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a novel formulation of group fairness in the contextual
multi-armed bandit (CMAB) setting. In the CMAB setting a sequential decision
maker must at each time step choose an arm to pull from a finite set of arms
after observing some context for each of the potential arm pulls. In our model
arms are partitioned into two or more sensitive groups based on some protected
feature (e.g., age, race, or socio-economic status). Despite the fact that
there may be differences in expected payout between the groups, we may wish to
ensure some form of fairness between picking arms from the various groups. In
this work we explore two definitions of fairness: equal group probability,
wherein the probability of pulling an arm from any of the protected groups is
the same; and proportional parity, wherein the probability of choosing an arm
from a particular group is proportional to the size of that group. We provide a
novel algorithm that can accommodate these notions of fairness for an arbitrary
number of groups, and provide bounds on the regret for our algorithm. We then
validate our algorithm using synthetic data as well as two real-world datasets
for intervention settings wherein we want to allocate resources fairly across
protected groups.
","[{'version': 'v1', 'created': 'Mon, 9 Dec 2019 01:02:35 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Feb 2020 17:26:41 GMT'}]",2020-02-19,"[['Schumann', 'Candice', ''], ['Lang', 'Zhi', ''], ['Mattei', 'Nicholas', ''], ['Dickerson', 'John P.', '']]"
1219700,1912.0780399999999,Shufang Zhu,"Shufang Zhu, Giuseppe De Giacomo, Geguang Pu, Moshe Vardi",LTLf Synthesis with Fairness and Stability Assumptions,,,,,cs.AI cs.FL cs.GT cs.LO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In synthesis, assumptions are constraints on the environment that rule out
certain environment behaviors. A key observation here is that even if we
consider systems with LTLf goals on finite traces, environment assumptions need
to be expressed over infinite traces, since accomplishing the agent goals may
require an unbounded number of environment action. To solve synthesis with
respect to finite-trace LTLf goals under infinite-trace assumptions, we could
reduce the problem to LTL synthesis. Unfortunately, while synthesis in LTLf and
in LTL have the same worst-case complexity (both 2EXPTIME-complete), the
algorithms available for LTL synthesis are much more difficult in practice than
those for LTLf synthesis. In this work we show that in interesting cases we can
avoid such a detour to LTL synthesis and keep the simplicity of LTLf synthesis.
Specifically, we develop a BDD-based fixpoint-based technique for handling
basic forms of fairness and of stability assumptions. We show, empirically,
that this technique performs much better than standard LTL synthesis.
","[{'version': 'v1', 'created': 'Tue, 17 Dec 2019 03:44:39 GMT'}]",2019-12-18,"[['Zhu', 'Shufang', ''], ['De Giacomo', 'Giuseppe', ''], ['Pu', 'Geguang', ''], ['Vardi', 'Moshe', '']]"
1219951,1912.08055,Heramb Nemlekar,"Yifang Chen, Alex Cuellar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar
  and Stefanos Nikolaidis",Fair Contextual Multi-Armed Bandits: Theory and Experiments,"9 pages, 9 figures",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When an AI system interacts with multiple users, it frequently needs to make
allocation decisions. For instance, a virtual agent decides whom to pay
attention to in a group setting, or a factory robot selects a worker to deliver
a part. Demonstrating fairness in decision making is essential for such systems
to be broadly accepted. We introduce a Multi-Armed Bandit algorithm with
fairness constraints, where fairness is defined as a minimum rate that a task
or a resource is assigned to a user. The proposed algorithm uses contextual
information about the users and the task and makes no assumptions on how the
losses capturing the performance of different users are generated. We provide
theoretical guarantees of performance and empirical results from simulation and
an online user study. The results highlight the benefit of accounting for
contexts in fair decision making, especially when users perform better at some
contexts and worse at others.
","[{'version': 'v1', 'created': 'Fri, 13 Dec 2019 22:25:34 GMT'}]",2019-12-18,"[['Chen', 'Yifang', ''], ['Cuellar', 'Alex', ''], ['Luo', 'Haipeng', ''], ['Modi', 'Jignesh', ''], ['Nemlekar', 'Heramb', ''], ['Nikolaidis', 'Stefanos', '']]"
1220284,1912.0838800000001,Vedant Nanda,"Vedant Nanda and Pan Xu and Karthik Abinav Sankararaman and John P.
  Dickerson and Aravind Srinivasan","Balancing the Tradeoff between Profit and Fairness in Rideshare
  Platforms During High-Demand Hours","8 pages, 4 figures, Accepted at AAAI 2020 & AIES (Oral) 2020",,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Rideshare platforms, when assigning requests to drivers, tend to maximize
profit for the system and/or minimize waiting time for riders. Such platforms
can exacerbate biases that drivers may have over certain types of requests. We
consider the case of peak hours when the demand for rides is more than the
supply of drivers. Drivers are well aware of their advantage during the peak
hours and can choose to be selective about which rides to accept. Moreover, if
in such a scenario, the assignment of requests to drivers (by the platform) is
made only to maximize profit and/or minimize wait time for riders, requests of
a certain type (e.g. from a non-popular pickup location, or to a non-popular
drop-off location) might never be assigned to a driver. Such a system can be
highly unfair to riders. However, increasing fairness might come at a cost of
the overall profit made by the rideshare platform. To balance these conflicting
goals, we present a flexible, non-adaptive algorithm, \lpalg, that allows the
platform designer to control the profit and fairness of the system via
parameters $\alpha$ and $\beta$ respectively. We model the matching problem as
an online bipartite matching where the set of drivers is offline and requests
arrive online. Upon the arrival of a request, we use \lpalg to assign it to a
driver (the driver might then choose to accept or reject it) or reject the
request. We formalize the measures of profit and fairness in our setting and
show that by using \lpalg, the competitive ratios for profit and fairness
measures would be no worse than $\alpha/e$ and $\beta/e$ respectively.
Extensive experimental results on both real-world and synthetic datasets
confirm the validity of our theoretical lower bounds. Additionally, they show
that $\lpalg$ under some choice of $(\alpha, \beta)$ can beat two natural
heuristics, Greedy and Uniform, on \emph{both} fairness and profit.
","[{'version': 'v1', 'created': 'Wed, 18 Dec 2019 05:39:11 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Sep 2020 01:41:53 GMT'}]",2020-09-08,"[['Nanda', 'Vedant', ''], ['Xu', 'Pan', ''], ['Sankararaman', 'Karthik Abinav', ''], ['Dickerson', 'John P.', ''], ['Srinivasan', 'Aravind', '']]"
1223099,1912.11203,Sasha Rubin,Benjamin Aminof and Giuseppe De Giacomo and Sasha Rubin,"Stochastic Fairness and Language-Theoretic Fairness in Planning on
  Nondeterministic Domains",,,,,cs.AI cs.FL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We address two central notions of fairness in the literature of planning on
nondeterministic fully observable domains. The first, which we call stochastic
fairness, is classical, and assumes an environment which operates
probabilistically using possibly unknown probabilities. The second, which is
language-theoretic, assumes that if an action is taken from a given state
infinitely often then all its possible outcomes should appear infinitely often
(we call this state-action fairness). While the two notions coincide for
standard reachability goals, they diverge for temporally extended goals. This
important difference has been overlooked in the planning literature, and we
argue has led to confusion in a number of published algorithms which use
reductions that were stated for state-action fairness, for which they are
incorrect, while being correct for stochastic fairness. We remedy this and
provide an optimal sound and complete algorithm for solving state-action fair
planning for LTL/LTLf goals, as well as a correct proof of the lower bound of
the goal-complexity (our proof is general enough that it provides new proofs
also for the no-fairness and stochastic-fairness cases). Overall, we show that
stochastic fairness is better behaved than state-action fairness.
","[{'version': 'v1', 'created': 'Tue, 24 Dec 2019 04:35:33 GMT'}]",2019-12-25,"[['Aminof', 'Benjamin', ''], ['De Giacomo', 'Giuseppe', ''], ['Rubin', 'Sasha', '']]"
1225126,1912.1323,Vahid Noroozi,"Vahid Noroozi, Sara Bahaadini, Samira Sheikhi, Nooshin Mojab, Philip
  S. Yu",Leveraging Semi-Supervised Learning for Fairness using Neural Networks,"6 pages, 5 figures, accepted to ICMLA 2019",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There has been a growing concern about the fairness of decision-making
systems based on machine learning. The shortage of labeled data has been always
a challenging problem facing machine learning based systems. In such scenarios,
semi-supervised learning has shown to be an effective way of exploiting
unlabeled data to improve upon the performance of model. Notably, unlabeled
data do not contain label information which itself can be a significant source
of bias in training machine learning systems. This inspired us to tackle the
challenge of fairness by formulating the problem in a semi-supervised
framework. In this paper, we propose a semi-supervised algorithm using neural
networks benefiting from unlabeled data to not just improve the performance but
also improve the fairness of the decision-making process. The proposed model,
called SSFair, exploits the information in the unlabeled data to mitigate the
bias in the training data.
","[{'version': 'v1', 'created': 'Tue, 31 Dec 2019 09:11:26 GMT'}]",2020-01-01,"[['Noroozi', 'Vahid', ''], ['Bahaadini', 'Sara', ''], ['Sheikhi', 'Samira', ''], ['Mojab', 'Nooshin', ''], ['Yu', 'Philip S.', '']]"
1225502,2001.0008899999998,Candice Schumann,"Debjani Saha, Candice Schumann, Duncan C. McElfresh, John P.
  Dickerson, Michelle L. Mazurek, Michael Carl Tschantz",Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,,,,,cs.CY cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bias in machine learning has manifested injustice in several areas, such as
medicine, hiring, and criminal justice. In response, computer scientists have
developed myriad definitions of fairness to correct this bias in fielded
algorithms. While some definitions are based on established legal and ethical
norms, others are largely mathematical. It is unclear whether the general
public agrees with these fairness definitions, and perhaps more importantly,
whether they understand these definitions. We take initial steps toward
bridging this gap between ML researchers and the public, by addressing the
question: does a lay audience understand a basic definition of ML fairness? We
develop a metric to measure comprehension of three such
definitions--demographic parity, equal opportunity, and equalized odds. We
evaluate this metric using an online survey, and investigate the relationship
between comprehension and sentiment, demographics, and the definition itself.
","[{'version': 'v1', 'created': 'Tue, 17 Dec 2019 00:58:54 GMT'}, {'version': 'v2', 'created': 'Wed, 11 Mar 2020 16:23:08 GMT'}, {'version': 'v3', 'created': 'Thu, 2 Jul 2020 21:13:01 GMT'}]",2020-07-06,"[['Saha', 'Debjani', ''], ['Schumann', 'Candice', ''], ['McElfresh', 'Duncan C.', ''], ['Dickerson', 'John P.', ''], ['Mazurek', 'Michelle L.', ''], ['Tschantz', 'Michael Carl', '']]"
1225742,2001.0032899999999,Dallas Card,Dallas Card and Noah A. Smith,On Consequentialism and Fairness,Updating to published version,"Front. Artif. Intell., 08 May 2020",10.3389/frai.2020.00034,,cs.AI cs.CY cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work on fairness in machine learning has primarily emphasized how to
define, quantify, and encourage ""fair"" outcomes. Less attention has been paid,
however, to the ethical foundations which underlie such efforts. Among the
ethical perspectives that should be taken into consideration is
consequentialism, the position that, roughly speaking, outcomes are all that
matter. Although consequentialism is not free from difficulties, and although
it does not necessarily provide a tractable way of choosing actions (because of
the combined problems of uncertainty, subjectivity, and aggregation), it
nevertheless provides a powerful foundation from which to critique the existing
literature on machine learning fairness. Moreover, it brings to the fore some
of the tradeoffs involved, including the problem of who counts, the pros and
cons of using a policy, and the relative value of the distant future. In this
paper we provide a consequentialist critique of common definitions of fairness
within machine learning, as well as a machine learning perspective on
consequentialism. We conclude with a broader discussion of the issues of
learning and randomization, which have important implications for the ethics of
automated decision making systems.
","[{'version': 'v1', 'created': 'Thu, 2 Jan 2020 05:39:48 GMT'}, {'version': 'v2', 'created': 'Mon, 11 May 2020 04:36:44 GMT'}]",2020-05-12,"[['Card', 'Dallas', ''], ['Smith', 'Noah A.', '']]"
1230274,2001.04861,Xueru Zhang,"Xueru Zhang, Mingyan Liu",Fairness in Learning-Based Sequential Decision Algorithms: A Survey,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Algorithmic fairness in decision-making has been studied extensively in
static settings where one-shot decisions are made on tasks such as
classification. However, in practice most decision-making processes are of a
sequential nature, where decisions made in the past may have an impact on
future data. This is particularly the case when decisions affect the
individuals or users generating the data used for future decisions. In this
survey, we review existing literature on the fairness of data-driven sequential
decision-making. We will focus on two types of sequential decisions: (1) past
decisions have no impact on the underlying user population and thus no impact
on future data; (2) past decisions have an impact on the underlying user
population and therefore the future data, which can then impact future
decisions. In each case the impact of various fairness interventions on the
underlying population is examined.
","[{'version': 'v1', 'created': 'Tue, 14 Jan 2020 15:49:57 GMT'}]",2020-01-15,"[['Zhang', 'Xueru', ''], ['Liu', 'Mingyan', '']]"
1232106,2001.0669300000002,Vijay Arya,"Karan Dabas, Nishtha Madan, Vijay Arya, Sameep Mehta, Gautam Singh,
  Tanmoy Chakraborty",Fair Transfer of Multiple Style Attributes in Text,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To preserve anonymity and obfuscate their identity on online platforms users
may morph their text and portray themselves as a different gender or
demographic. Similarly, a chatbot may need to customize its communication style
to improve engagement with its audience. This manner of changing the style of
written text has gained significant attention in recent years. Yet these past
research works largely cater to the transfer of single style attributes. The
disadvantage of focusing on a single style alone is that this often results in
target text where other existing style attributes behave unpredictably or are
unfairly dominated by the new style. To counteract this behavior, it would be
nice to have a style transfer mechanism that can transfer or control multiple
styles simultaneously and fairly. Through such an approach, one could obtain
obfuscated or written text incorporated with a desired degree of multiple soft
styles such as female-quality, politeness, or formalness.
  In this work, we demonstrate that the transfer of multiple styles cannot be
achieved by sequentially performing multiple single-style transfers. This is
because each single style-transfer step often reverses or dominates over the
style incorporated by a previous transfer step. We then propose a neural
network architecture for fairly transferring multiple style attributes in a
given text. We test our architecture on the Yelp data set to demonstrate our
superior performance as compared to existing one-style transfer steps performed
in a sequence.
","[{'version': 'v1', 'created': 'Sat, 18 Jan 2020 15:38:04 GMT'}]",2020-01-22,"[['Dabas', 'Karan', ''], ['Madan', 'Nishtha', ''], ['Arya', 'Vijay', ''], ['Mehta', 'Sameep', ''], ['Singh', 'Gautam', ''], ['Chakraborty', 'Tanmoy', '']]"
1232991,2001.0757800000001,Nicholas Asher,"Nicholas Asher, Soumya Paul, Chris Russell",Adequate and fair explanations,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Explaining sophisticated machine-learning based systems is an important issue
at the foundations of AI. Recent efforts have shown various methods for
providing explanations. These approaches can be broadly divided into two
schools: those that provide a local and human interpreatable approximation of a
machine learning algorithm, and logical approaches that exactly characterise
one aspect of the decision. In this paper we focus upon the second school of
exact explanations with a rigorous logical foundation. There is an
epistemological problem with these exact methods. While they can furnish
complete explanations, such explanations may be too complex for humans to
understand or even to write down in human readable form. Interpretability
requires epistemically accessible explanations, explanations humans can grasp.
Yet what is a sufficiently complete epistemically accessible explanation still
needs clarification. We do this here in terms of counterfactuals, following
[Wachter et al., 2017]. With counterfactual explanations, many of the
assumptions needed to provide a complete explanation are left implicit. To do
so, counterfactual explanations exploit the properties of a particular data
point or sample, and as such are also local as well as partial explanations. We
explore how to move from local partial explanations to what we call complete
local explanations and then to global ones. But to preserve accessibility we
argue for the need for partiality. This partiality makes it possible to hide
explicit biases present in the algorithm that may be injurious or unfair.We
investigate how easy it is to uncover these biases in providing complete and
fair explanations by exploiting the structure of the set of counterfactuals
providing a complete local explanation.
","[{'version': 'v1', 'created': 'Tue, 21 Jan 2020 14:42:51 GMT'}]",2020-01-22,"[['Asher', 'Nicholas', ''], ['Paul', 'Soumya', ''], ['Russell', 'Chris', '']]"
1235186,2001.0977300000002,Zachary Lipton,"Sina Fazelpour, Zachary C. Lipton",Algorithmic Fairness from a Non-ideal Perspective,"Accepted for publication at the AAAI/ACM Conference on Artificial
  Intelligence, Ethics, and Society (AIES) 2020",,,,cs.CY cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by recent breakthroughs in predictive modeling, practitioners in
both industry and government have turned to machine learning with hopes of
operationalizing predictions to drive automated decisions. Unfortunately, many
social desiderata concerning consequential decisions, such as justice or
fairness, have no natural formulation within a purely predictive framework. In
efforts to mitigate these problems, researchers have proposed a variety of
metrics for quantifying deviations from various statistical parities that we
might expect to observe in a fair world and offered a variety of algorithms in
attempts to satisfy subsets of these parities or to trade off the degree to
which they are satisfied against utility. In this paper, we connect this
approach to \emph{fair machine learning} to the literature on ideal and
non-ideal methodological approaches in political philosophy. The ideal approach
requires positing the principles according to which a just world would operate.
In the most straightforward application of ideal theory, one supports a
proposed policy by arguing that it closes a discrepancy between the real and
the perfectly just world. However, by failing to account for the mechanisms by
which our non-ideal world arose, the responsibilities of various
decision-makers, and the impacts of proposed policies, naive applications of
ideal thinking can lead to misguided interventions. In this paper, we
demonstrate a connection between the fair machine learning literature and the
ideal approach in political philosophy, and argue that the increasingly
apparent shortcomings of proposed fair machine learning algorithms reflect
broader troubles faced by the ideal approach. We conclude with a critical
discussion of the harms of misguided solutions, a reinterpretation of
impossibility results, and directions for future research.
","[{'version': 'v1', 'created': 'Wed, 8 Jan 2020 18:44:41 GMT'}]",2020-01-28,"[['Fazelpour', 'Sina', ''], ['Lipton', 'Zachary C.', '']]"
1235197,2001.09784,Dana Pessach,Dana Pessach and Erez Shmueli,Algorithmic Fairness,"31 pages, 1 figure, This is a survey article that reviews the field
  of algorithmic fairness",,,,cs.CY cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An increasing number of decisions regarding the daily lives of human beings
are being controlled by artificial intelligence (AI) algorithms in spheres
ranging from healthcare, transportation, and education to college admissions,
recruitment, provision of loans and many more realms. Since they now touch on
many aspects of our lives, it is crucial to develop AI algorithms that are not
only accurate but also objective and fair. Recent studies have shown that
algorithmic decision-making may be inherently prone to unfairness, even when
there is no intention for it. This paper presents an overview of the main
concepts of identifying, measuring and improving algorithmic fairness when
using AI algorithms. The paper begins by discussing the causes of algorithmic
bias and unfairness and the common definitions and measures for fairness.
Fairness-enhancing mechanisms are then reviewed and divided into pre-process,
in-process and post-process mechanisms. A comprehensive comparison of the
mechanisms is then conducted, towards a better understanding of which
mechanisms should be used in different scenarios. The paper then describes the
most commonly used fairness-related datasets in this field. Finally, the paper
ends by reviewing several emerging research sub-fields of algorithmic fairness.
","[{'version': 'v1', 'created': 'Tue, 21 Jan 2020 19:01:38 GMT'}]",2020-01-28,"[['Pessach', 'Dana', ''], ['Shmueli', 'Erez', '']]"
1238118,2002.00695,Vasileios Iosifidis,"Vasileios Iosifidis, Besnik Fetahu, Eirini Ntoutsi",FAE: A Fairness-Aware Ensemble Framework,6 pages,"IEEE International Conference on Big Data, 2019",,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated decision making based on big data and machine learning (ML)
algorithms can result in discriminatory decisions against certain protected
groups defined upon personal data like gender, race, sexual orientation etc.
Such algorithms designed to discover patterns in big data might not only pick
up any encoded societal biases in the training data, but even worse, they might
reinforce such biases resulting in more severe discrimination. The majority of
thus far proposed fairness-aware machine learning approaches focus solely on
the pre-, in- or post-processing steps of the machine learning process, that
is, input data, learning algorithms or derived models, respectively. However,
the fairness problem cannot be isolated to a single step of the ML process.
Rather, discrimination is often a result of complex interactions between big
data and algorithms, and therefore, a more holistic approach is required. The
proposed FAE (Fairness-Aware Ensemble) framework combines fairness-related
interventions at both pre- and postprocessing steps of the data analysis
process. In the preprocessing step, we tackle the problems of
under-representation of the protected group (group imbalance) and of
class-imbalance by generating balanced training samples. In the post-processing
step, we tackle the problem of class overlapping by shifting the decision
boundary in the direction of fairness.
","[{'version': 'v1', 'created': 'Mon, 3 Feb 2020 13:05:18 GMT'}]",2020-02-06,"[['Iosifidis', 'Vasileios', ''], ['Fetahu', 'Besnik', ''], ['Ntoutsi', 'Eirini', '']]"
1239044,2002.0162100000002,Yunfeng Zhang,"Yunfeng Zhang, Rachel K. E. Bellamy, Kush R. Varshney",Joint Optimization of AI Fairness and Utility: A Human-Centered Approach,To appear in AIES 2020 proceedings,,10.1145/3375627.3375862,,cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Today, AI is increasingly being used in many high-stakes decision-making
applications in which fairness is an important concern. Already, there are many
examples of AI being biased and making questionable and unfair decisions. The
AI research community has proposed many methods to measure and mitigate
unwanted biases, but few of them involve inputs from human policy makers. We
argue that because different fairness criteria sometimes cannot be
simultaneously satisfied, and because achieving fairness often requires
sacrificing other objectives such as model accuracy, it is key to acquire and
adhere to human policy makers' preferences on how to make the tradeoff among
these objectives. In this paper, we propose a framework and some exemplar
methods for eliciting such preferences and for optimizing an AI model according
to these preferences.
","[{'version': 'v1', 'created': 'Wed, 5 Feb 2020 03:31:48 GMT'}]",2020-02-06,"[['Zhang', 'Yunfeng', ''], ['Bellamy', 'Rachel K. E.', ''], ['Varshney', 'Kush R.', '']]"
1239697,2002.0227399999999,Sara Ahmadian,"Sara Ahmadian, Alessandro Epasto, Ravi Kumar, Mohammad Mahdian",Fair Correlation Clustering,,,,,cs.DS cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we study correlation clustering under fairness constraints.
Fair variants of $k$-median and $k$-center clustering have been studied
recently, and approximation algorithms using a notion called fairlet
decomposition have been proposed. We obtain approximation algorithms for fair
correlation clustering under several important types of fairness constraints.
  Our results hinge on obtaining a fairlet decomposition for correlation
clustering by introducing a novel combinatorial optimization problem. We define
a fairlet decomposition with cost similar to the $k$-median cost and this
allows us to obtain approximation algorithms for a wide range of fairness
constraints.
  We complement our theoretical results with an in-depth analysis of our
algorithms on real graphs where we show that fair solutions to correlation
clustering can be obtained with limited increase in cost compared to the
state-of-the-art (unfair) algorithms.
","[{'version': 'v1', 'created': 'Thu, 6 Feb 2020 14:28:21 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Mar 2020 22:27:51 GMT'}]",2020-03-04,"[['Ahmadian', 'Sara', ''], ['Epasto', 'Alessandro', ''], ['Kumar', 'Ravi', ''], ['Mahdian', 'Mohammad', '']]"
1240931,2002.03508,Sainyam Galhotra Mr,"Saba Ahmadi, Sainyam Galhotra, Barna Saha, Roy Schwartz",Fair Correlation Clustering,,,,,cs.DS cs.AI cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  In this paper we study the problem of correlation clustering under fairness
constraints. In the classic correlation clustering problem, we are given a
complete graph where each edge is labeled positive or negative. The goal is to
obtain a clustering of the vertices that minimizes disagreements -- the number
of negative edges trapped inside a cluster plus positive edges between
different clusters.
  We consider two variations of fairness constraint for the problem of
correlation clustering where each node has a color, and the goal is to form
clusters that do not over-represent vertices of any color.
  The first variant aims to generate clusters with minimum disagreements, where
the distribution of a feature (e.g. gender) in each cluster is same as the
global distribution. For the case of two colors when the desired ratio of the
number of colors in each cluster is $1:p$, we get
$\mathcal{O}(p^2)$-approximation algorithm. Our algorithm could be extended to
the case of multiple colors. We prove this problem is NP-hard.
  The second variant considers relative upper and lower bounds on the number of
nodes of any color in a cluster. The goal is to avoid violating upper and lower
bounds corresponding to each color in each cluster while minimizing the total
number of disagreements. Along with our theoretical results, we show the
effectiveness of our algorithm to generate fair clusters by empirical
evaluation on real world data sets.
","[{'version': 'v1', 'created': 'Mon, 10 Feb 2020 02:59:17 GMT'}]",2020-02-11,"[['Ahmadi', 'Saba', ''], ['Galhotra', 'Sainyam', ''], ['Saha', 'Barna', ''], ['Schwartz', 'Roy', '']]"
1243924,2002.0650100000003,Hikaru Ogura,Hikaru Ogura and Akiko Takeda,Convex Fairness Constrained Model Using Causal Effect Estimators,"10 pages, 5 figures, Accepted for the 2nd Workshop on Fairness,
  Accountability, Transparency, Ethics and Society on the Web (FATES on the Web
  2020), held in conjunction with the WWW'20",,10.1145/3366424.3383556,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have seen much research on fairness in machine learning. Here,
mean difference (MD) or demographic parity is one of the most popular measures
of fairness. However, MD quantifies not only discrimination but also
explanatory bias which is the difference of outcomes justified by explanatory
features. In this paper, we devise novel models, called FairCEEs, which remove
discrimination while keeping explanatory bias. The models are based on
estimators of causal effect utilizing propensity score analysis. We prove that
FairCEEs with the squared loss theoretically outperform a naive MD constraint
model. We provide an efficient algorithm for solving FairCEEs in regression and
binary classification tasks. In our experiment on synthetic and real-world data
in these two tasks, FairCEEs outperformed an existing model that considers
explanatory bias in specific cases.
","[{'version': 'v1', 'created': 'Sun, 16 Feb 2020 03:40:04 GMT'}]",2020-02-18,"[['Ogura', 'Hikaru', ''], ['Takeda', 'Akiko', '']]"
1244169,2002.0674600000002,Yoichi Chikahara,"Yoichi Chikahara, Shinsaku Sakaue, Akinori Fujino, Hisashi Kashima","Learning Individually Fair Classifier with Path-Specific Causal-Effect
  Constraint","28 pages, 8 figures, 4 tables",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning is increasingly used to make decisions for individuals in
various fields, which require to achieve good prediction accuracy while
ensuring fairness with respect to such sensitive features as race or gender.
This problem, however, remains difficult in complex real-world scenarios. To
effectively quantify unfairness in such scenarios, existing methods utilize
{\it path-specific causal effects}. However, none of them can ensure fairness
for each individual without making impractical assumptions. Specifically, these
assumptions require us to formulate the true data-generating processes as the
{\it causal model}, which requires an extremely deep understanding of data and
is unrealistic in practice. In this paper, we propose a framework for learning
an individually fair classifier without relying on the causal model. For this
goal, we define the {\it probability of individual unfairness} (PIU) and solve
an optimization problem that constrains its upper bound, which can be estimated
from data without the causal model. We elucidate why this constraint can
guarantee fairness for each individual. Experimental results demonstrate that
our method learns an individually fair classifier at a slight cost of
prediction accuracy.
","[{'version': 'v1', 'created': 'Mon, 17 Feb 2020 02:46:17 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Jun 2020 08:29:23 GMT'}]",2020-06-17,"[['Chikahara', 'Yoichi', ''], ['Sakaue', 'Shinsaku', ''], ['Fujino', 'Akinori', ''], ['Kashima', 'Hisashi', '']]"
1244570,2002.0714699999999,Aaron Roth,"Christopher Jung and Sampath Kannan and Changhwa Lee and Mallesh M.
  Pai and Aaron Roth and Rakesh Vohra",Fair Prediction with Endogenous Behavior,,,,,econ.TH cs.AI cs.GT cs.LG econ.EM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is increasing regulatory interest in whether machine learning
algorithms deployed in consequential domains (e.g. in criminal justice) treat
different demographic groups ""fairly."" However, there are several proposed
notions of fairness, typically mutually incompatible. Using criminal justice as
an example, we study a model in which society chooses an incarceration rule.
Agents of different demographic groups differ in their outside options (e.g.
opportunity for legal employment) and decide whether to commit crimes. We show
that equalizing type I and type II errors across groups is consistent with the
goal of minimizing the overall crime rate; other popular notions of fairness
are not.
","[{'version': 'v1', 'created': 'Tue, 18 Feb 2020 16:07:25 GMT'}]",2020-02-19,"[['Jung', 'Christopher', ''], ['Kannan', 'Sampath', ''], ['Lee', 'Changhwa', ''], ['Pai', 'Mallesh M.', ''], ['Roth', 'Aaron', ''], ['Vohra', 'Rakesh', '']]"
1246477,2002.09054,Lionel Robert,"Lionel P. Robert, Casey Pierce, Liz Morris, Sangmi Kim, Rasha Alahmad","Designing Fair AI for Managing Employees in Organizations: A Review,
  Critique, and Design Agenda","66 pages, 2 figures",,,,cs.HC cs.AI cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Organizations are rapidly deploying artificial intelligence (AI) systems to
manage their workers. However, AI has been found at times to be unfair to
workers. Unfairness toward workers has been associated with decreased worker
effort and increased worker turnover. To avoid such problems, AI systems must
be designed to support fairness and redress instances of unfairness. Despite
the attention related to AI unfairness, there has not been a theoretical and
systematic approach to developing a design agenda. This paper addresses the
issue in three ways. First, we introduce the organizational justice theory,
three different fairness types (distributive, procedural, interactional), and
the frameworks for redressing instances of unfairness (retributive justice,
restorative justice). Second, we review the design literature that specifically
focuses on issues of AI fairness in organizations. Third, we propose a design
agenda for AI fairness in organizations that applies each of the fairness types
to organizational scenarios. Then, the paper concludes with implications for
future research.
","[{'version': 'v1', 'created': 'Thu, 20 Feb 2020 22:52:43 GMT'}]",2020-02-24,"[['Robert', 'Lionel P.', ''], ['Pierce', 'Casey', ''], ['Morris', 'Liz', ''], ['Kim', 'Sangmi', ''], ['Alahmad', 'Rasha', '']]"
1246894,2002.09471,Yue Zhang,"Yue Zhang, Arti Ramesh",Learning Fairness-aware Relational Structures,Accepted for publication in ECAI 2020,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The development of fair machine learning models that effectively avert bias
and discrimination is an important problem that has garnered attention in
recent years. The necessity of encoding complex relational dependencies among
the features and variables for competent predictions require the development of
fair, yet expressive relational models. In this work, we introduce Fair-A3SL, a
fairness-aware structure learning algorithm for learning relational structures,
which incorporates fairness measures while learning relational graphical model
structures. Our approach is versatile in being able to encode a wide range of
fairness metrics such as statistical parity difference, overestimation,
equalized odds, and equal opportunity, including recently proposed relational
fairness measures. While existing approaches employ the fairness measures on
pre-determined model structures post prediction, Fair-A3SL directly learns the
structure while optimizing for the fairness measures and hence is able to
remove any structural bias in the model. We demonstrate the effectiveness of
our learned model structures when compared with the state-of-the-art fairness
models quantitatively and qualitatively on datasets representing three
different modeling scenarios: i) a relational dataset, ii) a recidivism
prediction dataset widely used in studying discrimination, and iii) a
recommender systems dataset. Our results show that Fair-A3SL can learn fair,
yet interpretable and expressive structures capable of making accurate
predictions.
","[{'version': 'v1', 'created': 'Fri, 21 Feb 2020 18:53:52 GMT'}]",2020-02-24,"[['Zhang', 'Yue', ''], ['Ramesh', 'Arti', '']]"
1247735,2002.10312,Anian Ruoss,"Anian Ruoss, Mislav Balunovi\'c, Marc Fischer, and Martin Vechev",Learning Certified Individually Fair Representations,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To effectively enforce fairness constraints one needs to define an
appropriate notion of fairness and employ representation learning in order to
impose this notion without compromising downstream utility for the data
consumer. A desirable notion is individual fairness as it guarantees similar
treatment for similar individuals. In this work, we introduce the first method
which generalizes individual fairness to rich similarity notions via logical
constraints while also enabling data consumers to obtain fairness certificates
for their models. The key idea is to learn a representation that provably maps
similar individuals to latent representations at most $\epsilon$ apart in
$\ell_{\infty}$-distance, enabling data consumers to certify individual
fairness by proving $\epsilon$-robustness of their classifier. Our experimental
evaluation on six real-world datasets and a wide range of fairness constraints
demonstrates that our approach is expressive enough to capture similarity
notions beyond existing distance metrics while scaling to realistic use cases.
","[{'version': 'v1', 'created': 'Mon, 24 Feb 2020 15:41:34 GMT'}]",2020-02-25,"[['Ruoss', 'Anian', ''], ['Balunović', 'Mislav', ''], ['Fischer', 'Marc', ''], ['Vechev', 'Martin', '']]"
1248187,2002.10764,Gourab K Patro,"Gourab K Patro, Arpita Biswas, Niloy Ganguly, Krishna P. Gummadi,
  Abhijnan Chakraborty","FairRec: Two-Sided Fairness for Personalized Recommendations in
  Two-Sided Platforms",In Proceedings of The Web Conference (WWW) 2020,,10.1145/3366423.3380196,,cs.AI cs.GT,http://creativecommons.org/licenses/by/4.0/,"  We investigate the problem of fair recommendation in the context of two-sided
online platforms, comprising customers on one side and producers on the other.
Traditionally, recommendation services in these platforms have focused on
maximizing customer satisfaction by tailoring the results according to the
personalized preferences of individual customers. However, our investigation
reveals that such customer-centric design may lead to unfair distribution of
exposure among the producers, which may adversely impact their well-being. On
the other hand, a producer-centric design might become unfair to the customers.
Thus, we consider fairness issues that span both customers and producers. Our
approach involves a novel mapping of the fair recommendation problem to a
constrained version of the problem of fairly allocating indivisible goods. Our
proposed FairRec algorithm guarantees at least Maximin Share (MMS) of exposure
for most of the producers and Envy-Free up to One item (EF1) fairness for every
customer. Extensive evaluations over multiple real-world datasets show the
effectiveness of FairRec in ensuring two-sided fairness while incurring a
marginal loss in the overall recommendation quality.
","[{'version': 'v1', 'created': 'Tue, 25 Feb 2020 09:43:48 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Jun 2020 12:54:52 GMT'}]",2020-06-24,"[['Patro', 'Gourab K', ''], ['Biswas', 'Arpita', ''], ['Ganguly', 'Niloy', ''], ['Gummadi', 'Krishna P.', ''], ['Chakraborty', 'Abhijnan', '']]"
1248197,2002.10774,Pietro Di Stefano,"Pietro G. Di Stefano, James M. Hickey, Vlasios Vasileiou",Counterfactual fairness: removing direct effects through regularization,"10 pages, 4 figures",,,,cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building machine learning models that are fair with respect to an
unprivileged group is a topical problem. Modern fairness-aware algorithms often
ignore causal effects and enforce fairness through modifications applicable to
only a subset of machine learning models. In this work, we propose a new
definition of fairness that incorporates causality through the Controlled
Direct Effect (CDE). We develop regularizations to tackle classical fairness
measures and present a causal regularization that satisfies our new fairness
definition by removing the impact of unprivileged group variables on the model
outcomes as measured by the CDE. These regularizations are applicable to any
model trained using by iteratively minimizing a loss through differentiation.
We demonstrate our approaches using both gradient boosting and logistic
regression on: a synthetic dataset, the UCI Adult (Census) Dataset, and a
real-world credit-risk dataset. Our results were found to mitigate unfairness
from the predictions with small reductions in model performance.
","[{'version': 'v1', 'created': 'Tue, 25 Feb 2020 10:13:55 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2020 11:28:34 GMT'}]",2020-02-27,"[['Di Stefano', 'Pietro G.', ''], ['Hickey', 'James M.', ''], ['Vasileiou', 'Vlasios', '']]"
1251182,2003.00827,Laleh Seyyed-Kalantari,"Laleh Seyyed-Kalantari, Guanxiong Liu, Matthew McDermott, Marzyeh
  Ghassemi",CheXclusion: Fairness gaps in deep chest X-ray classifiers,,,,,cs.CV cs.AI cs.LG eess.IV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning systems have received much attention recently for their
ability to achieve expert-level performance on clinical tasks, particularly in
medical imaging. Here, we examine the extent to which state-of-the-art deep
learning classifiers trained to yield diagnostic labels from X-ray images are
biased with respect to protected attributes. We train convolution neural
networks to predict 14 diagnostic labels in three prominent public chest X-ray
datasets: MIMIC-CXR, Chest-Xray8, and CheXpert. We then evaluate the TPR
disparity - the difference in true positive rates (TPR) and - underdiagnosis
rate - the false positive rate of a non-diagnosis - among different protected
attributes such as patient sex, age, race, and insurance type. We demonstrate
that TPR disparities exist in the state-of-the-art classifiers in all datasets,
for all clinical tasks, and all subgroups. We find that TPR disparities are
most commonly not significantly correlated with a subgroup's proportional
disease burden; further, we find that some subgroups and subsection of the
population are chronically underdiagnosed. Such performance disparities have
real consequences as models move from papers to products, and should be
carefully audited prior to deployment.
","[{'version': 'v1', 'created': 'Fri, 14 Feb 2020 22:08:12 GMT'}]",2020-03-03,"[['Seyyed-Kalantari', 'Laleh', ''], ['Liu', 'Guanxiong', ''], ['McDermott', 'Matthew', ''], ['Ghassemi', 'Marzyeh', '']]"
1251880,2003.01525,Juliana Ferreira J,Juliana Jansen Ferreira and Mateus de Souza Monteiro,Evidence-based explanation to promote fairness in AI systems,Fair & Responsible AI Workshop @ CHI2020,,,,cs.HC cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As Artificial Intelligence (AI) technology gets more intertwined with every
system, people are using AI to make decisions on their everyday activities. In
simple contexts, such as Netflix recommendations, or in more complex context
like in judicial scenarios, AI is part of people's decisions. People make
decisions and usually, they need to explain their decision to others or in some
matter. It is particularly critical in contexts where human expertise is
central to decision-making. In order to explain their decisions with AI
support, people need to understand how AI is part of that decision. When
considering the aspect of fairness, the role that AI has on a decision-making
process becomes even more sensitive since it affects the fairness and the
responsibility of those people making the ultimate decision. We have been
exploring an evidence-based explanation design approach to 'tell the story of a
decision'. In this position paper, we discuss our approach for AI systems using
fairness sensitive cases in the literature.
","[{'version': 'v1', 'created': 'Tue, 3 Mar 2020 14:22:11 GMT'}]",2020-03-04,"[['Ferreira', 'Juliana Jansen', ''], ['Monteiro', 'Mateus de Souza', '']]"
1257275,2003.0692,Boris Ruf,"Boris Ruf, Chaouki Boutharouite, Marcin Detyniecki",Getting Fairness Right: Towards a Toolbox for Practitioners,Accepted at the Workshop on Fair and Responsible AI at CHI2020,,,,cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The potential risk of AI systems unintentionally embedding and reproducing
bias has attracted the attention of machine learning practitioners and society
at large. As policy makers are willing to set the standards of algorithms and
AI techniques, the issue on how to refine existing regulation, in order to
enforce that decisions made by automated systems are fair and
non-discriminatory, is again critical. Meanwhile, researchers have demonstrated
that the various existing metrics for fairness are statistically mutually
exclusive and the right choice mostly depends on the use case and the
definition of fairness.
  Recognizing that the solutions for implementing fair AI are not purely
mathematical but require the commitments of the stakeholders to define the
desired nature of fairness, this paper proposes to draft a toolbox which helps
practitioners to ensure fair AI practices. Based on the nature of the
application and the available training data, but also on legal requirements and
ethical, philosophical and cultural dimensions, the toolbox aims to identify
the most appropriate fairness objective. This approach attempts to structure
the complex landscape of fairness metrics and, therefore, makes the different
available options more accessible to non-technical people. In the proven
absence of a silver bullet solution for fair AI, this toolbox intends to
produce the fairest AI systems possible with respect to their local context.
","[{'version': 'v1', 'created': 'Sun, 15 Mar 2020 20:53:50 GMT'}]",2020-03-17,"[['Ruf', 'Boris', ''], ['Boutharouite', 'Chaouki', ''], ['Detyniecki', 'Marcin', '']]"
1257415,2003.0706,Mithun Chakraborty,"Nawal Benabbou, Mithun Chakraborty, Ayumi Igarashi, Yair Zick",Finding Fair and Efficient Allocations When Valuations Don't Add Up,,,10.1007/978-3-030-57980-7_3,,cs.AI cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present new results on the fair and efficient allocation of
indivisible goods to agents whose preferences correspond to matroid rank
functions. This is a versatile valuation class with several desirable
properties (such as monotonicity and submodularity), which naturally lends
itself to a number of real-world domains. We use these properties to our
advantage; first, we show that when agent valuations are matroid rank
functions, a socially optimal (i.e. utilitarian social welfare-maximizing)
allocation that achieves envy-freeness up to one item (EF1) exists and is
computationally tractable. We also prove that the Nash welfare-maximizing and
the leximin allocations both exhibit this fairness/efficiency combination, by
showing that they can be achieved by minimizing any symmetric strictly convex
function over utilitarian optimal outcomes. To the best of our knowledge, this
is the first valuation function class not subsumed by additive valuations for
which it has been established that an allocation maximizing Nash welfare is
EF1. Moreover, for a subclass of these valuation functions based on maximum
(unweighted) bipartite matching, we show that a leximin allocation can be
computed in polynomial time. Additionally, we explore possible extensions of
our results to fairness criteria other than EF1 as well as to generalizations
of the above valuation classes.
","[{'version': 'v1', 'created': 'Mon, 16 Mar 2020 07:42:27 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Mar 2020 03:16:39 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 05:35:19 GMT'}]",2020-10-06,"[['Benabbou', 'Nawal', ''], ['Chakraborty', 'Mithun', ''], ['Igarashi', 'Ayumi', ''], ['Zick', 'Yair', '']]"
1263439,2003.1308399999998,Daniel Garijo,Daniel Garijo and Mar\'ia Poveda-Villal\'on,"Best Practices for Implementing FAIR Vocabularies and Ontologies on the
  Web","16 pages, 4 figures",,,,cs.DL cs.AI cs.DB,http://creativecommons.org/licenses/by/4.0/,"  With the adoption of Semantic Web technologies, an increasing number of
vocabularies and ontologies have been developed in different domains, ranging
from Biology to Agronomy or Geosciences. However, many of these ontologies are
still difficult to find, access and understand by researchers due to a lack of
documentation, URI resolving issues, versioning problems, etc. In this chapter
we describe guidelines and best practices for creating accessible,
understandable and reusable ontologies on the Web, using standard practices and
pointing to existing tools and frameworks developed by the Semantic Web
community. We illustrate our guidelines with concrete examples, in order to
help researchers implement these practices in their future vocabularies.
","[{'version': 'v1', 'created': 'Sun, 29 Mar 2020 17:40:04 GMT'}]",2020-03-31,"[['Garijo', 'Daniel', ''], ['Poveda-Villalón', 'María', '']]"
1274321,2004.09551,Nabeel Gillani,"Eric Chu, Nabeel Gillani, Sneha Priscilla Makini",Games for Fairness and Interpretability,,,,,cs.CY cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As Machine Learning (ML) systems becomes more ubiquitous, ensuring the fair
and equitable application of their underlying algorithms is of paramount
importance. We argue that one way to achieve this is to proactively cultivate
public pressure for ML developers to design and develop fairer algorithms --
and that one way to cultivate public pressure while simultaneously serving the
interests and objectives of algorithm developers is through gameplay. We
propose a new class of games -- ``games for fairness and interpretability'' --
as one example of an incentive-aligned approach for producing fairer and more
equitable algorithms. Games for fairness and interpretability are
carefully-designed games with mass appeal. They are inherently engaging,
provide insights into how machine learning models work, and ultimately produce
data that helps researchers and developers improve their algorithms. We
highlight several possible examples of games, their implications for fairness
and interpretability, how their proliferation could creative positive public
pressure by narrowing the gap between algorithm developers and the general
public, and why the machine learning community could benefit from them.
","[{'version': 'v1', 'created': 'Mon, 20 Apr 2020 18:09:32 GMT'}]",2020-04-22,"[['Chu', 'Eric', ''], ['Gillani', 'Nabeel', ''], ['Makini', 'Sneha Priscilla', '']]"
1274739,2004.0996899999998,Javier Del Ser Dr.,"Antonio LaTorre, Daniel Molina, Eneko Osaba, Javier Del Ser, Francisco
  Herrera","Fairness in Bio-inspired Optimization Research: A Prescription of
  Methodological Guidelines for Comparing Meta-heuristics","43 pages, 4 figures",,,,cs.NE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bio-inspired optimization (including Evolutionary Computation and Swarm
Intelligence) is a growing research topic with many competitive bio-inspired
algorithms being proposed every year. In such an active area, preparing a
successful proposal of a new bio-inspired algorithm is not an easy task. Given
the maturity of this research field, proposing a new optimization technique
with innovative elements is no longer enough. Apart from the novelty, results
reported by the authors should be proven to achieve a significant advance over
previous outcomes from the state of the art. Unfortunately, not all new
proposals deal with this requirement properly. Some of them fail to select an
appropriate benchmark or reference algorithms to compare with. In other cases,
the validation process carried out is not defined in a principled way (or is
even not done at all). Consequently, the significance of the results presented
in such studies cannot be guaranteed. In this work we review several
recommendations in the literature and propose methodological guidelines to
prepare a successful proposal, taking all these issues into account. We expect
these guidelines to be useful not only for authors, but also for reviewers and
editors along their assessment of new contributions to the field.
","[{'version': 'v1', 'created': 'Sun, 19 Apr 2020 04:46:45 GMT'}]",2020-04-22,"[['LaTorre', 'Antonio', ''], ['Molina', 'Daniel', ''], ['Osaba', 'Eneko', ''], ['Del Ser', 'Javier', ''], ['Herrera', 'Francisco', '']]"
1275156,2004.1038600000002,Jingfeng Zhang,"Jingfeng Zhang, Cheng Li, Antonio Robles-Kelly and Mohan Kankanhalli",Hierarchically Fair Federated Learning,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When the federated learning is adopted among competitive agents with siloed
datasets, agents are self-interested and participate only if they are fairly
rewarded. To encourage the application of federated learning, this paper
employs a management strategy, i.e., more contributions should lead to more
rewards. We propose a novel hierarchically fair federated learning (HFFL)
framework. Under this framework, agents are rewarded in proportion to their
pre-negotiated contribution levels. HFFL+ extends this to incorporate
heterogeneous models. Theoretical analysis and empirical evaluation on several
datasets confirm the efficacy of our frameworks in upholding fairness and thus
facilitating federated learning in the competitive settings.
","[{'version': 'v1', 'created': 'Wed, 22 Apr 2020 03:41:06 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 11:42:33 GMT'}]",2020-05-04,"[['Zhang', 'Jingfeng', ''], ['Li', 'Cheng', ''], ['Robles-Kelly', 'Antonio', ''], ['Kankanhalli', 'Mohan', '']]"
1276239,2004.1146899999999,Martin Aleksandrov D,Martin Aleksandrov,"Jealousy-freeness and other common properties in Fair Division of Mixed
  Manna","12 pages, 1 table, 2 figures",,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider a fair division setting where indivisible items are allocated to
agents. Each agent in the setting has strictly negative, zero or strictly
positive utility for each item. We, thus, make a distinction between items that
are good for some agents and bad for other agents (i.e. mixed), good for
everyone (i.e. goods) or bad for everyone (i.e. bads). For this model, we study
axiomatic concepts of allocations such as jealousy-freeness up to one item,
envy-freeness up to one item and Pareto-optimality. We obtain many new
possibility and impossibility results in regard to combinations of these
properties. We also investigate new computational tasks related to such
combinations. Thus, we advance the state-of-the-art in fair division of mixed
manna.
","[{'version': 'v1', 'created': 'Thu, 23 Apr 2020 21:39:12 GMT'}, {'version': 'v2', 'created': 'Wed, 6 May 2020 22:52:03 GMT'}, {'version': 'v3', 'created': 'Tue, 12 May 2020 19:18:02 GMT'}]",2020-05-14,"[['Aleksandrov', 'Martin', '']]"
1283269,2005.0347399999998,Arpita Biswas,"Arpita Biswas, Suvam Mukherjee",Ensuring Fairness under Prior Probability Shifts,,,,,cs.LG cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we study the problem of fair classification in the presence of
prior probability shifts, where the training set distribution differs from the
test set. This phenomenon can be observed in the yearly records of several
real-world datasets, such as recidivism records and medical expenditure
surveys. If unaccounted for, such shifts can cause the predictions of a
classifier to become unfair towards specific population subgroups. While the
fairness notion called Proportional Equality (PE) accounts for such shifts, a
procedure to ensure PE-fairness was unknown.
  In this work, we propose a method, called CAPE, which provides a
comprehensive solution to the aforementioned problem. CAPE makes novel use of
prevalence estimation techniques, sampling and an ensemble of classifiers to
ensure fair predictions under prior probability shifts. We introduce a metric,
called prevalence difference (PD), which CAPE attempts to minimize in order to
ensure PE-fairness. We theoretically establish that this metric exhibits
several desirable properties.
  We evaluate the efficacy of CAPE via a thorough empirical evaluation on
synthetic datasets. We also compare the performance of CAPE with several
popular fair classifiers on real-world datasets like COMPAS (criminal risk
assessment) and MEPS (medical expenditure panel survey). The results indicate
that CAPE ensures PE-fair predictions, while performing well on other
performance metrics.
","[{'version': 'v1', 'created': 'Wed, 6 May 2020 13:07:05 GMT'}]",2020-05-08,"[['Biswas', 'Arpita', ''], ['Mukherjee', 'Suvam', '']]"
1284650,2005.04855,Toby Walsh,Toby Walsh,Fair Division: The Computer Scientist's Perspective,To appear in Proceedings of IJCAI 2020,,,,cs.AI cs.GT cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  I survey recent progress on a classic and challenging problem in social
choice: the fair division of indivisible items. I discuss how a computational
perspective has provided interesting insights into and understanding of how to
divide items fairly and efficiently. This has involved bringing to bear tools
such as those used in knowledge representation, computational complexity,
approximation methods, game theory, online analysis and communication
complexity
","[{'version': 'v1', 'created': 'Mon, 11 May 2020 04:19:38 GMT'}]",2020-05-12,"[['Walsh', 'Toby', '']]"
1285701,2005.05906,Brent Mittelstadt,"Sandra Wachter, Brent Mittelstadt, Chris Russell","Why Fairness Cannot Be Automated: Bridging the Gap Between EU
  Non-Discrimination Law and AI",,,10.2139/ssrn.3547922,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This article identifies a critical incompatibility between European notions
of discrimination and existing statistical measures of fairness. First, we
review the evidential requirements to bring a claim under EU non-discrimination
law. Due to the disparate nature of algorithmic and human discrimination, the
EU's current requirements are too contextual, reliant on intuition, and open to
judicial interpretation to be automated. Second, we show how the legal
protection offered by non-discrimination law is challenged when AI, not humans,
discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes,
prejudice) and unintentional biases (e.g. organisational practices or
internalised stereotypes) which can act as a signal to victims that
discrimination has occurred. Finally, we examine how existing work on fairness
in machine learning lines up with procedures for assessing cases under EU
non-discrimination law. We propose ""conditional demographic disparity"" (CDD) as
a standard baseline statistical measurement that aligns with the European Court
of Justice's ""gold standard."" Establishing a standard set of statistical
evidence for automated discrimination cases can help ensure consistent
procedures for assessment, but not judicial interpretation, of cases involving
AI and automated systems. Through this proposal for procedural regularity in
the identification and assessment of automated discrimination, we clarify how
to build considerations of fairness into automated systems as far as possible
while still respecting and enabling the contextual approach to judicial
interpretation practiced under EU non-discrimination law.
  N.B. Abridged abstract
","[{'version': 'v1', 'created': 'Tue, 12 May 2020 16:30:12 GMT'}]",2020-05-13,"[['Wachter', 'Sandra', ''], ['Mittelstadt', 'Brent', ''], ['Russell', 'Chris', '']]"
1285817,2005.0602199999998,Carlos Toxtli,"Carlos Toxtli, Angela Richmond-Fuller, Saiph Savage",Reputation Agent: Prompting Fair Reviews in Gig Markets,"12 pages, 5 figures, The Web Conference 2020, ACM WWW 2020",,10.1145/3366423.3380199,,cs.HC cs.AI cs.LG,http://creativecommons.org/publicdomain/zero/1.0/,"  Our study presents a new tool, Reputation Agent, to promote fairer reviews
from requesters (employers or customers) on gig markets. Unfair reviews,
created when requesters consider factors outside of a worker's control, are
known to plague gig workers and can result in lost job opportunities and even
termination from the marketplace. Our tool leverages machine learning to
implement an intelligent interface that: (1) uses deep learning to
automatically detect when an individual has included unfair factors into her
review (factors outside the worker's control per the policies of the market);
and (2) prompts the individual to reconsider her review if she has incorporated
unfair factors. To study the effectiveness of Reputation Agent, we conducted a
controlled experiment over different gig markets. Our experiment illustrates
that across markets, Reputation Agent, in contrast with traditional approaches,
motivates requesters to review gig workers' performance more fairly. We discuss
how tools that bring more transparency to employers about the policies of a gig
market can help build empathy thus resulting in reasoned discussions around
potential injustices towards workers generated by these interfaces. Our vision
is that with tools that promote truth and transparency we can bring fairer
treatment to gig workers.
","[{'version': 'v1', 'created': 'Fri, 8 May 2020 01:56:10 GMT'}]",2020-05-14,"[['Toxtli', 'Carlos', ''], ['Richmond-Fuller', 'Angela', ''], ['Savage', 'Saiph', '']]"
1286647,2005.06852,Pieter Delobelle,"Pieter Delobelle and Paul Temple and Gilles Perrouin and Beno\^it
  Fr\'enay and Patrick Heymans and Bettina Berendt","Ethical Adversaries: Towards Mitigating Unfairness with Adversarial
  Machine Learning","15 pages, 3 figures, 1 table",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning is being integrated into a growing number of critical
systems with far-reaching impacts on society. Unexpected behaviour and unfair
decision processes are coming under increasing scrutiny due to this widespread
use and its theoretical considerations. Individuals, as well as organisations,
notice, test, and criticize unfair results to hold model designers and
deployers accountable. We offer a framework that assists these groups in
mitigating unfair representations stemming from the training datasets. Our
framework relies on two inter-operating adversaries to improve fairness. First,
a model is trained with the goal of preventing the guessing of protected
attributes' values while limiting utility losses. This first step optimizes the
model's parameters for fairness. Second, the framework leverages evasion
attacks from adversarial machine learning to generate new examples that will be
misclassified. These new examples are then used to retrain and improve the
model in the first step. These two steps are iteratively applied until a
significant improvement in fairness is obtained. We evaluated our framework on
well-studied datasets in the fairness literature -- including COMPAS -- where
it can surpass other approaches concerning demographic parity, equality of
opportunity and also the model's utility. We also illustrate our findings on
the subtle difficulties when mitigating unfairness and highlight how our
framework can assist model designers.
","[{'version': 'v1', 'created': 'Thu, 14 May 2020 10:10:19 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Sep 2020 16:47:17 GMT'}]",2020-09-02,"[['Delobelle', 'Pieter', ''], ['Temple', 'Paul', ''], ['Perrouin', 'Gilles', ''], ['Frénay', 'Benoît', ''], ['Heymans', 'Patrick', ''], ['Berendt', 'Bettina', '']]"
1287088,2005.07293,Ninareh Mehrabi,"Ninareh Mehrabi, Yuzhong Huang, Fred Morstatter",Statistical Equity: A Fairness Classification Objective,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning systems have been shown to propagate the societal errors of
the past. In light of this, a wealth of research focuses on designing solutions
that are ""fair."" Even with this abundance of work, there is no singular
definition of fairness, mainly because fairness is subjective and context
dependent. We propose a new fairness definition, motivated by the principle of
equity, that considers existing biases in the data and attempts to make
equitable decisions that account for these previous historical biases. We
formalize our definition of fairness, and motivate it with its appropriate
contexts. Next, we operationalize it for equitable classification. We perform
multiple automatic and human evaluations to show the effectiveness of our
definition and demonstrate its utility for aspects of fairness, such as the
feedback loop.
","[{'version': 'v1', 'created': 'Thu, 14 May 2020 23:19:38 GMT'}]",2020-05-18,"[['Mehrabi', 'Ninareh', ''], ['Huang', 'Yuzhong', ''], ['Morstatter', 'Fred', '']]"
1289695,2005.099,Deepak P,Deepak P and Savitha Sam Abraham,Fair Outlier Detection,"In Proceedings of The 21th International Conference on Web
  Information Systems Engineering (WISE 2020), Amsterdam and Leiden, The
  Netherlands",,,,cs.LG cs.AI cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An outlier detection method may be considered fair over specified sensitive
attributes if the results of outlier detection are not skewed towards
particular groups defined on such sensitive attributes. In this task, we
consider, for the first time to our best knowledge, the task of fair outlier
detection. In this work, we consider the task of fair outlier detection over
multiple multi-valued sensitive attributes (e.g., gender, race, religion,
nationality, marital status etc.). We propose a fair outlier detection method,
FairLOF, that is inspired by the popular LOF formulation for neighborhood-based
outlier detection. We outline ways in which unfairness could be induced within
LOF and develop three heuristic principles to enhance fairness, which form the
basis of the FairLOF method. Being a novel task, we develop an evaluation
framework for fair outlier detection, and use that to benchmark FairLOF on
quality and fairness of results. Through an extensive empirical evaluation over
real-world datasets, we illustrate that FairLOF is able to achieve significant
improvements in fairness at sometimes marginal degradations on result quality
as measured against the fairness-agnostic LOF method.
","[{'version': 'v1', 'created': 'Wed, 20 May 2020 08:02:41 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Aug 2020 20:18:41 GMT'}]",2020-08-06,"[['P', 'Deepak', ''], ['Abraham', 'Savitha Sam', '']]"
1290225,2005.1043,Jungseock Joo,"Jungseock Joo, Kimmo K\""arkk\""ainen","Gender Slopes: Counterfactual Fairness for Computer Vision Models by
  Attribute Manipulation",,,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated computer vision systems have been applied in many domains including
security, law enforcement, and personal devices, but recent reports suggest
that these systems may produce biased results, discriminating against people in
certain demographic groups. Diagnosing and understanding the underlying true
causes of model biases, however, are challenging tasks because modern computer
vision systems rely on complex black-box models whose behaviors are hard to
decode. We propose to use an encoder-decoder network developed for image
attribute manipulation to synthesize facial images varying in the dimensions of
gender and race while keeping other signals intact. We use these synthesized
images to measure counterfactual fairness of commercial computer vision
classifiers by examining the degree to which these classifiers are affected by
gender and racial cues controlled in the images, e.g., feminine faces may
elicit higher scores for the concept of nurse and lower scores for STEM-related
concepts. We also report the skewed gender representations in an online search
service on profession-related keywords, which may explain the origin of the
biases encoded in the models.
","[{'version': 'v1', 'created': 'Thu, 21 May 2020 02:33:28 GMT'}]",2020-05-22,"[['Joo', 'Jungseock', ''], ['Kärkkäinen', 'Kimmo', '']]"
1292769,2005.1297399999999,Nasim Sonboli,"Nasim Sonboli, Farzad Eskandanian, Robin Burke, Weiwen Liu, Bamshad
  Mobasher",Opportunistic Multi-aspect Fairness through Personalized Re-ranking,,,,,cs.IR cs.AI cs.LG stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As recommender systems have become more widespread and moved into areas with
greater social impact, such as employment and housing, researchers have begun
to seek ways to ensure fairness in the results that such systems produce. This
work has primarily focused on developing recommendation approaches in which
fairness metrics are jointly optimized along with recommendation accuracy.
However, the previous work had largely ignored how individual preferences may
limit the ability of an algorithm to produce fair recommendations. Furthermore,
with few exceptions, researchers have only considered scenarios in which
fairness is measured relative to a single sensitive feature or attribute (such
as race or gender). In this paper, we present a re-ranking approach to
fairness-aware recommendation that learns individual preferences across
multiple fairness dimensions and uses them to enhance provider fairness in
recommendation results. Specifically, we show that our opportunistic and
metric-agnostic approach achieves a better trade-off between accuracy and
fairness than prior re-ranking approaches and does so across multiple fairness
dimensions.
","[{'version': 'v1', 'created': 'Thu, 21 May 2020 04:25:20 GMT'}]",2020-05-28,"[['Sonboli', 'Nasim', ''], ['Eskandanian', 'Farzad', ''], ['Burke', 'Robin', ''], ['Liu', 'Weiwen', ''], ['Mobasher', 'Bamshad', '']]"
1294416,2005.14621,Ibrahim Alabdulmohsin,Ibrahim Alabdulmohsin,Fair Classification via Unconstrained Optimization,,,,,cs.LG cs.AI stat.AP stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Achieving the Bayes optimal binary classification rule subject to group
fairness constraints is known to be reducible, in some cases, to learning a
group-wise thresholding rule over the Bayes regressor. In this paper, we extend
this result by proving that, in a broader setting, the Bayes optimal fair
learning rule remains a group-wise thresholding rule over the Bayes regressor
but with a (possible) randomization at the thresholds. This provides a stronger
justification to the post-processing approach in fair classification, in which
(1) a predictor is learned first, after which (2) its output is adjusted to
remove bias. We show how the post-processing rule in this two-stage approach
can be learned quite efficiently by solving an unconstrained optimization
problem. The proposed algorithm can be applied to any black-box machine
learning model, such as deep neural networks, random forests and support vector
machines. In addition, it can accommodate many fairness criteria that have been
previously proposed in the literature, such as equalized odds and statistical
parity. We prove that the algorithm is Bayes consistent and motivate it,
furthermore, via an impossibility result that quantifies the tradeoff between
accuracy and fairness across multiple demographic groups. Finally, we conclude
by validating the algorithm on the Adult benchmark dataset.
","[{'version': 'v1', 'created': 'Thu, 21 May 2020 11:29:05 GMT'}]",2020-06-01,"[['Alabdulmohsin', 'Ibrahim', '']]"
1296284,2006.0177,Lily Hu,Lily Hu and Issa Kohler-Hausmann,What's Sex Got To Do With Fair Machine Learning?,"11 pages, 5 figures, ACM Conference on Fairness, Accountability, and
  Transparency",,10.1145/3351095.3375674,,cs.CY cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Debate about fairness in machine learning has largely centered around
competing definitions of what fairness or nondiscrimination between groups
requires. However, little attention has been paid to what precisely a group is.
Many recent approaches to ""fairness"" require one to specify a causal model of
the data generating process. These exercises make an implicit ontological
assumption that a racial or sex group is simply a collection of individuals who
share a given trait. We show this by exploring the formal assumption of
modularity in causal models, which holds that the dependencies captured by one
causal pathway are invariant to interventions on any other pathways. Causal
models of sex propose two substantive claims: 1) There exists a feature,
sex-on-its-own, that is an inherent trait of an individual that causally brings
about social phenomena external to it in the world; and 2) the relations
between sex and its effects can be modified in whichever ways and the former
feature would still retain the meaning that sex has in our world. We argue that
this ontological picture is false. Many of the ""effects"" that sex purportedly
""causes"" are in fact constitutive features of sex as a social status. They give
the social meaning of sex features, meanings that are precisely what make sex
discrimination a distinctively morally problematic type of action. Correcting
this conceptual error has a number of implications for how models can be used
to detect discrimination. Formal diagrams of constitutive relations present an
entirely different path toward reasoning about discrimination. Whereas causal
diagrams guide the construction of sophisticated modular counterfactuals,
constitutive diagrams identify a different kind of counterfactual as central to
an inquiry on discrimination: one that asks how the social meaning of a group
would be changed if its non-modular features were altered.
","[{'version': 'v1', 'created': 'Tue, 2 Jun 2020 16:51:39 GMT'}, {'version': 'v2', 'created': 'Thu, 4 Jun 2020 22:54:18 GMT'}]",2020-06-08,"[['Hu', 'Lily', ''], ['Kohler-Hausmann', 'Issa', '']]"
1296560,2006.0204600000002,Zuohui Fu,"Zuohui Fu, Yikun Xian, Ruoyuan Gao, Jieyu Zhao, Qiaoying Huang,
  Yingqiang Ge, Shuyuan Xu, Shijie Geng, Chirag Shah, Yongfeng Zhang, Gerard de
  Melo",Fairness-Aware Explainable Recommendation over Knowledge Graphs,,,,,cs.IR cs.AI cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There has been growing attention on fairness considerations recently,
especially in the context of intelligent decision making systems. Explainable
recommendation systems, in particular, may suffer from both explanation bias
and performance disparity. In this paper, we analyze different groups of users
according to their level of activity, and find that bias exists in
recommendation performance between different groups. We show that inactive
users may be more susceptible to receiving unsatisfactory recommendations, due
to insufficient training data for the inactive users, and that their
recommendations may be biased by the training records of more active users, due
to the nature of collaborative filtering, which leads to an unfair treatment by
the system. We propose a fairness constrained approach via heuristic re-ranking
to mitigate this unfairness problem in the context of explainable
recommendation over knowledge graphs. We experiment on several real-world
datasets with state-of-the-art knowledge graph-based explainable recommendation
algorithms. The promising results show that our algorithm is not only able to
provide high-quality explainable recommendations, but also reduces the
recommendation unfairness in several respects.
","[{'version': 'v1', 'created': 'Wed, 3 Jun 2020 05:04:38 GMT'}, {'version': 'v2', 'created': 'Sun, 28 Jun 2020 02:34:35 GMT'}]",2020-06-30,"[['Fu', 'Zuohui', ''], ['Xian', 'Yikun', ''], ['Gao', 'Ruoyuan', ''], ['Zhao', 'Jieyu', ''], ['Huang', 'Qiaoying', ''], ['Ge', 'Yingqiang', ''], ['Xu', 'Shuyuan', ''], ['Geng', 'Shijie', ''], ['Shah', 'Chirag', ''], ['Zhang', 'Yongfeng', ''], ['de Melo', 'Gerard', '']]"
1296698,2006.0218399999999,Agnes Cseh,"Katar\'ina Cechl\'arov\'a, \'Agnes Cseh, Zsuzsanna Jank\'o, Mari\'an
  Kire\v{s}, Luk\'a\v{s} Mi\v{n}o",A quest for a fair schedule: The Young Physicists' Tournament,,,,,cs.AI cs.DM math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Young Physicists Tournament is an established team-oriented scientific
competition between high school students from 37 countries on 5 continents. The
competition consists of scientific discussions called Fights. Three or four
teams participate in each Fight, each of whom presents a problem while rotating
the roles of Presenter, Opponent, Reviewer, and Observer among them.
  The rules of a few countries require that each team announce in advance 3
problems they will present at the national tournament. The task of the
organizers is to choose the composition of Fights in such a way that each team
presents each of its chosen problems exactly once and within a single Fight no
problem is presented more than once. Besides formalizing these feasibility
conditions, in this paper we formulate several additional fairness conditions
for tournament schedules. We show that the fulfillment of some of them can be
ensured by constructing suitable edge colorings in bipartite graphs. To find
fair schedules, we propose integer linear programs and test them on real as
well as randomly generated data.
","[{'version': 'v1', 'created': 'Wed, 3 Jun 2020 11:45:06 GMT'}]",2020-06-04,"[['Cechlárová', 'Katarína', ''], ['Cseh', 'Ágnes', ''], ['Jankó', 'Zsuzsanna', ''], ['Kireš', 'Marián', ''], ['Miňo', 'Lukáš', '']]"
1299292,2006.04778,Vijay Keswani,"L. Elisa Celis and Lingxiao Huang and Vijay Keswani and Nisheeth K.
  Vishnoi","Fair Classification with Noisy Protected Attributes: A Framework with
  Provable Guarantees",,,,,cs.LG cs.AI cs.CY cs.DS stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Due to the deployment of classification algorithms in a multitude of
applications directly and indirectly affecting people and society, developing
methods that are fair with respect to protected attributes such as gender or
race is crucial. However, protected attributes in datasets may be inaccurate
due to noise in the data collection or if the protected attributes are imputed
either in whole or in part. Such inaccuracies can prevent existing fair
classification algorithms from achieving their claimed fairness guarantees.
Motivated by this, recent works have studied the fair classification problem in
which a binary protected attribute is ""noisy"" (the protected type is flipped
with a known fixed probability) by either suggesting optimization using tighter
statistical or equalized odds constraints to counter the noise or by
identifying conditions under which prior equalized odds post-processing
algorithms can handle noisy attributes. We extend the study of noise-tolerant
fair classification to a very general setting. Our main contribution is an
optimization framework for learning a fair classifier in the presence of noisy
perturbations in the protected attributes that can be employed with linear and
linear-fractional class of fairness constraints, comes with probabilistic
guarantees on accuracy and fairness, and can handle multiple, non-binary
protected attributes. Empirically, we show that our framework can be used to
attain either statistical rate or false positive rate fairness guarantees with
a minimal loss in accuracy, even when the noise corruption is large in two
real-world datasets. Prior existing noisy fair classification approaches, on
the other hand, either do not always achieve the desired fairness levels or
suffer a larger loss in accuracy for guaranteeing high fairness compared to our
framework.
","[{'version': 'v1', 'created': 'Mon, 8 Jun 2020 17:52:48 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 14:18:18 GMT'}]",2020-10-15,"[['Celis', 'L. Elisa', ''], ['Huang', 'Lingxiao', ''], ['Keswani', 'Vijay', ''], ['Vishnoi', 'Nisheeth K.', '']]"
1299769,2006.05255,\'Angel Gonz\'alez-Prieto,"Jes\'us Bobadilla, Ra\'ul Lara-Cabrera, \'Angel Gonz\'alez-Prieto,
  Fernando Ortega",DeepFair: Deep Learning for Improving Fairness in Recommender Systems,"18 pages, 9 figures, 4 tables",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The lack of bias management in Recommender Systems leads to minority groups
receiving unfair recommendations. Moreover, the trade-off between equity and
precision makes it difficult to obtain recommendations that meet both criteria.
Here we propose a Deep Learning based Collaborative Filtering algorithm that
provides recommendations with an optimum balance between fairness and accuracy
without knowing demographic information about the users. Experimental results
show that it is possible to make fair recommendations without losing a
significant proportion of accuracy.
","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 13:39:38 GMT'}]",2020-06-11,"[['Bobadilla', 'Jesús', ''], ['Lara-Cabrera', 'Raúl', ''], ['González-Prieto', 'Ángel', ''], ['Ortega', 'Fernando', '']]"
1300477,2006.05963,Violet Xinying Chen,"Violet Xinying Chen, J.N. Hooker",Balancing Fairness and Efficiency in an Optimization Model,,,,,math.OC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Optimization models generally aim for efficiency by maximizing total benefit
or minimizing cost. Yet a trade-off between fairness and efficiency is an
important element of many practical decisions. We propose a principled and
practical method for balancing these two criteria in an optimization model.
Following a critical assessment of existing schemes, we define a set of social
welfare functions (SWFs) that combine Rawlsian leximax fairness and
utilitarianism and overcome some of the weaknesses of previous approaches. In
particular, we regulate the equity/efficiency trade-off with a single parameter
that has a meaningful interpretation in practical contexts. We formulate the
SWFs using mixed integer constraints and sequentially maximize them subject to
constraints that define the problem at hand. After providing practical
step-by-step instructions for implementation, we demonstrate the method on
problems of realistic size involving healthcare resource allocation and
disaster preparation. The solution times are modest, ranging from a fraction of
a second to 18 seconds for a given value of the trade-off parameter.
","[{'version': 'v1', 'created': 'Wed, 10 Jun 2020 17:24:42 GMT'}]",2020-06-11,"[['Chen', 'Violet Xinying', ''], ['Hooker', 'J. N.', '']]"
1300596,2006.06082,Subhabrata Majumdar,"Emily Dodwell, Cheryl Flynn, Balachander Krishnamurthy, Subhabrata
  Majumdar, Ritwik Mitra",System to Integrate Fairness Transparently: An Industry Approach,"11 pages, 2 figures",,,,cs.CY cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There have been significant research efforts to address the issue of
unintentional bias in Machine Learning (ML). Many well-known companies have
dealt with the fallout after the deployment of their products due to this
issue. In an industrial context, enterprises have large-scale ML solutions for
a broad class of use cases deployed for different swaths of customers. Trading
off the cost of detecting and mitigating bias across this landscape over the
lifetime of each use case against the risk of impact to the brand image is a
key consideration. We propose a framework for industrial uses that addresses
their methodological and mechanization needs. Our approach benefits from prior
experience handling security and privacy concerns as well as past internal ML
projects. Through significant reuse of bias handling ability at every stage in
the ML development lifecycle to guide users we can lower overall costs of
reducing bias.
","[{'version': 'v1', 'created': 'Wed, 10 Jun 2020 21:54:27 GMT'}]",2020-06-12,"[['Dodwell', 'Emily', ''], ['Flynn', 'Cheryl', ''], ['Krishnamurthy', 'Balachander', ''], ['Majumdar', 'Subhabrata', ''], ['Mitra', 'Ritwik', '']]"
1302420,2006.0790600000003,Shahin Jabbari,"Aida Rahmattalabi, Shahin Jabbari, Himabindu Lakkaraju, Phebe Vayanos,
  Eric Rice, Milind Tambe",Fair Influence Maximization: A Welfare Optimization Approach,,,,,cs.SI cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Several social interventions (e.g., suicide and HIV prevention) leverage
social network information to maximize outreach. Algorithmic influence
maximization techniques have been proposed to aid with the choice of
influencers (or peer leaders) in such interventions. Traditional algorithms for
influence maximization have not been designed with social interventions in
mind. As a result, they may disproportionately exclude minority communities
from the benefits of the intervention. This has motivated research on fair
influence maximization. Existing techniques require committing to a single
domain-specific fairness measure. This makes it hard for a decision maker to
meaningfully compare these notions and their resulting trade-offs across
different applications.
  We address these shortcomings by extending the principles of cardinal welfare
to the influence maximization setting, which is underlain by complex
connections between members of different communities. We generalize the theory
regarding these principles and show under what circumstances these principles
can be satisfied by a welfare function. We then propose a family of welfare
functions that are governed by a single inequity aversion parameter which
allows a decision maker to study task-dependent trade-offs between fairness and
total influence and effectively trade off quantities like influence gap by
varying this parameter. We use these welfare functions as a fairness notion to
rule out undesirable allocations. We show that the resulting optimization
problem is monotone and submodular and can be solved with optimality
guarantees. Finally, we carry out a detailed experimental analysis on synthetic
and real social networks and should that high welfare can be achieved without
sacrificing the total influence significantly. Interestingly we can show there
exists welfare functions that empirically satisfy all of the principles.
","[{'version': 'v1', 'created': 'Sun, 14 Jun 2020 14:08:10 GMT'}]",2020-06-16,"[['Rahmattalabi', 'Aida', ''], ['Jabbari', 'Shahin', ''], ['Lakkaraju', 'Himabindu', ''], ['Vayanos', 'Phebe', ''], ['Rice', 'Eric', ''], ['Tambe', 'Milind', '']]"
1303202,2006.08688,Ke Yang,"Ke Yang, Joshua R. Loftus, Julia Stoyanovich",Causal intersectionality for fair ranking,,,,,cs.LG cs.AI stat.AP stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we propose a causal modeling approach to intersectional
fairness, and a flexible, task-specific method for computing intersectionally
fair rankings. Rankings are used in many contexts, ranging from Web search
results to college admissions, but causal inference for fair rankings has
received limited attention. Additionally, the growing literature on causal
fairness has directed little attention to intersectionality. By bringing these
issues together in a formal causal framework we make the application of
intersectionality in fair machine learning explicit, connected to important
real world effects and domain knowledge, and transparent about technical
limitations. We experimentally evaluate our approach on real and synthetic
datasets, exploring its behaviour under different structural assumptions.
","[{'version': 'v1', 'created': 'Mon, 15 Jun 2020 18:57:46 GMT'}]",2020-06-17,"[['Yang', 'Ke', ''], ['Loftus', 'Joshua R.', ''], ['Stoyanovich', 'Julia', '']]"
1304599,2006.10085,Mehrdad Ghadiri,"Mehrdad Ghadiri, Samira Samadi, Santosh Vempala",Fair k-Means Clustering,"17 pages, 9 figures",,,,cs.LG cs.AI cs.CG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We show that the popular $k$-means clustering algorithm (Lloyd's heuristic),
used for a variety of scientific data, can result in outcomes that are
unfavorable to subgroups of data (e.g., demographic groups). Such biased
clusterings can have deleterious implications for human-centric applications
such as resource allocation. We present a fair $k$-means objective and
algorithm to choose cluster centers that provide equitable costs for different
groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for
$k$-means, inheriting its simplicity, efficiency, and stability. In comparison
with standard Lloyd's, we find that on benchmark data sets, Fair-Lloyd exhibits
unbiased performance by ensuring that all groups have balanced costs in the
output $k$-clustering, while incurring a negligible increase in running time,
thus making it a viable fair option wherever $k$-means is currently used.
","[{'version': 'v1', 'created': 'Wed, 17 Jun 2020 18:05:17 GMT'}]",2020-06-19,"[['Ghadiri', 'Mehrdad', ''], ['Samadi', 'Samira', ''], ['Vempala', 'Santosh', '']]"
1305430,2006.1091600000002,Seyed Abdulaziz Esmaeili,"Seyed A. Esmaeili, Brian Brubach, Leonidas Tsepenekas, John P.
  Dickerson",Probabilistic Fair Clustering,,,,,cs.LG cs.AI cs.DS stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In clustering problems, a central decision-maker is given a complete metric
graph over vertices and must provide a clustering of vertices that minimizes
some objective function. In fair clustering problems, vertices are endowed with
a color (e.g., membership in a group), and the features of a valid clustering
might also include the representation of colors in that clustering. Prior work
in fair clustering assumes complete knowledge of group membership. In this
paper, we generalize prior work by assuming imperfect knowledge of group
membership through probabilistic assignments. We present clustering algorithms
in this more general setting with approximation ratio guarantees. We also
address the problem of ""metric membership"", where different groups have a
notion of order and distance. Experiments are conducted using our proposed
algorithms as well as baselines to validate our approach and also surface
nuanced concerns when group membership is not known deterministically.
","[{'version': 'v1', 'created': 'Fri, 19 Jun 2020 01:34:21 GMT'}]",2020-06-22,"[['Esmaeili', 'Seyed A.', ''], ['Brubach', 'Brian', ''], ['Tsepenekas', 'Leonidas', ''], ['Dickerson', 'John P.', '']]"
1306251,2006.11737,Deepak Vijaykeerthy,"Philips George John, Deepak Vijaykeerthy, Diptikalyan Saha",Verifying Individual Fairness in Machine Learning Models,"An extended version of the paper accepted at UAI 2020, 12 pages, code
  is available at https://github.com/philips-george/ifv-uai-2020",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of whether a given decision model, working with
structured data, has individual fairness. Following the work of Dwork, a model
is individually biased (or unfair) if there is a pair of valid inputs which are
close to each other (according to an appropriate metric) but are treated
differently by the model (different class label, or large difference in
output), and it is unbiased (or fair) if no such pair exists. Our objective is
to construct verifiers for proving individual fairness of a given model, and we
do so by considering appropriate relaxations of the problem. We construct
verifiers which are sound but not complete for linear classifiers, and
kernelized polynomial/radial basis function classifiers. We also report the
experimental results of evaluating our proposed algorithms on publicly
available datasets.
","[{'version': 'v1', 'created': 'Sun, 21 Jun 2020 08:37:54 GMT'}]",2020-06-23,"[['John', 'Philips George', ''], ['Vijaykeerthy', 'Deepak', ''], ['Saha', 'Diptikalyan', '']]"
1306631,2006.12117,Sheeba Samuel,"Sheeba Samuel, Frank L\""offler, Birgitta K\""onig-Ries","Machine Learning Pipelines: Provenance, Reproducibility and FAIR Data
  Principles","Accepted at ProvenanceWeek 2020
  (https://iitdbgroup.github.io/ProvenanceWeek2020/)",,,,cs.LG cs.AI stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Machine learning (ML) is an increasingly important scientific tool supporting
decision making and knowledge generation in numerous fields. With this, it also
becomes more and more important that the results of ML experiments are
reproducible. Unfortunately, that often is not the case. Rather, ML, similar to
many other disciplines, faces a reproducibility crisis. In this paper, we
describe our goals and initial steps in supporting the end-to-end
reproducibility of ML pipelines. We investigate which factors beyond the
availability of source code and datasets influence reproducibility of ML
experiments. We propose ways to apply FAIR data practices to ML workflows. We
present our preliminary results on the role of our tool, ProvBook, in capturing
and comparing provenance of ML experiments and their reproducibility using
Jupyter Notebooks.
","[{'version': 'v1', 'created': 'Mon, 22 Jun 2020 10:17:34 GMT'}]",2020-06-23,"[['Samuel', 'Sheeba', ''], ['Löffler', 'Frank', ''], ['König-Ries', 'Birgitta', '']]"
1307270,2006.1275600000001,Kinjal Basu,"Kinjal Basu, Cyrus DiCiccio, Heloise Logan, Noureddine El Karoui",A Framework for Fairness in Two-Sided Marketplaces,"15 pages, 7 Tables",,,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many interesting problems in the Internet industry can be framed as a
two-sided marketplace problem. Examples include search applications and
recommender systems showing people, jobs, movies, products, restaurants, etc.
Incorporating fairness while building such systems is crucial and can have a
deep social and economic impact (applications include job recommendations,
recruiters searching for candidates, etc.). In this paper, we propose a
definition and develop an end-to-end framework for achieving fairness while
building such machine learning systems at scale. We extend prior work to
develop an optimization framework that can tackle fairness constraints from
both the source and destination sides of the marketplace, as well as dynamic
aspects of the problem. The framework is flexible enough to adapt to different
definitions of fairness and can be implemented in very large-scale settings. We
perform simulations to show the efficacy of our approach.
","[{'version': 'v1', 'created': 'Tue, 23 Jun 2020 04:47:37 GMT'}]",2020-06-24,"[['Basu', 'Kinjal', ''], ['DiCiccio', 'Cyrus', ''], ['Logan', 'Heloise', ''], ['Karoui', 'Noureddine El', '']]"
1311259,2006.16745,Sami Zhioua,"Karima Makhlouf, Sami Zhioua, Catuscia Palamidessi",On the Applicability of ML Fairness Notions,,,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  ML-based predictive systems are increasingly used to support decisions with a
critical impact on individuals' lives such as college admission, job hiring,
child custody, criminal risk assessment, etc. As a result, fairness emerged as
an important requirement to guarantee that predictive systems do not
discriminate against specific individuals or entire sub-populations, in
particular, minorities. Given the inherent subjectivity of viewing the concept
of fairness, several notions of fairness have been introduced in the
literature. This paper is a survey of fairness notions that, unlike other
surveys in the literature, addresses the question of ""which notion of fairness
is most suited to a given real-world scenario and why?"". Our attempt to answer
this question consists in (1) identifying the set of fairness-related
characteristics of the real-world scenario at hand, (2) analyzing the behavior
of each fairness notion, and then (3) fitting these two elements to recommend
the most suitable fairness notion in every specific setup. The results are
summarized in a decision diagram that can be used by practitioners and policy
makers to navigate the relatively large catalogue of fairness notions.
","[{'version': 'v1', 'created': 'Tue, 30 Jun 2020 13:01:06 GMT'}]",2020-07-01,"[['Makhlouf', 'Karima', ''], ['Zhioua', 'Sami', ''], ['Palamidessi', 'Catuscia', '']]"
1314404,2007.02890,Robert Long,Robert Long,"Fairness in machine learning: against false positive rate equality as a
  measure of fairness",,,,,cs.CY cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As machine learning informs increasingly consequential decisions, different
metrics have been proposed for measuring algorithmic bias or unfairness. Two
popular fairness measures are calibration and equality of false positive rate.
Each measure seems intuitively important, but notably, it is usually impossible
to satisfy both measures. For this reason, a large literature in machine
learning speaks of a fairness tradeoff between these two measures. This framing
assumes that both measures are, in fact, capturing something important. To
date, philosophers have not examined this crucial assumption, and examined to
what extent each measure actually tracks a normatively important property. This
makes this inevitable statistical conflict, between calibration and false
positive rate equality, an important topic for ethics. In this paper, I give an
ethical framework for thinking about these measures and argue that, contrary to
initial appearances, false positive rate equality does not track anything about
fairness, and thus sets an incoherent standard for evaluating the fairness of
algorithms.
","[{'version': 'v1', 'created': 'Mon, 6 Jul 2020 17:03:58 GMT'}]",2020-07-07,"[['Long', 'Robert', '']]"
1316957,2007.05443,Hansol Lee,Ren\'e F. Kizilcec and Hansol Lee,Algorithmic Fairness in Education,"Forthcoming in W. Holmes & K. Porayska-Pomsta (Eds.), Ethics in
  Artificial Intelligence in Education, Taylor & Francis",,,,cs.CY cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Data-driven predictive models are increasingly used in education to support
students, instructors, and administrators. However, there are concerns about
the fairness of the predictions and uses of these algorithmic systems. In this
introduction to algorithmic fairness in education, we draw parallels to prior
literature on educational access, bias, and discrimination, and we examine core
components of algorithmic systems (measurement, model learning, and action) to
identify sources of bias and discrimination in the process of developing and
deploying these systems. Statistical, similarity-based, and causal notions of
fairness are reviewed and contrasted in the way they apply in educational
contexts. Recommendations for policy makers and developers of educational
technology offer guidance for how to promote algorithmic fairness in education.
","[{'version': 'v1', 'created': 'Fri, 10 Jul 2020 15:35:10 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Sep 2020 01:25:59 GMT'}]",2020-09-21,"[['Kizilcec', 'René F.', ''], ['Lee', 'Hansol', '']]"
1317030,2007.05516,Pavan Ravishankar,"Pavan Ravishankar, Pranshu Malviya, Balaraman Ravindran","A Causal Linear Model to Quantify Edge Unfairness for Unfair Edge
  Prioritization and Discrimination Removal","Accepted in the Workshop on Law and Machine Learning, ICML 2020;
  First two authors contributed equally",,,,cs.AI cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The dataset can be generated by an unfair mechanism in numerous settings. For
instance, a judicial system is unfair if it rejects the bail plea of an accused
based on the race. To mitigate the unfairness in the procedure generating the
dataset, we need to identify the sources of unfairness, quantify the unfairness
in these sources, quantify how these sources affect the overall unfairness, and
prioritize the sources before addressing the real-world issues underlying them.
Prior work of (Zhang, et. al, 2017) identifies and removes discrimination after
data is generated but does not suggest a methodology to mitigate unfairness in
the data generation phase. We use the notion of an unfair edge, same as
(Chiappa, et. al, 2018), to be the source of discrimination and quantify
unfairness along an unfair edge. We also quantify overall unfairness in a
particular decision towards a subset of sensitive attributes in terms of edge
unfairness and measure the sensitivity of the former when the latter is varied.
Using the formulation of cumulative unfairness in terms of edge unfairness, we
alter the discrimination removal methodology discussed in (Zhang, et. al, 2017)
by not formulating it as an optimization problem. This helps in getting rid of
constraints that grow exponentially in the number of sensitive attributes and
values taken by them. Finally, we discuss a priority algorithm for policymakers
to address the real-world issues underlying the edges that result in
unfairness. The experimental section validates the linear model assumption made
to quantify edge unfairness.
","[{'version': 'v1', 'created': 'Fri, 10 Jul 2020 17:50:41 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Jul 2020 17:02:58 GMT'}, {'version': 'v3', 'created': 'Thu, 10 Sep 2020 17:50:23 GMT'}]",2020-09-11,"[['Ravishankar', 'Pavan', ''], ['Malviya', 'Pranshu', ''], ['Ravindran', 'Balaraman', '']]"
1317538,2007.06024,Kailash Karthik Saravanakumar,Kailash Karthik S,The Impossibility Theorem of Machine Fairness -- A Causal Perspective,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the increasing pervasive use of machine learning in social and economic
settings, there has been an interest in the notion of machine bias in the AI
community. Models trained on historic data reflect the biases that exist in
society and are propagated to the future through their decisions. A recent
study conducted by ProPublica revealed that the COMPAS recidivism prediction
tool was biased against the African-American community. There are three
prominent metrics of fairness used in the community, and it has been
statistically proved that it is impossible to satisfy them at the same time --
which has led to ambiguity about the definition of fairness. In this report,
causal perspective to the impossibility theorem of fairness is presented along
with a causal goal for machine fairness.
","[{'version': 'v1', 'created': 'Sun, 12 Jul 2020 15:56:15 GMT'}]",2020-07-14,"[['S', 'Kailash Karthik', '']]"
1318213,2007.06699,Nisarg Shah,"Safwan Hossain, Evi Micha, Nisarg Shah",Fair Algorithms for Multi-Agent Multi-Armed Bandits,,,,,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a multi-agent variant of the classical multi-armed bandit problem,
in which there are N agents and K arms, and pulling an arm generates a
(possibly different) stochastic reward to each agent. Unlike the classical
multi-armed bandit problem, the goal is not to learn the ""best arm"", as each
agent may perceive a different arm as best for her. Instead, we seek to learn a
fair distribution over arms. Drawing on a long line of research in economics
and computer science, we use the Nash social welfare as our notion of fairness.
We design multi-agent variants of three classic multi-armed bandit algorithms,
and show that they achieve sublinear regret, now measured in terms of the Nash
social welfare.
","[{'version': 'v1', 'created': 'Mon, 13 Jul 2020 21:20:04 GMT'}]",2020-07-15,"[['Hossain', 'Safwan', ''], ['Micha', 'Evi', ''], ['Shah', 'Nisarg', '']]"
1326717,2007.15203,Vijay  Menon,Vijay Menon and Kate Larson,"Algorithmic Stability in Fair Allocation of Indivisible Goods Among Two
  Agents",,,,,cs.GT cs.AI cs.DS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a notion of algorithmic stability for scenarios where cardinal
preferences are elicited. Informally, our definition captures the idea that an
agent should not experience a large change in their utility as long as they
make ""small"" or ""innocuous"" mistakes while reporting their preferences. We
study this notion in the context of fair and efficient allocations of
indivisible goods among two agents, and show that it is impossible to achieve
exact stability along with even a weak notion of fairness and even approximate
efficiency. As a result, we propose two relaxations to stability, namely,
approximate-stability and weak-approximate-stability, and show how existing
algorithms in the fair division literature that guarantee fair and efficient
outcomes perform poorly with respect to these relaxations. This leads us to the
explore the possibility of designing new algorithms that are more stable.
Towards this end we present a general characterization result for pairwise
maximin share allocations, and in turn use it to design an algorithm that is
approximately-stable and guarantees a pairwise maximin share and Pareto optimal
allocation for two agents. Finally, we present a simple framework that can be
used to modify existing fair and efficient algorithms in order to ensure that
they also achieve weak-approximate-stability.
","[{'version': 'v1', 'created': 'Thu, 30 Jul 2020 03:09:02 GMT'}]",2020-07-31,"[['Menon', 'Vijay', ''], ['Larson', 'Kate', '']]"
1326784,2007.15270,Gopiram Roshan Lal,G Roshan Lal and Sahin Cem Geyik and Krishnaram Kenthapadi,Fairness-Aware Online Personalization,"Accepted in RecSys 2020, FAccTRec Workshop: Responsible
  Recommendation",,,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decision making in crucial applications such as lending, hiring, and college
admissions has witnessed increasing use of algorithmic models and techniques as
a result of a confluence of factors such as ubiquitous connectivity, ability to
collect, aggregate, and process large amounts of fine-grained data using cloud
computing, and ease of access to applying sophisticated machine learning
models. Quite often, such applications are powered by search and recommendation
systems, which in turn make use of personalized ranking algorithms. At the same
time, there is increasing awareness about the ethical and legal challenges
posed by the use of such data-driven systems. Researchers and practitioners
from different disciplines have recently highlighted the potential for such
systems to discriminate against certain population groups, due to biases in the
datasets utilized for learning their underlying recommendation models. We
present a study of fairness in online personalization settings involving the
ranking of individuals. Starting from a fair warm-start machine-learned model,
we first demonstrate that online personalization can cause the model to learn
to act in an unfair manner if the user is biased in his/her responses. For this
purpose, we construct a stylized model for generating training data with
potentially biased features as well as potentially biased labels and quantify
the extent of bias that is learned by the model when the user responds in a
biased manner as in many real-world scenarios. We then formulate the problem of
learning personalized models under fairness constraints and present a
regularization based approach for mitigating biases in machine learning. We
demonstrate the efficacy of our approach through extensive simulations with
different parameter settings. Code:
https://github.com/groshanlal/Fairness-Aware-Online-Personalization
","[{'version': 'v1', 'created': 'Thu, 30 Jul 2020 07:16:17 GMT'}, {'version': 'v2', 'created': 'Sun, 6 Sep 2020 10:03:27 GMT'}]",2020-09-08,"[['Lal', 'G Roshan', ''], ['Geyik', 'Sahin Cem', ''], ['Kenthapadi', 'Krishnaram', '']]"
1327631,2007.16117,Jakub Marecek,"Ramen Ghosh and Jakub Marecek and Wynita M. Griggs and Matheus Souza
  and Robert N. Shorten",Predictability and Fairness in Social Sensing,"14 pages, 6 figures",,,,eess.SP cs.AI cs.SY eess.SY math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In many applications, one may benefit from the collaborative collection of
data for sensing a physical phenomenon, which is known as social sensing. We
show how to make social sensing (1) predictable, in the sense of guaranteeing
that the number of queries per participant will be independent of the initial
state, in expectation, even when the population of participants varies over
time, and (2) fair, in the sense of guaranteeing that the number of queries per
participant will be equalised among the participants, in expectation, even when
the population of participants varies over time.
  In a use case, we consider a large, high-density network of participating
parked vehicles. When awoken by an administrative centre, this network proceeds
to search for moving, missing entities of interest using RFID-based techniques.
We regulate the number and geographical distribution of the parked vehicles
that are ""Switched On"" and thus actively searching for the moving entity of
interest. In doing so, we seek to conserve vehicular energy consumption while,
at the same time, maintaining good geographical coverage of the city such that
the moving entity of interest is likely to be located within an acceptable time
frame. Which vehicle participants are ""Switched On"" at any point in time is
determined periodically through the use of stochastic techniques. This is
illustrated on the example of a missing Alzheimer's patient in Melbourne,
Australia.
","[{'version': 'v1', 'created': 'Fri, 31 Jul 2020 15:04:08 GMT'}]",2020-08-03,"[['Ghosh', 'Ramen', ''], ['Marecek', 'Jakub', ''], ['Griggs', 'Wynita M.', ''], ['Souza', 'Matheus', ''], ['Shorten', 'Robert N.', '']]"
1327929,2008.00207,Simeng Bian,"Simeng Bian, Xi Huang, Ziyu Shao",Online Task Scheduling for Fog Computing with Multi-Resource Fairness,,,,,cs.NI cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In fog computing systems, one key challenge is online task scheduling, i.e.,
to decide the resource allocation for tasks that are continuously generated
from end devices. The design is challenging because of various uncertainties
manifested in fog computing systems; e.g., tasks' resource demands remain
unknown before their actual arrivals. Recent works have applied deep
reinforcement learning (DRL) techniques to conduct online task scheduling and
improve various objectives. However, they overlook the multi-resource fairness
for different tasks, which is key to achieving fair resource sharing among
tasks but in general non-trivial to achieve. Thusly, it is still an open
problem to design an online task scheduling scheme with multi-resource
fairness. In this paper, we address the above challenges. Particularly, by
leveraging DRL techniques and adopting the idea of dominant resource fairness
(DRF), we propose FairTS, an online task scheduling scheme that learns directly
from experience to effectively shorten average task slowdown while ensuring
multi-resource fairness among tasks. Simulation results show that FairTS
outperforms state-of-the-art schemes with an ultra-low task slowdown and better
resource fairness.
","[{'version': 'v1', 'created': 'Sat, 1 Aug 2020 07:57:40 GMT'}]",2020-08-04,"[['Bian', 'Simeng', ''], ['Huang', 'Xi', ''], ['Shao', 'Ziyu', '']]"
1329936,2008.02214,Jesse Russell,Jesse Russell,"Machine Learning Fairness in Justice Systems: Base Rates, False
  Positives, and False Negatives",,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning best practice statements have proliferated, but there is a
lack of consensus on what the standards should be. For fairness standards in
particular, there is little guidance on how fairness might be achieved in
practice. Specifically, fairness in errors (both false negatives and false
positives) can pose a problem of how to set weights, how to make unavoidable
tradeoffs, and how to judge models that present different kinds of errors
across racial groups. This paper considers the consequences of having higher
rates of false positives for one racial group and higher rates of false
negatives for another racial group. The paper examines how different errors in
justice settings can present problems for machine learning applications, the
limits of computation for resolving tradeoffs, and how solutions might have to
be crafted through courageous conversations with leadership, line workers,
stakeholders, and impacted communities.
","[{'version': 'v1', 'created': 'Wed, 5 Aug 2020 16:31:40 GMT'}]",2020-08-06,"[['Russell', 'Jesse', '']]"
1335068,2008.07346,Federico Ruggeri,"Federico Ruggeri, Francesca Lagioia, Marco Lippi, Paolo Torroni",Memory networks for consumer protection:unfairness exposed,,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has demonstrated how data-driven AI methods can leverage consumer
protection by supporting the automated analysis of legal documents. However, a
shortcoming of data-driven approaches is poor explainability. We posit that in
this domain useful explanations of classifier outcomes can be provided by
resorting to legal rationales. We thus consider several configurations of
memory-augmented neural networks where rationales are given a special role in
the modeling of context knowledge. Our results show that rationales not only
contribute to improve the classification accuracy, but are also able to offer
meaningful, natural language explanations of otherwise opaque classifier
outcomes.
","[{'version': 'v1', 'created': 'Fri, 24 Jul 2020 14:25:54 GMT'}]",2020-08-18,"[['Ruggeri', 'Federico', ''], ['Lagioia', 'Francesca', ''], ['Lippi', 'Marco', ''], ['Torroni', 'Paolo', '']]"
1335155,2008.07433,Sriram Vasudevan,"Sriram Vasudevan, Krishnaram Kenthapadi",LiFT: A Scalable Framework for Measuring Fairness in ML Applications,Accepted for publication in CIKM 2020,,10.1145/3340531.3412705,,cs.LG cs.AI cs.CY cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many internet applications are powered by machine learned models, which are
usually trained on labeled datasets obtained through either implicit / explicit
user feedback signals or human judgments. Since societal biases may be present
in the generation of such datasets, it is possible for the trained models to be
biased, thereby resulting in potential discrimination and harms for
disadvantaged groups. Motivated by the need for understanding and addressing
algorithmic bias in web-scale ML systems and the limitations of existing
fairness toolkits, we present the LinkedIn Fairness Toolkit (LiFT), a framework
for scalable computation of fairness metrics as part of large ML systems. We
highlight the key requirements in deployed settings, and present the design of
our fairness measurement system. We discuss the challenges encountered in
incorporating fairness tools in practice and the lessons learned during
deployment at LinkedIn. Finally, we provide open problems based on practical
experience.
","[{'version': 'v1', 'created': 'Fri, 14 Aug 2020 03:55:31 GMT'}]",2020-08-18,"[['Vasudevan', 'Sriram', ''], ['Kenthapadi', 'Krishnaram', '']]"
1335495,2008.07773,Muhammad Umer Siddique,"Umer Siddique, Paul Weng, Matthieu Zimmer","Learning Fair Policies in Multiobjective (Deep) Reinforcement Learning
  with Average and Discounted Rewards",,,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As the operations of autonomous systems generally affect simultaneously
several users, it is crucial that their designs account for fairness
considerations. In contrast to standard (deep) reinforcement learning (RL), we
investigate the problem of learning a policy that treats its users equitably.
In this paper, we formulate this novel RL problem, in which an objective
function, which encodes a notion of fairness that we formally define, is
optimized. For this problem, we provide a theoretical discussion where we
examine the case of discounted rewards and that of average rewards. During this
analysis, we notably derive a new result in the standard RL setting, which is
of independent interest: it states a novel bound on the approximation error
with respect to the optimal average reward of that of a policy optimal for the
discounted reward. Since learning with discounted rewards is generally easier,
this discussion further justifies finding a fair policy for the average reward
by learning a fair policy for the discounted reward. Thus, we describe how
several classic deep RL algorithms can be adapted to our fair optimization
problem, and we validate our approach with extensive experiments in three
different domains.
","[{'version': 'v1', 'created': 'Tue, 18 Aug 2020 07:17:53 GMT'}]",2020-08-19,"[['Siddique', 'Umer', ''], ['Weng', 'Paul', ''], ['Zimmer', 'Matthieu', '']]"
1338602,2008.10880,Rik Helwegen MSc,"Rik Helwegen, Christos Louizos and Patrick Forr\'e",Improving Fair Predictions Using Variational Inference In Causal Models,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The importance of algorithmic fairness grows with the increasing impact
machine learning has on people's lives. Recent work on fairness metrics shows
the need for causal reasoning in fairness constraints. In this work, a
practical method named FairTrade is proposed for creating flexible prediction
models which integrate fairness constraints on sensitive causal paths. The
method uses recent advances in variational inference in order to account for
unobserved confounders. Further, a method outline is proposed which uses the
causal mechanism estimates to audit black box models. Experiments are conducted
on simulated data and on a real dataset in the context of detecting unlawful
social welfare. This research aims to contribute to machine learning techniques
which honour our ethical and legal boundaries.
","[{'version': 'v1', 'created': 'Tue, 25 Aug 2020 08:27:11 GMT'}]",2020-08-26,"[['Helwegen', 'Rik', ''], ['Louizos', 'Christos', ''], ['Forré', 'Patrick', '']]"
1340844,2008.13122,Vincent Grari,"Vincent Grari, Sylvain Lamprier, Marcin Detyniecki",Adversarial Learning for Counterfactual Fairness,"11 pages, 5 figures",,,,cs.LG cs.AI cs.CY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, fairness has become an important topic in the machine
learning research community. In particular, counterfactual fairness aims at
building prediction models which ensure fairness at the most individual level.
Rather than globally considering equity over the entire population, the idea is
to imagine what any individual would look like with a variation of a given
attribute of interest, such as a different gender or race for instance.
Existing approaches rely on Variational Auto-encoding of individuals, using
Maximum Mean Discrepancy (MMD) penalization to limit the statistical dependence
of inferred representations with their corresponding sensitive attributes. This
enables the simulation of counterfactual samples used for training the target
fair model, the goal being to produce similar outcomes for every alternate
version of any individual. In this work, we propose to rely on an adversarial
neural learning approach, that enables more powerful inference than with MMD
penalties, and is particularly better fitted for the continuous setting, where
values of sensitive attributes cannot be exhaustively enumerated. Experiments
show significant improvements in term of counterfactual fairness for both the
discrete and the continuous settings.
","[{'version': 'v1', 'created': 'Sun, 30 Aug 2020 09:06:03 GMT'}]",2020-09-01,"[['Grari', 'Vincent', ''], ['Lamprier', 'Sylvain', ''], ['Detyniecki', 'Marcin', '']]"
1342945,2009.01442,Srinivasan Ravichandran,"Srinivasan Ravichandran, Drona Khurana, Bharath Venkatesh, Narayanan
  Unny Edakunni",FairXGBoost: Fairness-aware Classification in XGBoost,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Highly regulated domains such as finance have long favoured the use of
machine learning algorithms that are scalable, transparent, robust and yield
better performance. One of the most prominent examples of such an algorithm is
XGBoost. Meanwhile, there is also a growing interest in building fair and
unbiased models in these regulated domains and numerous bias-mitigation
algorithms have been proposed to this end. However, most of these
bias-mitigation methods are restricted to specific model families such as
logistic regression or support vector machine models, thus leaving modelers
with a difficult decision of choosing between fairness from the bias-mitigation
algorithms and scalability, transparency, performance from algorithms such as
XGBoost. We aim to leverage the best of both worlds by proposing a fair variant
of XGBoost that enjoys all the advantages of XGBoost, while also matching the
levels of fairness from the state-of-the-art bias-mitigation algorithms.
Furthermore, the proposed solution requires very little in terms of changes to
the original XGBoost library, thus making it easy for adoption. We provide an
empirical analysis of our proposed method on standard benchmark datasets used
in the fairness community.
","[{'version': 'v1', 'created': 'Thu, 3 Sep 2020 04:08:23 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 05:14:38 GMT'}]",2020-10-08,"[['Ravichandran', 'Srinivasan', ''], ['Khurana', 'Drona', ''], ['Venkatesh', 'Bharath', ''], ['Edakunni', 'Narayanan Unny', '']]"
1343037,2009.01534,Yossi Adi,"Shahar Segal, Yossi Adi, Benny Pinkas, Carsten Baum, Chaya Ganesh,
  Joseph Keshet",Fairness in the Eyes of the Data: Certifying Machine-Learning Models,,,,,cs.AI cs.CR cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a framework that allows to certify the fairness degree of a model
based on an interactive and privacy-preserving test. The framework verifies any
trained model, regardless of its training process and architecture. Thus, it
allows us to evaluate any deep learning model on multiple fairness definitions
empirically. We tackle two scenarios, where either the test data is privately
available only to the tester or is publicly known in advance, even to the model
creator. We investigate the soundness of the proposed approach using
theoretical analysis and present statistical guarantees for the interactive
test. Finally, we provide a cryptographic technique to automate fairness
testing and certified inference with only black-box access to the model at hand
while hiding the participants' sensitive data.
","[{'version': 'v1', 'created': 'Thu, 3 Sep 2020 09:22:39 GMT'}]",2020-09-04,"[['Segal', 'Shahar', ''], ['Adi', 'Yossi', ''], ['Pinkas', 'Benny', ''], ['Baum', 'Carsten', ''], ['Ganesh', 'Chaya', ''], ['Keshet', 'Joseph', '']]"
1343926,2009.02423,Harshal Chaudhari,"Harshal A. Chaudhari, Sangdi Lin, Ondrej Linda",A General Framework for Fairness in Multistakeholder Recommendations,"7 pages, 3 figures",,,,cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contemporary recommender systems act as intermediaries on multi-sided
platforms serving high utility recommendations from sellers to buyers. Such
systems attempt to balance the objectives of multiple stakeholders including
sellers, buyers, and the platform itself. The difficulty in providing
recommendations that maximize the utility for a buyer, while simultaneously
representing all the sellers on the platform has lead to many interesting
research problems.Traditionally, they have been formulated as integer linear
programs which compute recommendations for all the buyers together in an
\emph{offline} fashion, by incorporating coverage constraints so that the
individual sellers are proportionally represented across all the recommended
items. Such approaches can lead to unforeseen biases wherein certain buyers
consistently receive low utility recommendations in order to meet the global
seller coverage constraints. To remedy this situation, we propose a general
formulation that incorporates seller coverage objectives alongside individual
buyer objectives in a real-time personalized recommender system. In addition,
we leverage highly scalable submodular optimization algorithms to provide
recommendations to each buyer with provable theoretical quality bounds.
Furthermore, we empirically evaluate the efficacy of our approach using data
from an online real-estate marketplace.
","[{'version': 'v1', 'created': 'Fri, 4 Sep 2020 23:54:06 GMT'}]",2020-09-09,"[['Chaudhari', 'Harshal A.', ''], ['Lin', 'Sangdi', ''], ['Linda', 'Ondrej', '']]"
1344093,2009.02590,Nasim Sonboli,"Nasim Sonboli, Robin Burke, Nicholas Mattei, Farzad Eskandanian, Tian
  Gao","""And the Winner Is..."": Dynamic Lotteries for Multi-group Fairness-Aware
  Recommendation",,,,,cs.IR cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As recommender systems are being designed and deployed for an increasing
number of socially-consequential applications, it has become important to
consider what properties of fairness these systems exhibit. There has been
considerable research on recommendation fairness. However, we argue that the
previous literature has been based on simple, uniform and often uni-dimensional
notions of fairness assumptions that do not recognize the real-world
complexities of fairness-aware applications. In this paper, we explicitly
represent the design decisions that enter into the trade-off between accuracy
and fairness across multiply-defined and intersecting protected groups,
supporting multiple fairness metrics. The framework also allows the recommender
to adjust its performance based on the historical view of recommendations that
have been delivered over a time horizon, dynamically rebalancing between
fairness concerns. Within this framework, we formulate lottery-based mechanisms
for choosing between fairness concerns, and demonstrate their performance in
two recommendation domains.
","[{'version': 'v1', 'created': 'Sat, 5 Sep 2020 20:15:14 GMT'}]",2020-09-08,"[['Sonboli', 'Nasim', ''], ['Burke', 'Robin', ''], ['Mattei', 'Nicholas', ''], ['Eskandanian', 'Farzad', ''], ['Gao', 'Tian', '']]"
1345886,2009.04383,Venkata Sriram Siddhardh Nadendla,Mukund Telukunta and Venkata Sriram Siddhardh Nadendla,"On the Identification of Fair Auditors to Evaluate Recommender Systems
  based on a Novel Non-Comparative Fairness Notion","10 pages, Accepted to FAccTRec-2020",,,,cs.CY cs.AI cs.HC cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decision-support systems are information systems that offer support to
people's decisions in various applications such as judiciary, real-estate and
banking sectors. Lately, these support systems have been found to be
discriminatory in the context of many practical deployments. In an attempt to
evaluate and mitigate these biases, algorithmic fairness literature has been
nurtured using notions of comparative justice, which relies primarily on
comparing two/more individuals or groups within the society that is supported
by such systems. However, such a fairness notion is not very useful in the
identification of fair auditors who are hired to evaluate latent biases within
decision-support systems. As a solution, we introduce a paradigm shift in
algorithmic fairness via proposing a new fairness notion based on the principle
of non-comparative justice. Assuming that the auditor makes fairness
evaluations based on some (potentially unknown) desired properties of the
decision-support system, the proposed fairness notion compares the system's
outcome with that of the auditor's desired outcome. We show that the proposed
fairness notion also provides guarantees in terms of comparative fairness
notions by proving that any system can be deemed fair from the perspective of
comparative fairness (e.g. individual fairness and statistical parity) if it is
non-comparatively fair with respect to an auditor who has been deemed fair with
respect to the same fairness notions. We also show that the converse holds true
in the context of individual fairness. A brief discussion is also presented
regarding how our fairness notion can be used to identify fair and reliable
auditors, and how we can use them to quantify biases in decision-support
systems.
","[{'version': 'v1', 'created': 'Wed, 9 Sep 2020 16:04:41 GMT'}]",2020-09-10,"[['Telukunta', 'Mukund', ''], ['Nadendla', 'Venkata Sriram Siddhardh', '']]"
1345944,2009.04441,Diego Antognini,"Kirtan Padh, Diego Antognini, Emma Lejal Glaude, Boi Faltings, Claudiu
  Musat","Addressing Fairness in Classification with a Model-Agnostic
  Multi-Objective Algorithm","Under review. 11 pages, 3 figures, 4 tables",,,,cs.LG cs.AI cs.IR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The goal of fairness in classification is to learn a classifier that does not
discriminate against groups of individuals based on sensitive attributes, such
as race and gender. One approach to designing fair algorithms is to use
relaxations of fairness notions as regularization terms or in a constrained
optimization problem. We observe that the hyperbolic tangent function can
approximate the indicator function. We leverage this property to define a
differentiable relaxation that approximates fairness notions provably better
than existing relaxations. In addition, we propose a model-agnostic
multi-objective architecture that can simultaneously optimize for multiple
fairness notions and multiple sensitive attributes and supports all statistical
parity-based notions of fairness. We use our relaxation with the
multi-objective architecture to learn fair classifiers. Experiments on public
datasets show that our method suffers a significantly lower loss of accuracy
than current debiasing algorithms relative to the unconstrained model.
","[{'version': 'v1', 'created': 'Wed, 9 Sep 2020 17:40:24 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Sep 2020 17:17:00 GMT'}]",2020-09-15,"[['Padh', 'Kirtan', ''], ['Antognini', 'Diego', ''], ['Glaude', 'Emma Lejal', ''], ['Faltings', 'Boi', ''], ['Musat', 'Claudiu', '']]"
1346143,2009.04640,Arjun Prakash,"Lauren Boswell, Arjun Prakash",On the Fairness of 'Fake' Data in Legal AI,Submitted to the Artificial Intelligence Ethics Journal,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The economics of smaller budgets and larger case numbers necessitates the use
of AI in legal proceedings. We examine the concept of disparate impact and how
biases in the training data lead to the search for fairer AI. This paper seeks
to begin the discourse on what such an implementation would actually look like
with a criticism of pre-processing methods in a legal context . We outline how
pre-processing is used to correct biased data and then examine the legal
implications of effectively changing cases in order to achieve a fairer outcome
including the black box problem and the slow encroachment on legal precedent.
Finally we present recommendations on how to avoid the pitfalls of
pre-processed data with methods that either modify the classifier or correct
the output in the final step.
","[{'version': 'v1', 'created': 'Thu, 10 Sep 2020 02:23:19 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Sep 2020 08:35:55 GMT'}]",2020-09-14,"[['Boswell', 'Lauren', ''], ['Prakash', 'Arjun', '']]"
1346786,2009.05283,David Berend,"Yushi Cao, David Berend, Palina Tolmach, Moshe Levy, Guy Amit, Asaf
  Shabtai, Yuval Elovici, Yang Liu","Fairness Matters -- A Data-Driven Framework Towards Fair and High
  Performing Facial Recognition Systems","32 pages, 10 Figures, pre-print, adjusted title",,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Facial recognition technologies are widely used in governmental and
industrial applications. Together with the advancements in deep learning (DL),
human-centric tasks such as accurate age prediction based on face images become
feasible. However, the issue of fairness when predicting the age for different
ethnicity and gender remains an open problem. Policing systems use age to
estimate the likelihood of someone to commit a crime, where younger suspects
tend to be more likely involved. Unfair age prediction may lead to unfair
treatment of humans not only in crime prevention but also in marketing,
identity acquisition and authentication. Therefore, this work follows two
parts. First, an empirical study is conducted evaluating performance and
fairness of state-of-the-art systems for age prediction including baseline and
most recent works of academia and the main industrial service providers (Amazon
AWS and Microsoft Azure). Building on the findings we present a novel approach
to mitigate unfairness and enhance performance, using distribution-aware
dataset curation and augmentation. Distribution-awareness is based on
out-of-distribution detection which is utilized to validate equal and diverse
DL system behavior towards e.g. ethnicity and gender. In total we train 24 DNN
models and utilize one million data points to assess performance and fairness
of the state-of-the-art for face recognition algorithms. We demonstrate an
improvement in mean absolute age prediction error from 7.70 to 3.39 years and a
4-fold increase in fairness towards ethnicity when compared to related work.
Utilizing the presented methodology we are able to outperform leading industry
players such as Amazon AWS or Microsoft Azure in both fairness and age
prediction accuracy and provide the necessary guidelines to assess quality and
enhance face recognition systems based on DL techniques.
","[{'version': 'v1', 'created': 'Fri, 11 Sep 2020 08:32:36 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Sep 2020 01:14:56 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Sep 2020 05:52:51 GMT'}]",2020-09-17,"[['Cao', 'Yushi', ''], ['Berend', 'David', ''], ['Tolmach', 'Palina', ''], ['Levy', 'Moshe', ''], ['Amit', 'Guy', ''], ['Shabtai', 'Asaf', ''], ['Elovici', 'Yuval', ''], ['Liu', 'Yang', '']]"
1347754,2009.06251,Boris Ruf,Boris Ruf and Marcin Detyniecki,Active Fairness Instead of Unawareness,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The possible risk that AI systems could promote discrimination by reproducing
and enforcing unwanted bias in data has been broadly discussed in research and
society. Many current legal standards demand to remove sensitive attributes
from data in order to achieve ""fairness through unawareness"". We argue that
this approach is obsolete in the era of big data where large datasets with
highly correlated attributes are common. In the contrary, we propose the active
use of sensitive attributes with the purpose of observing and controlling any
kind of discrimination, and thus leading to fair results.
","[{'version': 'v1', 'created': 'Mon, 14 Sep 2020 08:14:17 GMT'}]",2020-09-15,"[['Ruf', 'Boris', ''], ['Detyniecki', 'Marcin', '']]"
1347892,2009.06389,Sahib Singh,"Tom Farrand, Fatemehsadat Mireshghallah, Sahib Singh, Andrew Trask","Neither Private Nor Fair: Impact of Data Imbalance on Utility and
  Fairness in Differential Privacy","5 pages, 5 figures",,,,cs.LG cs.AI cs.CR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deployment of deep learning in different fields and industries is growing day
by day due to its performance, which relies on the availability of data and
compute. Data is often crowd-sourced and contains sensitive information about
its contributors, which leaks into models that are trained on it. To achieve
rigorous privacy guarantees, differentially private training mechanisms are
used. However, it has recently been shown that differential privacy can
exacerbate existing biases in the data and have disparate impacts on the
accuracy of different subgroups of data. In this paper, we aim to study these
effects within differentially private deep learning. Specifically, we aim to
study how different levels of imbalance in the data affect the accuracy and the
fairness of the decisions made by the model, given different levels of privacy.
We demonstrate that even small imbalances and loose privacy guarantees can
cause disparate impacts.
","[{'version': 'v1', 'created': 'Thu, 10 Sep 2020 18:35:49 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 16:00:29 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Oct 2020 11:55:05 GMT'}]",2020-10-06,"[['Farrand', 'Tom', ''], ['Mireshghallah', 'Fatemehsadat', ''], ['Singh', 'Sahib', ''], ['Trask', 'Andrew', '']]"
1348019,2009.06516,Debabrota Basu,"Bishwamittra Ghosh, Debabrota Basu, Kuldeep S. Meel",Justicia: A Stochastic SAT Approach to Formally Verify Fairness,"24 pages, 7 figures, 5 theorems",,,,cs.AI cs.CY cs.LG cs.LO,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As a technology ML is oblivious to societal good or bad, and thus, the field
of fair machine learning has stepped up to propose multiple mathematical
definitions, algorithms, and systems to ensure different notions of fairness in
ML applications. Given the multitude of propositions, it has become imperative
to formally verify the fairness metrics satisfied by different algorithms on
different datasets. In this paper, we propose a \textit{stochastic
satisfiability} (SSAT) framework, Justicia, that formally verifies different
fairness measures of supervised learning algorithms with respect to the
underlying data distribution. We instantiate Justicia on multiple
classification and bias mitigation algorithms, and datasets to verify different
fairness metrics, such as disparate impact, statistical parity, and equalized
odds. Justicia is scalable, accurate, and operates on non-Boolean and compound
sensitive attributes unlike existing distribution-based verifiers, such as
FairSquare and VeriFair. Being distribution-based by design, Justicia is more
robust than the verifiers, such as AIF360, that operate on specific test
samples. We also theoretically bound the finite-sample error of the verified
fairness measure.
","[{'version': 'v1', 'created': 'Mon, 14 Sep 2020 15:23:51 GMT'}]",2020-09-15,"[['Ghosh', 'Bishwamittra', ''], ['Basu', 'Debabrota', ''], ['Meel', 'Kuldeep S.', '']]"
1350534,2009.09031,YooJung Choi,"YooJung Choi, Meihua Dang, Guy Van den Broeck",Group Fairness by Probabilistic Modeling with Latent Fair Decisions,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning systems are increasingly being used to make impactful
decisions such as loan applications and criminal justice risk assessments, and
as such, ensuring fairness of these systems is critical. This is often
challenging as the labels in the data are biased. This paper studies learning
fair probability distributions from biased data by explicitly modeling a latent
variable that represents a hidden, unbiased label. In particular, we aim to
achieve demographic parity by enforcing certain independencies in the learned
model. We also show that group fairness guarantees are meaningful only if the
distribution used to provide those guarantees indeed captures the real-world
data. In order to closely model the data distribution, we employ probabilistic
circuits, an expressive and tractable probabilistic model, and propose an
algorithm to learn them from incomplete data. We evaluate our approach on a
synthetic dataset in which observed labels indeed come from fair labels but
with added bias, and demonstrate that the fair labels are successfully
retrieved. Moreover, we show on real-world datasets that our approach not only
is a better model than existing methods of how the data was generated but also
achieves competitive accuracy.
","[{'version': 'v1', 'created': 'Fri, 18 Sep 2020 19:13:23 GMT'}]",2020-09-22,"[['Choi', 'YooJung', ''], ['Dang', 'Meihua', ''], ['Broeck', 'Guy Van den', '']]"
1352994,2009.11491,Anoop Krishnan Upendran Nair,"Anoop Krishnan, Ali Almadan, Ajita Rattani","Understanding Fairness of Gender Classification Algorithms Across
  Gender-Race Groups","19th IEEE International Conference On Machine Learning And
  Applications 2020 | Miami, Florida",,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated gender classification has important applications in many domains,
such as demographic research, law enforcement, online advertising, as well as
human-computer interaction. Recent research has questioned the fairness of this
technology across gender and race. Specifically, the majority of the studies
raised the concern of higher error rates of the face-based gender
classification system for darker-skinned people like African-American and for
women. However, to date, the majority of existing studies were limited to
African-American and Caucasian only. The aim of this paper is to investigate
the differential performance of the gender classification algorithms across
gender-race groups. To this aim, we investigate the impact of (a) architectural
differences in the deep learning algorithms and (b) training set imbalance, as
a potential source of bias causing differential performance across gender and
race. Experimental investigations are conducted on two latest large-scale
publicly available facial attribute datasets, namely, UTKFace and FairFace. The
experimental results suggested that the algorithms with architectural
differences varied in performance with consistency towards specific gender-race
groups. For instance, for all the algorithms used, Black females (Black race in
general) always obtained the least accuracy rates. Middle Eastern males and
Latino females obtained higher accuracy rates most of the time. Training set
imbalance further widens the gap in the unequal accuracy rates across all
gender-race groups. Further investigations using facial landmarks suggested
that facial morphological differences due to the bone structure influenced by
genetic and environmental factors could be the cause of the least performance
of Black females and Black race, in general.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 04:56:10 GMT'}]",2020-09-25,"[['Krishnan', 'Anoop', ''], ['Almadan', 'Ali', ''], ['Rattani', 'Ajita', '']]"
1354065,2009.12562,Ferdinando Fioretto,"Cuong Tran, Ferdinando Fioretto, Pascal Van Hentenryck","Differentially Private and Fair Deep Learning: A Lagrangian Dual
  Approach",,,,,cs.LG cs.AI cs.CR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A critical concern in data-driven decision making is to build models whose
outcomes do not discriminate against some demographic groups, including gender,
ethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of
the sensitive attributes is essential, while, in practice, these attributes may
not be available due to legal and ethical requirements. To address this
challenge, this paper studies a model that protects the privacy of the
individuals sensitive information while also allowing it to learn
non-discriminatory predictors. The method relies on the notion of differential
privacy and the use of Lagrangian duality to design neural networks that can
accommodate fairness constraints while guaranteeing the privacy of sensitive
attributes. The paper analyses the tension between accuracy, privacy, and
fairness and the experimental evaluation illustrates the benefits of the
proposed model on several prediction tasks.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 10:50:33 GMT'}]",2020-09-29,"[['Tran', 'Cuong', ''], ['Fioretto', 'Ferdinando', ''], ['Van Hentenryck', 'Pascal', '']]"
1355019,2009.13516,Chen Zhao,"Chen Zhao, Changbin Li, Jincheng Li, Feng Chen",Fair Meta-Learning For Few-Shot Classification,"2020 IEEE International Conference on Knowledge Graph (ICKG). arXiv
  admin note: text overlap with arXiv:2009.11406",,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Artificial intelligence nowadays plays an increasingly prominent role in our
life since decisions that were once made by humans are now delegated to
automated systems. A machine learning algorithm trained based on biased data,
however, tends to make unfair predictions. Developing classification algorithms
that are fair with respect to protected attributes of the data thus becomes an
important problem. Motivated by concerns surrounding the fairness effects of
sharing and few-shot machine learning tools, such as the Model Agnostic
Meta-Learning framework, we propose a novel fair fast-adapted few-shot
meta-learning approach that efficiently mitigates biases during meta-train by
ensuring controlling the decision boundary covariance that between the
protected variable and the signed distance from the feature vectors to the
decision boundary. Through extensive experiments on two real-world image
benchmarks over three state-of-the-art meta-learning algorithms, we empirically
demonstrate that our proposed approach efficiently mitigates biases on model
output and generalizes both accuracy and fairness to unseen tasks with a
limited amount of training samples.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 22:33:47 GMT'}]",2020-09-29,"[['Zhao', 'Chen', ''], ['Li', 'Changbin', ''], ['Li', 'Jincheng', ''], ['Chen', 'Feng', '']]"
1355153,2009.13650,Joseph Near,"Krystal Maughan, Joseph P. Near",Towards a Measure of Individual Fairness for Deep Learning,Presented at MD4SG '20,,,,cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning has produced big advances in artificial intelligence, but
trained neural networks often reflect and amplify bias in their training data,
and thus produce unfair predictions. We propose a novel measure of individual
fairness, called prediction sensitivity, that approximates the extent to which
a particular prediction is dependent on a protected attribute. We show how to
compute prediction sensitivity using standard automatic differentiation
capabilities present in modern deep learning frameworks, and present
preliminary empirical results suggesting that prediction sensitivity may be
effective for measuring bias in individual predictions.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 21:53:21 GMT'}]",2020-09-30,"[['Maughan', 'Krystal', ''], ['Near', 'Joseph P.', '']]"
1357800,2010.01470,Lequn Luke Wang,Lequn Wang and Thorsten Joachims,Fairness and Diversity for Rankings in Two-Sided Markets,,,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ranking items by their probability of relevance has long been the goal of
conventional ranking systems. While this maximizes traditional criteria of
ranking performance, there is a growing understanding that it is an
oversimplification in online platforms that serve not only a diverse user
population, but also the producers of the items. In particular, ranking
algorithms are expected to be fair in how they serve all groups of users -- not
just the majority group -- and they also need to be fair in how they divide
exposure among the items. These fairness considerations can partially be met by
adding diversity to the rankings, as done in several recent works, but we show
in this paper that user fairness, item fairness and diversity are fundamentally
different concepts. In particular, we find that algorithms that consider only
one of the three desiderata can fail to satisfy and even harm the other two. To
overcome this shortcoming, we present the first ranking algorithm that
explicitly enforces all three desiderata. The algorithm optimizes user and item
fairness as a convex optimization problem which can be solved optimally. From
its solution, a ranking policy can be derived via a new Birkhoff-von Neumann
decomposition algorithm that optimizes diversity. Beyond the theoretical
analysis, we provide a comprehensive empirical evaluation on a new benchmark
dataset to show the effectiveness of the proposed ranking algorithm on
controlling the three desiderata and the interplay between them.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 02:53:09 GMT'}]",2020-10-06,"[['Wang', 'Lequn', ''], ['Joachims', 'Thorsten', '']]"
1358872,2010.02542,Sakshi Udeshi,Ezekiel Soremekun and Sakshi Udeshi and Sudipta Chattopadhyay,Astraea: Grammar-based Fairness Testing,,,,,cs.SE cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Software often produces biased outputs. In particular, machine learning (ML)
based software are known to produce erroneous predictions when processing
discriminatory inputs. Such unfair program behavior can be caused by societal
bias. In the last few years, Amazon, Microsoft and Google have provided
software services that produce unfair outputs, mostly due to societal bias
(e.g. gender or race). In such events, developers are saddled with the task of
conducting fairness testing. Fairness testing is challenging; developers are
tasked with generating discriminatory inputs that reveal and explain biases.
  We propose a grammar-based fairness testing approach (called ASTRAEA) which
leverages context-free grammars to generate discriminatory inputs that reveal
fairness violations in software systems. Using probabilistic grammars, ASTRAEA
also provides fault diagnosis by isolating the cause of observed software bias.
ASTRAEA's diagnoses facilitate the improvement of ML fairness.
  ASTRAEA was evaluated on 18 software systems that provide three major natural
language processing (NLP) services. In our evaluation, ASTRAEA generated
fairness violations with a rate of ~18%. ASTRAEA generated over 573K
discriminatory test cases and found over 102K fairness violations. Furthermore,
ASTRAEA improves software fairness by ~76%, via model-retraining.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 08:19:01 GMT'}]",2020-10-07,"[['Soremekun', 'Ezekiel', ''], ['Udeshi', 'Sakshi', ''], ['Chattopadhyay', 'Sudipta', '']]"
1359558,2010.03228,Souradip Chakraborty Mr,"Souradip Chakraborty, Ekansh Verma, Saswata Sahoo, Jyotishka Datta","FairMixRep : Self-supervised Robust Representation Learning for
  Heterogeneous Data with Fairness constraints",This paper has been accepted at the ICDM'2020 DLC Workshop,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Representation Learning in a heterogeneous space with mixed variables of
numerical and categorical types has interesting challenges due to its complex
feature manifold. Moreover, feature learning in an unsupervised setup, without
class labels and a suitable learning loss function, adds to the problem
complexity. Further, the learned representation and subsequent predictions
should not reflect discriminatory behavior towards certain sensitive groups or
attributes. The proposed feature map should preserve maximum variations present
in the data and needs to be fair with respect to the sensitive variables. We
propose, in the first phase of our work, an efficient encoder-decoder framework
to capture the mixed-domain information. The second phase of our work focuses
on de-biasing the mixed space representations by adding relevant fairness
constraints. This ensures minimal information loss between the representations
before and after the fairness-preserving projections. Both the information
content and the fairness aspect of the final representation learned has been
validated through several metrics where it shows excellent performance. Our
work (FairMixRep) addresses the problem of Mixed Space Fair Representation
learning from an unsupervised perspective and learns a Universal representation
that is timely, unique, and a novel research contribution.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 07:23:02 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 06:12:38 GMT'}]",2020-10-15,"[['Chakraborty', 'Souradip', ''], ['Verma', 'Ekansh', ''], ['Sahoo', 'Saswata', ''], ['Datta', 'Jyotishka', '']]"
1359995,2010.03665,Pedro Saleiro,"Andr\'e F. Cruz, Pedro Saleiro, Catarina Bel\'em, Carlos Soares, Pedro
  Bizarro",A Bandit-Based Algorithm for Fairness-Aware Hyperparameter Optimization,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Considerable research effort has been guided towards algorithmic fairness but
there is still no major breakthrough. In practice, an exhaustive search over
all possible techniques and hyperparameters is needed to find optimal
fairness-accuracy trade-offs. Hence, coupled with the lack of tools for ML
practitioners, real-world adoption of bias reduction methods is still scarce.
To address this, we present Fairband, a bandit-based fairness-aware
hyperparameter optimization (HO) algorithm. Fairband is conceptually simple,
resource-efficient, easy to implement, and agnostic to both the objective
metrics, model types and the hyperparameter space being explored. Moreover, by
introducing fairness notions into HO, we enable seamless and efficient
integration of fairness objectives into real-world ML pipelines. We compare
Fairband with popular HO methods on four real-world decision-making datasets.
We show that Fairband can efficiently navigate the fairness-accuracy trade-off
through hyperparameter optimization. Furthermore, without extra training cost,
it consistently finds configurations attaining substantially improved fairness
at a comparatively small decrease in predictive accuracy.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 21:35:16 GMT'}]",2020-10-09,"[['Cruz', 'André F.', ''], ['Saleiro', 'Pedro', ''], ['Belém', 'Catarina', ''], ['Soares', 'Carlos', ''], ['Bizarro', 'Pedro', '']]"
1360263,2010.03933,Debo Cheng,"Zhenlong Xu (1), Jixue Liu (1), Debo Cheng (1), Jiuyong Li (1), Lin
  Liu (1), Ke Wang (2) ((1) STEM, Univsersity of South Austrlia, (2) Simon
  Frasier University)",Assessing the Fairness of Classifiers with Collider Bias,"9pages,7figures",,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The increasing maturity of machine learning technologies and their
applications to decisions relate to everyday decision making have brought
concerns about the fairness of the decisions. However, current fairness
assessment systems often suffer from collider bias, which leads to a spurious
association between the protected attribute and the outcomes. To achieve
fairness evaluation on prediction models at the individual level, in this
paper, we develop the causality-based theorems to support the use of direct
causal effect estimation for fairness assessment on a given a classifier
without access to original training data. Based on the theorems, an unbiased
situation test method is presented to assess individual fairness of predictions
by a classifier, through the elimination of the impact of collider bias of the
classifier on the fairness assessment. Extensive experiments have been
performed on synthetic and real-world data to evaluate the performance of the
proposed method. The experimental results show that the proposed method reduces
bias significantly.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 12:41:02 GMT'}]",2020-10-09,"[['Xu', 'Zhenlong', ''], ['Liu', 'Jixue', ''], ['Cheng', 'Debo', ''], ['Li', 'Jiuyong', ''], ['Liu', 'Lin', ''], ['Wang', 'Ke', '']]"
1360316,2010.03986,James Hickey,"Gareth P. Jones, James M. Hickey, Pietro G. Di Stefano, Charanpal
  Dhanjal, Laura C. Stoddart and Vlasios Vasileiou","Metrics and methods for a systematic comparison of fairness-aware
  machine learning algorithms","12 pages, 6 figures",,,,cs.LG cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding and removing bias from the decisions made by machine learning
models is essential to avoid discrimination against unprivileged groups.
Despite recent progress in algorithmic fairness, there is still no clear answer
as to which bias-mitigation approaches are most effective. Evaluation
strategies are typically use-case specific, rely on data with unclear bias, and
employ a fixed policy to convert model outputs to decision outcomes. To address
these problems, we performed a systematic comparison of a number of popular
fairness algorithms applicable to supervised classification. Our study is the
most comprehensive of its kind. It utilizes three real and four synthetic
datasets, and two different ways of converting model outputs to decisions. It
considers fairness, predictive-performance, calibration quality, and speed of
28 different modelling pipelines, corresponding to both fairness-unaware and
fairness-aware algorithms. We found that fairness-unaware algorithms typically
fail to produce adequately fair models and that the simplest algorithms are not
necessarily the fairest ones. We also found that fairness-aware algorithms can
induce fairness without material drops in predictive power. Finally, we found
that dataset idiosyncracies (e.g., degree of intrinsic unfairness, nature of
correlations) do affect the performance of fairness-aware approaches. Our
results allow the practitioner to narrow down the approach(es) they would like
to adopt without having to know in advance their fairness requirements.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 13:58:09 GMT'}]",2020-10-09,"[['Jones', 'Gareth P.', ''], ['Hickey', 'James M.', ''], ['Di Stefano', 'Pietro G.', ''], ['Dhanjal', 'Charanpal', ''], ['Stoddart', 'Laura C.', ''], ['Vasileiou', 'Vlasios', '']]"
1361170,2010.04840,Jiahao Chen,Leo de Castro and Jiahao Chen and Antigoni Polychroniadou,CryptoCredit: Securely Training Fair Models,8 pages,"Proceedings of the 1st ACM International Conference on AI in
  Finance (ICAIF '20), October 15-16, 2020, New York, NY, USA",10.1145/3383455.3422567,,cs.LG cs.AI cs.CR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When developing models for regulated decision making, sensitive features like
age, race and gender cannot be used and must be obscured from model developers
to prevent bias. However, the remaining features still need to be tested for
correlation with sensitive features, which can only be done with the knowledge
of those features. We resolve this dilemma using a fully homomorphic encryption
scheme, allowing model developers to train linear regression and logistic
regression models and test them for possible bias without ever revealing the
sensitive features in the clear. We demonstrate how it can be applied to
leave-one-out regression testing, and show using the adult income data set that
our method is practical to run.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 23:05:37 GMT'}]",2020-10-13,"[['de Castro', 'Leo', ''], ['Chen', 'Jiahao', ''], ['Polychroniadou', 'Antigoni', '']]"
1362217,2010.05887,Farzan Masrour,"Farzan Masrour, Pang-Ning Tan, Abdol-Hossein Esfahanian",Fairness Perception from a Network-Centric Perspective,,,,,cs.SI cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Algorithmic fairness is a major concern in recent years as the influence of
machine learning algorithms becomes more widespread. In this paper, we
investigate the issue of algorithmic fairness from a network-centric
perspective. Specifically, we introduce a novel yet intuitive function known as
network-centric fairness perception and provide an axiomatic approach to
analyze its properties. Using a peer-review network as case study, we also
examine its utility in terms of assessing the perception of fairness in paper
acceptance decisions. We show how the function can be extended to a group
fairness metric known as fairness visibility and demonstrate its relationship
to demographic parity. We also illustrate a potential pitfall of the fairness
visibility measure that can be exploited to mislead individuals into perceiving
that the algorithmic decisions are fair. We demonstrate how the problem can be
alleviated by increasing the local neighborhood size of the fairness perception
function.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 06:35:03 GMT'}]",2020-10-13,"[['Masrour', 'Farzan', ''], ['Tan', 'Pang-Ning', ''], ['Esfahanian', 'Abdol-Hossein', '']]"
1362443,2010.06113,Shubham Sharma,"Shubham Sharma, Alan H. Gee, David Paydarfar, Joydeep Ghosh",FaiR-N: Fair and Robust Neural Networks for Structured Data,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness in machine learning is crucial when individuals are subject to
automated decisions made by models in high-stake domains. Organizations that
employ these models may also need to satisfy regulations that promote
responsible and ethical A.I. While fairness metrics relying on comparing model
error rates across subpopulations have been widely investigated for the
detection and mitigation of bias, fairness in terms of the equalized ability to
achieve recourse for different protected attribute groups has been relatively
unexplored. We present a novel formulation for training neural networks that
considers the distance of data points to the decision boundary such that the
new objective: (1) reduces the average distance to the decision boundary
between two groups for individuals subject to a negative outcome in each group,
i.e. the network is more fair with respect to the ability to obtain recourse,
and (2) increases the average distance of data points to the boundary to
promote adversarial robustness. We demonstrate that training with this loss
yields more fair and robust neural networks with similar accuracies to models
trained without it. Moreover, we qualitatively motivate and empirically show
that reducing recourse disparity across groups also improves fairness measures
that rely on error rates. To the best of our knowledge, this is the first time
that recourse capabilities across groups are considered to train fairer neural
networks, and a relation between error rates based fairness and recourse based
fairness is investigated.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 01:53:15 GMT'}]",2020-10-14,"[['Sharma', 'Shubham', ''], ['Gee', 'Alan H.', ''], ['Paydarfar', 'David', ''], ['Ghosh', 'Joydeep', '']]"
1362859,2010.06529,"Julius von K\""ugelgen","Julius von K\""ugelgen, Umang Bhatt, Amir-Hossein Karimi, Isabel
  Valera, Adrian Weller, Bernhard Sch\""olkopf",On the Fairness of Causal Algorithmic Recourse,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While many recent works have studied the problem of algorithmic fairness from
the perspective of predictions, here we investigate the fairness of recourse
actions recommended to individuals to recover from an unfavourable
classification. To this end, we propose two new fairness criteria at the group
and individual level which---unlike prior work on equalising the average
distance from the decision boundary across protected groups---are based on a
causal framework that explicitly models relationships between input features,
thereby allowing to capture downstream effects of recourse actions performed in
the physical world. We explore how our criteria relate to others, such as
counterfactual fairness, and show that fairness of recourse is complementary to
fairness of prediction. We then investigate how to enforce fair recourse in the
training of the classifier. Finally, we discuss whether fairness violations in
the data generating process revealed by our criteria may be better addressed by
societal interventions and structural changes to the system, as opposed to
constraints on the classifier.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 16:35:06 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 09:48:33 GMT'}]",2020-10-15,"[['von Kügelgen', 'Julius', ''], ['Bhatt', 'Umang', ''], ['Karimi', 'Amir-Hossein', ''], ['Valera', 'Isabel', ''], ['Weller', 'Adrian', ''], ['Schölkopf', 'Bernhard', '']]"
1363150,2010.06820,James Foulds,"Kamrun Naher Keya, Rashidul Islam, Shimei Pan, Ian Stockwell, James R.
  Foulds",Equitable Allocation of Healthcare Resources with Fair Cox Models,"AAAI Fall Symposium on AI in Government and Public Sector (AAAI
  FSS-20), 2020",,,,cs.LG cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Healthcare programs such as Medicaid provide crucial services to vulnerable
populations, but due to limited resources, many of the individuals who need
these services the most languish on waiting lists. Survival models, e.g. the
Cox proportional hazards model, can potentially improve this situation by
predicting individuals' levels of need, which can then be used to prioritize
the waiting lists. Providing care to those in need can prevent
institutionalization for those individuals, which both improves quality of life
and reduces overall costs. While the benefits of such an approach are clear,
care must be taken to ensure that the prioritization process is fair or
independent of demographic information-based harmful stereotypes. In this work,
we develop multiple fairness definitions for survival models and corresponding
fair Cox proportional hazards models to ensure equitable allocation of
healthcare resources. We demonstrate the utility of our methods in terms of
fairness and predictive accuracy on two publicly available survival datasets.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 06:08:15 GMT'}]",2020-10-15,"[['Keya', 'Kamrun Naher', ''], ['Islam', 'Rashidul', ''], ['Pan', 'Shimei', ''], ['Stockwell', 'Ian', ''], ['Foulds', 'James R.', '']]"
1363384,2010.07054,Deepak P,Deepak P and Savitha Sam Abraham,Representativity Fairness in Clustering,In 12th ACM Web Science Conference (WebSci 2020),,10.1145/3394231.3397910,,cs.CY cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Incorporating fairness constructs into machine learning algorithms is a topic
of much societal importance and recent interest. Clustering, a fundamental task
in unsupervised learning that manifests across a number of web data scenarios,
has also been subject of attention within fair ML research. In this paper, we
develop a novel notion of fairness in clustering, called representativity
fairness. Representativity fairness is motivated by the need to alleviate
disparity across objects' proximity to their assigned cluster representatives,
to aid fairer decision making. We illustrate the importance of representativity
fairness in real-world decision making scenarios involving clustering and
provide ways of quantifying objects' representativity and fairness over it. We
develop a new clustering formulation, RFKM, that targets to optimize for
representativity fairness along with clustering quality. Inspired by the
$K$-Means framework, RFKM incorporates novel loss terms to formulate an
objective function. The RFKM objective and optimization approach guides it
towards clustering configurations that yield higher representativity fairness.
Through an empirical evaluation over a variety of public datasets, we establish
the effectiveness of our method. We illustrate that we are able to
significantly improve representativity fairness at only marginal impact to
clustering quality.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 21:50:06 GMT'}]",2020-10-15,"[['P', 'Deepak', ''], ['Abraham', 'Savitha Sam', '']]"
1363579,2010.07249,Elliot Creager,"Elliot Creager, J\""orn-Henrik Jacobsen, Richard Zemel","Exchanging Lessons Between Algorithmic Fairness and Domain
  Generalization",,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Standard learning approaches are designed to perform well on average for the
data distribution available at training time. Developing learning approaches
that are not overly sensitive to the training distribution is central to
research on domain- or out-of-distribution generalization, robust optimization
and fairness. In this work we focus on links between research on domain
generalization and algorithmic fairness -- where performance under a distinct
but related test distributions is studied -- and show how the two fields can be
mutually beneficial. While domain generalization methods typically rely on
knowledge of disjoint ""domains"" or ""environments"", ""sensitive"" label
information indicating which demographic groups are at risk of discrimination
is often used in the fairness literature. Drawing inspiration from recent
fairness approaches that improve worst-case performance without knowledge of
sensitive groups, we propose a novel domain generalization method that handles
the more realistic scenario where environment partitions are not provided. We
then show theoretically and empirically how different partitioning schemes can
lead to increased or decreased generalization performance, enabling us to
outperform Invariant Risk Minimization with handcrafted environments in
multiple cases. We also show how a re-interpretation of IRMv1 allows us for the
first time to directly optimize a common fairness criterion, group-sufficiency,
and thereby improve performance on a fair prediction task.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 17:11:46 GMT'}]",2020-10-15,"[['Creager', 'Elliot', ''], ['Jacobsen', 'Jörn-Henrik', ''], ['Zemel', 'Richard', '']]"
1363719,2010.07389,Christopher Frye,"Tom Begley, Tobias Schwedes, Christopher Frye, Ilya Feige",Explainability for fair machine learning,"8 pages, 3 figures, 2 tables, 1 appendix",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As the decisions made or influenced by machine learning models increasingly
impact our lives, it is crucial to detect, understand, and mitigate unfairness.
But even simply determining what ""unfairness"" should mean in a given context is
non-trivial: there are many competing definitions, and choosing between them
often requires a deep understanding of the underlying task. It is thus tempting
to use model explainability to gain insights into model fairness, however
existing explainability tools do not reliably indicate whether a model is
indeed fair. In this work we present a new approach to explaining fairness in
machine learning, based on the Shapley value paradigm. Our fairness
explanations attribute a model's overall unfairness to individual input
features, even in cases where the model does not operate on sensitive
attributes directly. Moreover, motivated by the linearity of Shapley
explainability, we propose a meta algorithm for applying existing training-time
fairness interventions, wherein one trains a perturbation to the original
model, rather than a new model entirely. By explaining the original model, the
perturbation, and the fair-corrected model, we gain insight into the
accuracy-fairness trade-off that is being made by the intervention. We further
show that this meta algorithm enjoys both flexibility and stability benefits
with no loss in performance.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 20:21:01 GMT'}]",2020-10-16,"[['Begley', 'Tom', ''], ['Schwedes', 'Tobias', ''], ['Frye', 'Christopher', ''], ['Feige', 'Ilya', '']]"
