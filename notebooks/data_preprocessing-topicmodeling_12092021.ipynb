{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and LDA Model Fitting for Iteration 3 of Fair Is:\n",
    "In this notebook I document preprocessing and LDA model fitting of my Primary Dataset for iteration 3 of the Fair Is project. \n",
    "For more information on the previous iterations of this project you can see my Data Managemant Plan and Methodologies Statement. \n",
    "\n",
    "To see how the Primary Dataset was created see Data Creation for Iteration 3 of Fair Is.\n",
    "\n",
    "This notebook is split between Preprocessing Steps and Model Fitting for our corpus of 308 papers.\n",
    "\n",
    "I conducted the following preprocessing steps:\n",
    "- **Tokenization**\n",
    "    - *ngrams*\n",
    "    - *bi-grams*\n",
    "    - *ngram verbs*\n",
    "    - *ngram nouns*\n",
    "    - *bigram nouns*\n",
    "- **Lematization**\n",
    "- **Creation of Dictionary and Document Term Matrices**\n",
    "\n",
    "I conducted the following Topic Modeling Steps: \n",
    "- **Fit model using LDA**\n",
    "- **Fitting other tipic CorEx (Correlation Explanation)**\n",
    "\n",
    "- **Further Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import nltk as nltk\n",
    "import gensim as gm\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aster/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in order to use the word_tokenize function we need the nltk punkt package. \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = os.path.join('../data/processed_data/csv/cleaned_primary_data_12022021.csv')\n",
    "data = pd.read_csv(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>unfair items detection educational measurement</td>\n",
       "      <td>measurement professionals come agreement defi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fairness academic course timetabling</td>\n",
       "      <td>consider problem creating fair course timetab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>safeguarding ecommerce advisor cheating behavi...</td>\n",
       "      <td>electronic marketplaces transaction buyers wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>decomposition maxmin fair curriculumbased cou...</td>\n",
       "      <td>propose decomposition maxmin fair curriculumb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fair assignment indivisible objects ordinal pr...</td>\n",
       "      <td>consider discrete assignment problem agents e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                              title  \\\n",
       "0  1     unfair items detection educational measurement   \n",
       "1  2               fairness academic course timetabling   \n",
       "2  3  safeguarding ecommerce advisor cheating behavi...   \n",
       "3  4   decomposition maxmin fair curriculumbased cou...   \n",
       "4  5  fair assignment indivisible objects ordinal pr...   \n",
       "\n",
       "                                            abstract  \n",
       "0   measurement professionals come agreement defi...  \n",
       "1   consider problem creating fair course timetab...  \n",
       "2   electronic marketplaces transaction buyers wi...  \n",
       "3   propose decomposition maxmin fair curriculumb...  \n",
       "4   consider discrete assignment problem agents e...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just checking out data real quick\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'title', 'abstract'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "\n",
    "Preprocessing is somewhat similar to cleaning, it describes the steps made to prepare data to be fit to a model. While the primary dataset is structured and cleaned it's still much closer to unstructured text than to something machine readable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokenization* is the process of seperating meaningful strings of text into units called tokens. in Text Analysis, and Natural Language Processing more generally, models don't \"understand\" or \"read\" text in the way a human does. \n",
    "\n",
    "In tokenization i'm also thing about ngrams, or a sequence of tokens where *n* is some number --- A single token would be a unigram, two tokens would be a bigram, and so forth.\n",
    "\n",
    "Using bigrams allows us to take account of terms like \"machine learning\" rather than consider them seperate terms. \n",
    "\n",
    "So we will create new columns from our abstracts: \n",
    "\n",
    "- unigram tokens for titles\n",
    "- unigram tokens for abstracts\n",
    "- bigram tokens for titles\n",
    "- bigram tokens for abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unigram tokens for titles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tookenization code was figured out by Professor Vicky Rampin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map = iterator (goes thru each row)\n",
    "# x = specific title that is being tokenized in the specific moment\n",
    "\n",
    "data['title_tokens'] = data['title'].map(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigram tokens for titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['title_bigrams'] = data['title_tokens'].apply(lambda row: list(nltk.bigrams(row)))\n",
    "#print(data['title_bigrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unigram tokens for abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's apply this to abstracts:\n",
    "data['abstract_tokens'] = data['abstract'].map(lambda x: nltk.word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigram tokens for abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_bigrams</th>\n",
       "      <th>abstract_tokens</th>\n",
       "      <th>abstract_bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>unfair items detection educational measurement</td>\n",
       "      <td>measurement professionals come agreement defi...</td>\n",
       "      <td>[unfair, items, detection, educational, measur...</td>\n",
       "      <td>[(unfair, items), (items, detection), (detecti...</td>\n",
       "      <td>[measurement, professionals, come, agreement, ...</td>\n",
       "      <td>[(measurement, professionals), (professionals,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fairness academic course timetabling</td>\n",
       "      <td>consider problem creating fair course timetab...</td>\n",
       "      <td>[fairness, academic, course, timetabling]</td>\n",
       "      <td>[(fairness, academic), (academic, course), (co...</td>\n",
       "      <td>[consider, problem, creating, fair, course, ti...</td>\n",
       "      <td>[(consider, problem), (problem, creating), (cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>safeguarding ecommerce advisor cheating behavi...</td>\n",
       "      <td>electronic marketplaces transaction buyers wi...</td>\n",
       "      <td>[safeguarding, ecommerce, advisor, cheating, b...</td>\n",
       "      <td>[(safeguarding, ecommerce), (ecommerce, adviso...</td>\n",
       "      <td>[electronic, marketplaces, transaction, buyers...</td>\n",
       "      <td>[(electronic, marketplaces), (marketplaces, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>decomposition maxmin fair curriculumbased cou...</td>\n",
       "      <td>propose decomposition maxmin fair curriculumb...</td>\n",
       "      <td>[decomposition, maxmin, fair, curriculumbased,...</td>\n",
       "      <td>[(decomposition, maxmin), (maxmin, fair), (fai...</td>\n",
       "      <td>[propose, decomposition, maxmin, fair, curricu...</td>\n",
       "      <td>[(propose, decomposition), (decomposition, max...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fair assignment indivisible objects ordinal pr...</td>\n",
       "      <td>consider discrete assignment problem agents e...</td>\n",
       "      <td>[fair, assignment, indivisible, objects, ordin...</td>\n",
       "      <td>[(fair, assignment), (assignment, indivisible)...</td>\n",
       "      <td>[consider, discrete, assignment, problem, agen...</td>\n",
       "      <td>[(consider, discrete), (discrete, assignment),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                              title  \\\n",
       "0  1     unfair items detection educational measurement   \n",
       "1  2               fairness academic course timetabling   \n",
       "2  3  safeguarding ecommerce advisor cheating behavi...   \n",
       "3  4   decomposition maxmin fair curriculumbased cou...   \n",
       "4  5  fair assignment indivisible objects ordinal pr...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0   measurement professionals come agreement defi...   \n",
       "1   consider problem creating fair course timetab...   \n",
       "2   electronic marketplaces transaction buyers wi...   \n",
       "3   propose decomposition maxmin fair curriculumb...   \n",
       "4   consider discrete assignment problem agents e...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0  [unfair, items, detection, educational, measur...   \n",
       "1          [fairness, academic, course, timetabling]   \n",
       "2  [safeguarding, ecommerce, advisor, cheating, b...   \n",
       "3  [decomposition, maxmin, fair, curriculumbased,...   \n",
       "4  [fair, assignment, indivisible, objects, ordin...   \n",
       "\n",
       "                                       title_bigrams  \\\n",
       "0  [(unfair, items), (items, detection), (detecti...   \n",
       "1  [(fairness, academic), (academic, course), (co...   \n",
       "2  [(safeguarding, ecommerce), (ecommerce, adviso...   \n",
       "3  [(decomposition, maxmin), (maxmin, fair), (fai...   \n",
       "4  [(fair, assignment), (assignment, indivisible)...   \n",
       "\n",
       "                                     abstract_tokens  \\\n",
       "0  [measurement, professionals, come, agreement, ...   \n",
       "1  [consider, problem, creating, fair, course, ti...   \n",
       "2  [electronic, marketplaces, transaction, buyers...   \n",
       "3  [propose, decomposition, maxmin, fair, curricu...   \n",
       "4  [consider, discrete, assignment, problem, agen...   \n",
       "\n",
       "                                    abstract_bigrams  \n",
       "0  [(measurement, professionals), (professionals,...  \n",
       "1  [(consider, problem), (problem, creating), (cr...  \n",
       "2  [(electronic, marketplaces), (marketplaces, tr...  \n",
       "3  [(propose, decomposition), (decomposition, max...  \n",
       "4  [(consider, discrete), (discrete, assignment),...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['abstract_bigrams'] = data['abstract_tokens'].apply(lambda row: list(nltk.bigrams(row)))\n",
    "#checking dataframe\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aster/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in order to use the nltk pos_tag function need averaged_perceptron_tagger\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying parts of speech tagging with unigram tokens\n",
    "data['title_tokens_pos'] = data['title_tokens'].apply(lambda row: list(nltk.pos_tag(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does this also work with bigrams? \n",
    "data['title_bigrams_pos'] = data['title_tokens_pos'].apply(lambda row: list(nltk.bigrams(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now parts of speech tagging for abstracts bigrams\n",
    "data['abstract_tokens_pos'] = data['abstract_tokens'].apply(lambda row: list(nltk.pos_tag(row)))\n",
    "data['abstract_bigrams_pos'] = data['abstract_tokens_pos'].apply(lambda row: list(nltk.bigrams(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_bigrams</th>\n",
       "      <th>abstract_tokens</th>\n",
       "      <th>abstract_bigrams</th>\n",
       "      <th>title_tokens_pos</th>\n",
       "      <th>title_bigrams_pos</th>\n",
       "      <th>abstract_tokens_pos</th>\n",
       "      <th>abstract_bigrams_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>unfair items detection educational measurement</td>\n",
       "      <td>measurement professionals come agreement defi...</td>\n",
       "      <td>[unfair, items, detection, educational, measur...</td>\n",
       "      <td>[(unfair, items), (items, detection), (detecti...</td>\n",
       "      <td>[measurement, professionals, come, agreement, ...</td>\n",
       "      <td>[(measurement, professionals), (professionals,...</td>\n",
       "      <td>[(unfair, JJ), (items, NNS), (detection, VBP),...</td>\n",
       "      <td>[((unfair, JJ), (items, NNS)), ((items, NNS), ...</td>\n",
       "      <td>[(measurement, NN), (professionals, NNS), (com...</td>\n",
       "      <td>[((measurement, NN), (professionals, NNS)), ((...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fairness academic course timetabling</td>\n",
       "      <td>consider problem creating fair course timetab...</td>\n",
       "      <td>[fairness, academic, course, timetabling]</td>\n",
       "      <td>[(fairness, academic), (academic, course), (co...</td>\n",
       "      <td>[consider, problem, creating, fair, course, ti...</td>\n",
       "      <td>[(consider, problem), (problem, creating), (cr...</td>\n",
       "      <td>[(fairness, JJ), (academic, JJ), (course, NN),...</td>\n",
       "      <td>[((fairness, JJ), (academic, JJ)), ((academic,...</td>\n",
       "      <td>[(consider, VB), (problem, NN), (creating, VBG...</td>\n",
       "      <td>[((consider, VB), (problem, NN)), ((problem, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>safeguarding ecommerce advisor cheating behavi...</td>\n",
       "      <td>electronic marketplaces transaction buyers wi...</td>\n",
       "      <td>[safeguarding, ecommerce, advisor, cheating, b...</td>\n",
       "      <td>[(safeguarding, ecommerce), (ecommerce, adviso...</td>\n",
       "      <td>[electronic, marketplaces, transaction, buyers...</td>\n",
       "      <td>[(electronic, marketplaces), (marketplaces, tr...</td>\n",
       "      <td>[(safeguarding, VBG), (ecommerce, NN), (adviso...</td>\n",
       "      <td>[((safeguarding, VBG), (ecommerce, NN)), ((eco...</td>\n",
       "      <td>[(electronic, JJ), (marketplaces, NNS), (trans...</td>\n",
       "      <td>[((electronic, JJ), (marketplaces, NNS)), ((ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>decomposition maxmin fair curriculumbased cou...</td>\n",
       "      <td>propose decomposition maxmin fair curriculumb...</td>\n",
       "      <td>[decomposition, maxmin, fair, curriculumbased,...</td>\n",
       "      <td>[(decomposition, maxmin), (maxmin, fair), (fai...</td>\n",
       "      <td>[propose, decomposition, maxmin, fair, curricu...</td>\n",
       "      <td>[(propose, decomposition), (decomposition, max...</td>\n",
       "      <td>[(decomposition, NN), (maxmin, NN), (fair, NN)...</td>\n",
       "      <td>[((decomposition, NN), (maxmin, NN)), ((maxmin...</td>\n",
       "      <td>[(propose, JJ), (decomposition, NN), (maxmin, ...</td>\n",
       "      <td>[((propose, JJ), (decomposition, NN)), ((decom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fair assignment indivisible objects ordinal pr...</td>\n",
       "      <td>consider discrete assignment problem agents e...</td>\n",
       "      <td>[fair, assignment, indivisible, objects, ordin...</td>\n",
       "      <td>[(fair, assignment), (assignment, indivisible)...</td>\n",
       "      <td>[consider, discrete, assignment, problem, agen...</td>\n",
       "      <td>[(consider, discrete), (discrete, assignment),...</td>\n",
       "      <td>[(fair, JJ), (assignment, NN), (indivisible, J...</td>\n",
       "      <td>[((fair, JJ), (assignment, NN)), ((assignment,...</td>\n",
       "      <td>[(consider, VB), (discrete, JJ), (assignment, ...</td>\n",
       "      <td>[((consider, VB), (discrete, JJ)), ((discrete,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                              title  \\\n",
       "0  1     unfair items detection educational measurement   \n",
       "1  2               fairness academic course timetabling   \n",
       "2  3  safeguarding ecommerce advisor cheating behavi...   \n",
       "3  4   decomposition maxmin fair curriculumbased cou...   \n",
       "4  5  fair assignment indivisible objects ordinal pr...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0   measurement professionals come agreement defi...   \n",
       "1   consider problem creating fair course timetab...   \n",
       "2   electronic marketplaces transaction buyers wi...   \n",
       "3   propose decomposition maxmin fair curriculumb...   \n",
       "4   consider discrete assignment problem agents e...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0  [unfair, items, detection, educational, measur...   \n",
       "1          [fairness, academic, course, timetabling]   \n",
       "2  [safeguarding, ecommerce, advisor, cheating, b...   \n",
       "3  [decomposition, maxmin, fair, curriculumbased,...   \n",
       "4  [fair, assignment, indivisible, objects, ordin...   \n",
       "\n",
       "                                       title_bigrams  \\\n",
       "0  [(unfair, items), (items, detection), (detecti...   \n",
       "1  [(fairness, academic), (academic, course), (co...   \n",
       "2  [(safeguarding, ecommerce), (ecommerce, adviso...   \n",
       "3  [(decomposition, maxmin), (maxmin, fair), (fai...   \n",
       "4  [(fair, assignment), (assignment, indivisible)...   \n",
       "\n",
       "                                     abstract_tokens  \\\n",
       "0  [measurement, professionals, come, agreement, ...   \n",
       "1  [consider, problem, creating, fair, course, ti...   \n",
       "2  [electronic, marketplaces, transaction, buyers...   \n",
       "3  [propose, decomposition, maxmin, fair, curricu...   \n",
       "4  [consider, discrete, assignment, problem, agen...   \n",
       "\n",
       "                                    abstract_bigrams  \\\n",
       "0  [(measurement, professionals), (professionals,...   \n",
       "1  [(consider, problem), (problem, creating), (cr...   \n",
       "2  [(electronic, marketplaces), (marketplaces, tr...   \n",
       "3  [(propose, decomposition), (decomposition, max...   \n",
       "4  [(consider, discrete), (discrete, assignment),...   \n",
       "\n",
       "                                    title_tokens_pos  \\\n",
       "0  [(unfair, JJ), (items, NNS), (detection, VBP),...   \n",
       "1  [(fairness, JJ), (academic, JJ), (course, NN),...   \n",
       "2  [(safeguarding, VBG), (ecommerce, NN), (adviso...   \n",
       "3  [(decomposition, NN), (maxmin, NN), (fair, NN)...   \n",
       "4  [(fair, JJ), (assignment, NN), (indivisible, J...   \n",
       "\n",
       "                                   title_bigrams_pos  \\\n",
       "0  [((unfair, JJ), (items, NNS)), ((items, NNS), ...   \n",
       "1  [((fairness, JJ), (academic, JJ)), ((academic,...   \n",
       "2  [((safeguarding, VBG), (ecommerce, NN)), ((eco...   \n",
       "3  [((decomposition, NN), (maxmin, NN)), ((maxmin...   \n",
       "4  [((fair, JJ), (assignment, NN)), ((assignment,...   \n",
       "\n",
       "                                 abstract_tokens_pos  \\\n",
       "0  [(measurement, NN), (professionals, NNS), (com...   \n",
       "1  [(consider, VB), (problem, NN), (creating, VBG...   \n",
       "2  [(electronic, JJ), (marketplaces, NNS), (trans...   \n",
       "3  [(propose, JJ), (decomposition, NN), (maxmin, ...   \n",
       "4  [(consider, VB), (discrete, JJ), (assignment, ...   \n",
       "\n",
       "                                abstract_bigrams_pos  \n",
       "0  [((measurement, NN), (professionals, NNS)), ((...  \n",
       "1  [((consider, VB), (problem, NN)), ((problem, N...  \n",
       "2  [((electronic, JJ), (marketplaces, NNS)), ((ma...  \n",
       "3  [((propose, JJ), (decomposition, NN)), ((decom...  \n",
       "4  [((consider, VB), (discrete, JJ)), ((discrete,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if this worked:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "*Lemmatization* is a commonly used pre-processing step in text analysis. When we lemmatize tokens, we shorten them to the shortest meaningful root of a word, called a lemma. For example *running* becomes run. \n",
    "\n",
    "To lemmatize we will using the WordNetLemmatizer, a tool that is part of the NLTK package and uses [WordNet](https://wordnet.princeton.edu/), a database of semantic relations between word forms in over 200 languages to lemmatize the words in our abstract and title data. \n",
    "\n",
    "Finally, WordNetLemmatizer allows us to chose the part of speech of the lemma. In this iteration I am selecting the verb part of speech, one because i'm considering the \"understanding\" of fairness as proceedural, in action and secondly for the sake of making a decision to move through this project. I also leave code as a comment to return noun forms of lemmas (if no part of speech is specified WordNetLemmatizer defaults to nouns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to use lemmatization we need to use import the WordNetLemmatizer and wordnet dictionary\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the WordNetLemmatizer to a variable\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_text(text):\n",
    "    return [lem.lemmatize(w, 'v') for w in text] #for verbs\n",
    "    #return [lem.lemmatize(w, 'n') for w in text] #remove hastag/pound, comment out verb line and run cell for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize titles:\n",
    "data['title_lemmatized']= data['title_tokens'].map(lambda x: lem_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize abstracts:\n",
    "data['abstract_lemmatized'] = data['abstract_tokens'].map(lambda x: lem_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get title lemma bigrams:\n",
    "data['title_bigram_lemmatized']=data['title_lemmatized'].apply(lambda row: list(nltk.bigrams(row)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get abstract lemma bigrams:\n",
    "data['abstract_bigram_lemmatized']=data['abstract_lemmatized'].apply(lambda row: list(nltk.bigrams(row)))\n",
    "                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Cleaning and Normalization:\n",
    "\n",
    "One last check to see if there are any rows without values (imagine an empty cell in a spreadsheet) we need to consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also deal with a stray column at the begiining since by changing it's name and setting it as our index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing column name\n",
    "data = data.rename(columns={\"X\":\"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to make sure it worked:\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set newly renamed column as index\n",
    "data = data.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to make sure it worked\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save our preprocessed data and we're ready for Topic Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/processed_data/csv/processed_primary_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just making sure everything looks good!\n",
    "#test = pd.read_csv('../data/processed_data/csv/processed_primary_data.csv')\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Dictionary, Corpus and Document Term Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CorEX - Correlation Explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
